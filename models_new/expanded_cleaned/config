!ModelConfig
config_data: !DataConfig
  data_statistics: !DataStatistics
    average_len_target_per_bucket:
    - 4.140326774608547
    - 5.108531027431742
    - 7.958035714285693
    - 13.630458715596294
    - 19.282285714285702
    - 19.158215010142012
    - 23.056010069225927
    - 24.663869463869425
    - 27.240155945419072
    - 29.649572649572605
    - 32.15597920277298
    - 35.04238410596021
    - 100.0
    - 108.09090909090908
    - 116.57142857142857
    - 125.0
    - 131.4
    - 140.5
    - null
    - 158.5
    - null
    - 172.0
    - 179.0
    - 188.4
    - null
    - null
    buckets:
    - !!python/tuple
      - 8
      - 8
    - !!python/tuple
      - 16
      - 16
    - !!python/tuple
      - 24
      - 24
    - !!python/tuple
      - 32
      - 32
    - !!python/tuple
      - 40
      - 40
    - !!python/tuple
      - 48
      - 48
    - !!python/tuple
      - 56
      - 56
    - !!python/tuple
      - 64
      - 64
    - !!python/tuple
      - 72
      - 72
    - !!python/tuple
      - 80
      - 80
    - !!python/tuple
      - 88
      - 88
    - !!python/tuple
      - 96
      - 96
    - !!python/tuple
      - 104
      - 104
    - !!python/tuple
      - 112
      - 112
    - !!python/tuple
      - 120
      - 120
    - !!python/tuple
      - 128
      - 128
    - !!python/tuple
      - 136
      - 136
    - !!python/tuple
      - 144
      - 144
    - !!python/tuple
      - 152
      - 152
    - !!python/tuple
      - 160
      - 160
    - !!python/tuple
      - 168
      - 168
    - !!python/tuple
      - 176
      - 176
    - !!python/tuple
      - 184
      - 184
    - !!python/tuple
      - 192
      - 192
    - !!python/tuple
      - 200
      - 200
    - !!python/tuple
      - 201
      - 201
    length_ratio_mean: 0.5463525593793229
    length_ratio_stats_per_bucket:
    - !!python/tuple
      - 0.5781455199493354
      - 0.20559989890762204
    - !!python/tuple
      - 0.5035479926045533
      - 0.31443786222138226
    - !!python/tuple
      - 0.5468974987608402
      - 0.6491670331407603
    - !!python/tuple
      - 0.91248259310849
      - 1.2010031762840225
    - !!python/tuple
      - 1.1660911955799533
      - 1.5511466062959094
    - !!python/tuple
      - 0.8187221782430846
      - 1.3467568304114719
    - !!python/tuple
      - 0.6641840418457079
      - 1.0642485351442208
    - !!python/tuple
      - 0.5038462754858931
      - 0.8043286661508741
    - !!python/tuple
      - 0.48069506489906394
      - 0.7869185585367217
    - !!python/tuple
      - 0.4669203885345722
      - 0.8762070208109582
    - !!python/tuple
      - 0.44644518448479037
      - 0.8098897847283998
    - !!python/tuple
      - 0.5449491437402029
      - 1.2820685486019436
    - !!python/tuple
      - 8.591055328871786
      - 3.9510322226387298
    - !!python/tuple
      - 7.201917367926802
      - 3.6758955236249236
    - !!python/tuple
      - 11.221567717996288
      - 2.1560031382015623
    - !!python/tuple
      - 6.269197912671014
      - 4.738615940455274
    - !!python/tuple
      - 16.691666666666666
      - 2.854761904761905
    - !!python/tuple
      - 16.043939393939393
      - 4.138753056115349
    - &id001 !!python/tuple
      - null
      - null
    - !!python/tuple
      - 24.547619047619047
      - 2.1190476190476204
    - *id001
    - !!python/tuple
      - 23.044642857142858
      - 1.6696428571428577
    - !!python/tuple
      - 19.53956228956229
      - 2.723775920544078
    - !!python/tuple
      - 20.744761904761905
      - 4.800873503286439
    - *id001
    - *id001
    length_ratio_std: 0.4182879569055765
    max_observed_len_source: 96
    max_observed_len_target: 192
    num_discarded: 0
    num_sents: 354727
    num_sents_per_bucket:
    - 149216
    - 173121
    - 14560
    - 2725
    - 1750
    - 1479
    - 1589
    - 2145
    - 2565
    - 2457
    - 2308
    - 755
    - 12
    - 11
    - 7
    - 6
    - 5
    - 4
    - 0
    - 2
    - 0
    - 2
    - 3
    - 5
    - 0
    - 0
    num_tokens_source: 4201025
    num_tokens_target: 2057535
    num_unks_source: 167
    num_unks_target: 134832
    size_vocab_source: 15608
    size_vocab_target: 4312
  eop_id: -1
  max_seq_len_source: 201
  max_seq_len_target: 201
  num_source_factors: 8
  num_target_factors: 1
config_decoder: !TransformerConfig
  act_type: relu
  attention_heads: 8
  block_prepended_cross_attention: false
  decoder_type: transformer
  depth_key_value: 624
  dropout_act: 0.1
  dropout_attention: 0.5
  dropout_prepost: 0.1
  feed_forward_num_hidden: 2048
  max_seq_len_source: 201
  max_seq_len_target: 201
  model_size: 512
  num_layers: 6
  positional_embedding_type: fixed
  postprocess_sequence: dr
  preprocess_sequence: n
  use_glu: false
  use_lhuc: false
config_embed_source: !EmbeddingConfig
  allow_sparse_grad: false
  dropout: 0.5
  factor_configs:
  - !FactorConfig
    combine: concat
    num_embed: 16
    share_embedding: false
    vocab_size: 432
  - !FactorConfig
    combine: concat
    num_embed: 16
    share_embedding: false
    vocab_size: 472
  - !FactorConfig
    combine: concat
    num_embed: 16
    share_embedding: false
    vocab_size: 64
  - !FactorConfig
    combine: concat
    num_embed: 16
    share_embedding: false
    vocab_size: 56
  - !FactorConfig
    combine: concat
    num_embed: 16
    share_embedding: false
    vocab_size: 760
  - !FactorConfig
    combine: concat
    num_embed: 16
    share_embedding: false
    vocab_size: 16
  - !FactorConfig
    combine: concat
    num_embed: 16
    share_embedding: false
    vocab_size: 24
  num_embed: 512
  num_factors: 8
  vocab_size: 15608
config_embed_target: !EmbeddingConfig
  allow_sparse_grad: false
  dropout: 0.5
  factor_configs: null
  num_embed: 512
  num_factors: 1
  vocab_size: 4312
config_encoder: !TransformerConfig
  act_type: relu
  attention_heads: 8
  block_prepended_cross_attention: false
  decoder_type: transformer
  depth_key_value: 624
  dropout_act: 0.1
  dropout_attention: 0.5
  dropout_prepost: 0.1
  feed_forward_num_hidden: 2048
  max_seq_len_source: 201
  max_seq_len_target: 201
  model_size: 624
  num_layers: 6
  positional_embedding_type: fixed
  postprocess_sequence: dr
  preprocess_sequence: n
  use_glu: false
  use_lhuc: false
config_length_task: null
dtype: float32
lhuc: false
neural_vocab_selection: null
neural_vocab_selection_block_loss: false
vocab_source_size: 15608
vocab_target_size: 4312
weight_tying_type: trg_softmax
