!ModelConfig
config_data: !DataConfig
  data_statistics: !DataStatistics
    average_len_target_per_bucket:
    - 4.215956318126238
    - 5.255615587998739
    - 8.219755751973558
    - 13.773279352226734
    - 18.90724117295031
    - 22.280465401102255
    - 24.01889168765744
    - 26.757188498402567
    - 28.605479452054844
    - 31.4091832588378
    - 33.720313179643405
    - 37.40891218872868
    - 100.05000000000001
    - 107.25000000000001
    - 115.0
    - 124.375
    - 134.42857142857142
    - 140.5
    - 148.0
    - 158.0
    - null
    - 173.0
    - 178.5
    - 187.33333333333334
    - 194.5
    - null
    buckets:
    - !!python/tuple
      - 8
      - 8
    - !!python/tuple
      - 16
      - 16
    - !!python/tuple
      - 24
      - 24
    - !!python/tuple
      - 32
      - 32
    - !!python/tuple
      - 40
      - 40
    - !!python/tuple
      - 48
      - 48
    - !!python/tuple
      - 56
      - 56
    - !!python/tuple
      - 64
      - 64
    - !!python/tuple
      - 72
      - 72
    - !!python/tuple
      - 80
      - 80
    - !!python/tuple
      - 88
      - 88
    - !!python/tuple
      - 96
      - 96
    - !!python/tuple
      - 104
      - 104
    - !!python/tuple
      - 112
      - 112
    - !!python/tuple
      - 120
      - 120
    - !!python/tuple
      - 128
      - 128
    - !!python/tuple
      - 136
      - 136
    - !!python/tuple
      - 144
      - 144
    - !!python/tuple
      - 152
      - 152
    - !!python/tuple
      - 160
      - 160
    - !!python/tuple
      - 168
      - 168
    - !!python/tuple
      - 176
      - 176
    - !!python/tuple
      - 184
      - 184
    - !!python/tuple
      - 192
      - 192
    - !!python/tuple
      - 200
      - 200
    - !!python/tuple
      - 201
      - 201
    length_ratio_mean: 0.5631922886448255
    length_ratio_stats_per_bucket:
    - !!python/tuple
      - 0.588892624403916
      - 0.20760204327439177
    - !!python/tuple
      - 0.5204595399491342
      - 0.3252743733178762
    - !!python/tuple
      - 0.588567356589642
      - 0.7037196037060348
    - !!python/tuple
      - 0.9152367604612587
      - 1.193622730324267
    - !!python/tuple
      - 1.057559786619243
      - 1.4358288699717354
    - !!python/tuple
      - 1.1639323170716318
      - 1.7449769807009596
    - !!python/tuple
      - 0.7269578631779554
      - 1.2037586395713893
    - !!python/tuple
      - 0.5783357659894331
      - 0.8871833275368517
    - !!python/tuple
      - 0.4947678477691236
      - 0.7965314276556164
    - !!python/tuple
      - 0.48821635982263106
      - 0.8632546665181101
    - !!python/tuple
      - 0.4498769928457549
      - 0.7441730769509901
    - !!python/tuple
      - 0.6038074343546629
      - 1.376625455660386
    - !!python/tuple
      - 8.52364298944623
      - 3.7048777075663675
    - !!python/tuple
      - 9.312100383631712
      - 4.347448593952467
    - !!python/tuple
      - 9.920299043062201
      - 3.0068695071959968
    - !!python/tuple
      - 7.31775934925658
      - 4.77182650527961
    - !!python/tuple
      - 15.61417748917749
      - 3.908159721960673
    - !!python/tuple
      - 13.163636363636364
      - 1.2339381288318816
    - !!python/tuple
      - 24.666666666666668
      - 0.0
    - !!python/tuple
      - 22.571428571428573
      - 0.0
    - &id001 !!python/tuple
      - null
      - null
    - !!python/tuple
      - 21.625
      - 0.0
    - !!python/tuple
      - 23.007575757575758
      - 6.825757575757574
    - !!python/tuple
      - 25.630952380952383
      - 1.3351390833475298
    - !!python/tuple
      - 20.42063492063492
      - 4.6258540916351345
    - *id001
    length_ratio_std: 0.4400367158119144
    max_observed_len_source: 96
    max_observed_len_target: 196
    num_discarded: 1
    num_sents: 354726
    num_sents_per_bucket:
    - 147979
    - 173980
    - 14821
    - 2717
    - 1671
    - 1633
    - 1588
    - 2191
    - 2555
    - 2461
    - 2299
    - 763
    - 20
    - 12
    - 5
    - 8
    - 7
    - 4
    - 1
    - 1
    - 0
    - 1
    - 2
    - 3
    - 4
    - 0
    num_tokens_source: 4201015
    num_tokens_target: 2127225
    num_unks_source: 167
    num_unks_target: 60
    size_vocab_source: 15608
    size_vocab_target: 4312
  eop_id: -1
  max_seq_len_source: 201
  max_seq_len_target: 201
  num_source_factors: 8
  num_target_factors: 1
config_decoder: !TransformerConfig
  act_type: relu
  attention_heads: 8
  block_prepended_cross_attention: false
  decoder_type: transformer
  depth_key_value: 624
  dropout_act: 0.1
  dropout_attention: 0.5
  dropout_prepost: 0.1
  feed_forward_num_hidden: 2048
  max_seq_len_source: 201
  max_seq_len_target: 201
  model_size: 512
  num_layers: 6
  positional_embedding_type: fixed
  postprocess_sequence: dr
  preprocess_sequence: n
  use_glu: false
  use_lhuc: false
config_embed_source: !EmbeddingConfig
  allow_sparse_grad: false
  dropout: 0.5
  factor_configs:
  - !FactorConfig
    combine: concat
    num_embed: 16
    share_embedding: false
    vocab_size: 432
  - !FactorConfig
    combine: concat
    num_embed: 16
    share_embedding: false
    vocab_size: 472
  - !FactorConfig
    combine: concat
    num_embed: 16
    share_embedding: false
    vocab_size: 64
  - !FactorConfig
    combine: concat
    num_embed: 16
    share_embedding: false
    vocab_size: 56
  - !FactorConfig
    combine: concat
    num_embed: 16
    share_embedding: false
    vocab_size: 760
  - !FactorConfig
    combine: concat
    num_embed: 16
    share_embedding: false
    vocab_size: 16
  - !FactorConfig
    combine: concat
    num_embed: 16
    share_embedding: false
    vocab_size: 24
  num_embed: 512
  num_factors: 8
  vocab_size: 15608
config_embed_target: !EmbeddingConfig
  allow_sparse_grad: false
  dropout: 0.5
  factor_configs: null
  num_embed: 512
  num_factors: 1
  vocab_size: 4312
config_encoder: !TransformerConfig
  act_type: relu
  attention_heads: 8
  block_prepended_cross_attention: false
  decoder_type: transformer
  depth_key_value: 624
  dropout_act: 0.1
  dropout_attention: 0.5
  dropout_prepost: 0.1
  feed_forward_num_hidden: 2048
  max_seq_len_source: 201
  max_seq_len_target: 201
  model_size: 624
  num_layers: 6
  positional_embedding_type: fixed
  postprocess_sequence: dr
  preprocess_sequence: n
  use_glu: false
  use_lhuc: false
config_length_task: null
dtype: float32
lhuc: false
neural_vocab_selection: null
neural_vocab_selection_block_loss: false
vocab_source_size: 15608
vocab_target_size: 4312
weight_tying_type: trg_softmax
