[2023-11-29:18:40:55:INFO:sockeye.utils:log_sockeye_version] Sockeye: 3.1.37, commit unknown, path /home/zifjia/sockeye_aws/sockeye/__init__.py
[2023-11-29:18:40:55:INFO:sockeye.utils:log_torch_version] PyTorch: 1.13.1+cu117 (/home/zifjia/data/conda/envs/signbank+/lib/python3.9/site-packages/torch/__init__.py)
[2023-11-29:18:40:55:INFO:sockeye.utils:log_basic_info] Command: /home/zifjia/sockeye_aws/sockeye/train.py --prepared-data data_sockeye_new_expanded_cleaned -vt data_new_cleaned/dev.spm.spoken -vs data_new_cleaned/dev.sign -vsf data_new_cleaned/dev.feat_x data_new_cleaned/dev.feat_y data_new_cleaned/dev.feat_x_rel data_new_cleaned/dev.feat_y_rel data_new_cleaned/dev.sign+ data_new_cleaned/dev.feat_col data_new_cleaned/dev.feat_row --output models_new/expanded_cleaned --overwrite-output --weight-tying-type trg_softmax --label-smoothing 0.2 --optimized-metric bleu --checkpoint-interval 4000 --update-interval 2 --max-num-epochs 300 --max-num-checkpoint-not-improved 10 --embed-dropout 0.5 --transformer-dropout-attention 0.5 --initial-learning-rate 0.0001 --learning-rate-reduce-factor 0.7 --learning-rate-reduce-num-not-improved 5 --decode-and-evaluate 500 --keep-last-params 1 --cache-last-best-params 1 --device-id 0 --seed 42 --source-factors-num-embed 16 16 16 16 16 16 16 --source-factors-combine concat --batch-size 2048 --params models_new/expanded/params.best
[2023-11-29:18:40:55:INFO:sockeye.utils:log_basic_info] Arguments: Namespace(config=None, source=None, source_factors=[], source_factors_use_source_vocab=[], target_factors=[], target_factors_use_target_vocab=[], target=None, end_of_prepending_tag=None, prepared_data='data_sockeye_new_expanded_cleaned', validation_source='data_new_cleaned/dev.sign', validation_source_factors=['data_new_cleaned/dev.feat_x', 'data_new_cleaned/dev.feat_y', 'data_new_cleaned/dev.feat_x_rel', 'data_new_cleaned/dev.feat_y_rel', 'data_new_cleaned/dev.sign+', 'data_new_cleaned/dev.feat_col', 'data_new_cleaned/dev.feat_row'], validation_target='data_new_cleaned/dev.spm.spoken', validation_target_factors=[], no_bucketing=False, bucket_width=8, bucket_scaling=False, max_seq_len=(95, 95), source_vocab=None, target_vocab=None, source_factor_vocabs=[], target_factor_vocabs=[], shared_vocab=False, num_words=(0, 0), word_min_count=(1, 1), pad_vocab_to_multiple_of=8, output='models_new/expanded_cleaned', overwrite_output=True, params='models_new/expanded/params.best', allow_missing_params=False, ignore_extra_params=False, encoder='transformer', decoder='transformer', num_layers=(6, 6), transformer_model_size=(512, 512), transformer_attention_heads=(8, 8), transformer_feed_forward_num_hidden=(2048, 2048), transformer_feed_forward_use_glu=False, transformer_activation_type=('relu', 'relu'), transformer_positional_embedding_type='fixed', transformer_block_prepended_cross_attention=False, transformer_preprocess=('n', 'n'), transformer_postprocess=('dr', 'dr'), lhuc=None, num_embed=(None, None), source_factors_num_embed=[16, 16, 16, 16, 16, 16, 16], target_factors_num_embed=[], source_factors_combine=['concat', 'concat', 'concat', 'concat', 'concat', 'concat', 'concat'], target_factors_combine=[], source_factors_share_embedding=[False, False, False, False, False, False, False], target_factors_share_embedding=[], weight_tying_type='trg_softmax', dtype='float32', clamp_to_dtype=False, amp=False, apex_amp=False, neural_vocab_selection=None, neural_vocab_selection_block_loss=False, batch_size=2048, batch_type='word', batch_sentences_multiple_of=8, update_interval=2, label_smoothing=0.2, label_smoothing_impl='mxnet', length_task=None, length_task_weight=1.0, length_task_layers=1, bow_task_weight=1.0, bow_task_pos_weight=10, target_factors_weight=[1.0], optimized_metric='bleu', checkpoint_interval=4000, min_samples=None, max_samples=None, min_updates=None, max_updates=None, max_seconds=None, max_checkpoints=None, max_num_checkpoint_not_improved=10, checkpoint_improvement_threshold=0.0, min_num_epochs=None, max_num_epochs=300, embed_dropout=(0.5, 0.5), transformer_dropout_attention=(0.5, 0.5), transformer_dropout_act=(0.1, 0.1), transformer_dropout_prepost=(0.1, 0.1), optimizer='adam', optimizer_betas=(0.9, 0.999), optimizer_eps=1e-08, dist=False, initial_learning_rate=0.0001, weight_decay=0.0, momentum=0.0, gradient_clipping_threshold=1.0, gradient_clipping_type='none', learning_rate_scheduler_type='plateau-reduce', learning_rate_reduce_factor=0.7, learning_rate_reduce_num_not_improved=5, learning_rate_warmup=0, no_reload_on_learning_rate_reduce=False, fixed_param_strategy=None, fixed_param_names=[], local_rank=None, deepspeed_fp16=False, deepspeed_bf16=False, decode_and_evaluate=500, stop_training_on_decoder_failure=False, seed=42, keep_last_params=1, keep_initializations=False, cache_last_best_params=1, cache_strategy='best', cache_metric='perplexity', dry_run=False, device_id=0, use_cpu=False, env=None, tf32=True, quiet=False, quiet_secondary_workers=False, no_logfile=False, loglevel='INFO', loglevel_secondary_workers='INFO')
[2023-11-29:18:40:55:INFO:__main__:train] Adjusting maximum length to reserve space for a BOS/EOS marker. New maximum length: (96, 96)
[2023-11-29:18:40:55:INFO:sockeye.utils:init_device] CUDA: allow tf32 (float32 but with 10 bits precision)
[2023-11-29:18:40:55:INFO:__main__:train] Training Device: cuda:0
[2023-11-29:18:40:55:INFO:sockeye.utils:seed_rngs] Random seed: 42
[2023-11-29:18:40:55:INFO:sockeye.utils:seed_rngs] PyTorch seed: 42
[2023-11-29:18:40:55:INFO:sockeye.data_io:get_prepared_data_iters] ===============================
[2023-11-29:18:40:55:INFO:sockeye.data_io:get_prepared_data_iters] Creating training data iterator
[2023-11-29:18:40:55:INFO:sockeye.data_io:get_prepared_data_iters] ===============================
[2023-11-29:18:40:56:INFO:sockeye.vocab:vocab_from_json] Vocabulary (15608 words) loaded from "data_sockeye_new_expanded_cleaned/vocab.src.0.json"
[2023-11-29:18:40:56:INFO:sockeye.vocab:vocab_from_json] Vocabulary (432 words) loaded from "data_sockeye_new_expanded_cleaned/vocab.src.1.json"
[2023-11-29:18:40:56:INFO:sockeye.vocab:vocab_from_json] Vocabulary (472 words) loaded from "data_sockeye_new_expanded_cleaned/vocab.src.2.json"
[2023-11-29:18:40:56:INFO:sockeye.vocab:vocab_from_json] Vocabulary (64 words) loaded from "data_sockeye_new_expanded_cleaned/vocab.src.3.json"
[2023-11-29:18:40:56:INFO:sockeye.vocab:vocab_from_json] Vocabulary (56 words) loaded from "data_sockeye_new_expanded_cleaned/vocab.src.4.json"
[2023-11-29:18:40:56:INFO:sockeye.vocab:vocab_from_json] Vocabulary (760 words) loaded from "data_sockeye_new_expanded_cleaned/vocab.src.5.json"
[2023-11-29:18:40:56:INFO:sockeye.vocab:vocab_from_json] Vocabulary (16 words) loaded from "data_sockeye_new_expanded_cleaned/vocab.src.6.json"
[2023-11-29:18:40:56:INFO:sockeye.vocab:vocab_from_json] Vocabulary (24 words) loaded from "data_sockeye_new_expanded_cleaned/vocab.src.7.json"
[2023-11-29:18:40:56:INFO:sockeye.vocab:vocab_from_json] Vocabulary (4312 words) loaded from "data_sockeye_new_expanded_cleaned/vocab.trg.0.json"
[2023-11-29:18:40:56:INFO:sockeye.data_io:log] Tokens: source 4201025 target 2057535
[2023-11-29:18:40:56:INFO:sockeye.data_io:log] Number of <unk> tokens: source 167 target 134832
[2023-11-29:18:40:56:INFO:sockeye.data_io:log] Vocabulary coverage: source 100% target 93%
[2023-11-29:18:40:56:INFO:sockeye.data_io:log] 354727 sequences across 26 buckets
[2023-11-29:18:40:56:INFO:sockeye.data_io:log] 0 sequences did not fit into buckets and were discarded
[2023-11-29:18:40:56:INFO:sockeye.data_io:describe_data_and_buckets] Bucket (8, 8): 149216 samples in 301 batches of 496, ~2053.6 target tokens/batch, trg/src length ratio: 0.58 (+-0.21)
[2023-11-29:18:40:56:INFO:sockeye.data_io:describe_data_and_buckets] Bucket (16, 16): 173121 samples in 433 batches of 400, ~2043.4 target tokens/batch, trg/src length ratio: 0.50 (+-0.31)
[2023-11-29:18:40:56:INFO:sockeye.data_io:describe_data_and_buckets] Bucket (24, 24): 14560 samples in 57 batches of 256, ~2037.3 target tokens/batch, trg/src length ratio: 0.55 (+-0.65)
[2023-11-29:18:40:56:INFO:sockeye.data_io:describe_data_and_buckets] Bucket (32, 32): 2725 samples in 18 batches of 152, ~2071.8 target tokens/batch, trg/src length ratio: 0.91 (+-1.20)
[2023-11-29:18:40:56:INFO:sockeye.data_io:describe_data_and_buckets] Bucket (40, 40): 1750 samples in 17 batches of 104, ~2005.4 target tokens/batch, trg/src length ratio: 1.17 (+-1.55)
[2023-11-29:18:40:56:INFO:sockeye.data_io:describe_data_and_buckets] Bucket (48, 48): 1479 samples in 15 batches of 104, ~1992.5 target tokens/batch, trg/src length ratio: 0.82 (+-1.35)
[2023-11-29:18:40:56:INFO:sockeye.data_io:describe_data_and_buckets] Bucket (56, 56): 1589 samples in 19 batches of 88, ~2028.9 target tokens/batch, trg/src length ratio: 0.66 (+-1.06)
[2023-11-29:18:40:56:INFO:sockeye.data_io:describe_data_and_buckets] Bucket (64, 64): 2145 samples in 27 batches of 80, ~1973.1 target tokens/batch, trg/src length ratio: 0.50 (+-0.80)
[2023-11-29:18:40:56:INFO:sockeye.data_io:describe_data_and_buckets] Bucket (72, 72): 2565 samples in 36 batches of 72, ~1961.3 target tokens/batch, trg/src length ratio: 0.48 (+-0.79)
[2023-11-29:18:40:56:INFO:sockeye.data_io:describe_data_and_buckets] Bucket (80, 80): 2457 samples in 35 batches of 72, ~2134.8 target tokens/batch, trg/src length ratio: 0.47 (+-0.88)
[2023-11-29:18:40:56:INFO:sockeye.data_io:describe_data_and_buckets] Bucket (88, 88): 2308 samples in 37 batches of 64, ~2058.0 target tokens/batch, trg/src length ratio: 0.45 (+-0.81)
[2023-11-29:18:40:56:INFO:sockeye.data_io:describe_data_and_buckets] Bucket (96, 96): 755 samples in 14 batches of 56, ~1962.4 target tokens/batch, trg/src length ratio: 0.54 (+-1.28)
[2023-11-29:18:40:56:INFO:sockeye.data_io:describe_data_and_buckets] Bucket (104, 104): 12 samples in 1 batches of 24, ~2400.0 target tokens/batch, trg/src length ratio: 8.59 (+-3.95)
[2023-11-29:18:40:56:INFO:sockeye.data_io:describe_data_and_buckets] Bucket (112, 112): 11 samples in 1 batches of 16, ~1729.5 target tokens/batch, trg/src length ratio: 7.20 (+-3.68)
[2023-11-29:18:40:56:INFO:sockeye.data_io:describe_data_and_buckets] Bucket (120, 120): 7 samples in 1 batches of 16, ~1865.1 target tokens/batch, trg/src length ratio: 11.22 (+-2.16)
[2023-11-29:18:40:56:INFO:sockeye.data_io:describe_data_and_buckets] Bucket (128, 128): 6 samples in 1 batches of 16, ~2000.0 target tokens/batch, trg/src length ratio: 6.27 (+-4.74)
[2023-11-29:18:40:56:INFO:sockeye.data_io:describe_data_and_buckets] Bucket (136, 136): 5 samples in 1 batches of 16, ~2102.4 target tokens/batch, trg/src length ratio: 16.69 (+-2.85)
[2023-11-29:18:40:56:INFO:sockeye.data_io:describe_data_and_buckets] Bucket (144, 144): 4 samples in 1 batches of 16, ~2248.0 target tokens/batch, trg/src length ratio: 16.04 (+-4.14)
[2023-11-29:18:40:56:INFO:sockeye.data_io:describe_data_and_buckets] Bucket (160, 160): 2 samples in 1 batches of 16, ~2536.0 target tokens/batch, trg/src length ratio: 24.55 (+-2.12)
[2023-11-29:18:40:56:INFO:sockeye.data_io:describe_data_and_buckets] Bucket (176, 176): 2 samples in 1 batches of 8, ~1376.0 target tokens/batch, trg/src length ratio: 23.04 (+-1.67)
[2023-11-29:18:40:56:INFO:sockeye.data_io:describe_data_and_buckets] Bucket (184, 184): 3 samples in 1 batches of 8, ~1432.0 target tokens/batch, trg/src length ratio: 19.54 (+-2.72)
[2023-11-29:18:40:56:INFO:sockeye.data_io:describe_data_and_buckets] Bucket (192, 192): 5 samples in 1 batches of 8, ~1507.2 target tokens/batch, trg/src length ratio: 20.74 (+-4.80)
[2023-11-29:18:40:56:INFO:sockeye.data_io:_load_shard] Loading shard data_sockeye_new_expanded_cleaned/shard.00000.
[2023-11-29:18:40:57:INFO:sockeye.data_io:get_validation_data_iter] =================================
[2023-11-29:18:40:57:INFO:sockeye.data_io:get_validation_data_iter] Creating validation data iterator
[2023-11-29:18:40:57:INFO:sockeye.data_io:get_validation_data_iter] =================================
[2023-11-29:18:40:57:INFO:sockeye.data_io:analyze_sequence_lengths] 2845 sequences of maximum length (201, 201) in '/shares/volk.cl.uzh/zifjia/signbank-plus/signbank_plus/nmt/signwriting-translation/data_new_cleaned/dev.sign' and '/shares/volk.cl.uzh/zifjia/signbank-plus/signbank_plus/nmt/signwriting-translation/data_new_cleaned/dev.spm.spoken'.
[2023-11-29:18:40:57:INFO:sockeye.data_io:analyze_sequence_lengths] Mean training target/source length ratio: 0.55 (+-0.35)
[2023-11-29:18:40:58:INFO:sockeye.data_io:log] Tokens: source 34431 target 16961
[2023-11-29:18:40:58:INFO:sockeye.data_io:log] Number of <unk> tokens: source 6 target 1092
[2023-11-29:18:40:58:INFO:sockeye.data_io:log] Vocabulary coverage: source 100% target 94%
[2023-11-29:18:40:58:INFO:sockeye.data_io:log] 2845 sequences across 26 buckets
[2023-11-29:18:40:58:INFO:sockeye.data_io:log] 0 sequences did not fit into buckets and were discarded
[2023-11-29:18:40:58:INFO:sockeye.data_io:describe_data_and_buckets] Bucket (8, 8): 1175 samples in 3 batches of 496, ~2053.6 target tokens/batch, trg/src length ratio: 0.60 (+-0.22)
[2023-11-29:18:40:58:INFO:sockeye.data_io:describe_data_and_buckets] Bucket (16, 16): 1420 samples in 4 batches of 400, ~2043.4 target tokens/batch, trg/src length ratio: 0.51 (+-0.32)
[2023-11-29:18:40:58:INFO:sockeye.data_io:describe_data_and_buckets] Bucket (24, 24): 90 samples in 1 batches of 256, ~2037.3 target tokens/batch, trg/src length ratio: 0.53 (+-0.58)
[2023-11-29:18:40:58:INFO:sockeye.data_io:describe_data_and_buckets] Bucket (32, 32): 23 samples in 1 batches of 152, ~2071.8 target tokens/batch, trg/src length ratio: 1.01 (+-1.16)
[2023-11-29:18:40:58:INFO:sockeye.data_io:describe_data_and_buckets] Bucket (40, 40): 9 samples in 1 batches of 104, ~2005.4 target tokens/batch, trg/src length ratio: 0.98 (+-0.79)
[2023-11-29:18:40:58:INFO:sockeye.data_io:describe_data_and_buckets] Bucket (48, 48): 16 samples in 1 batches of 104, ~1992.5 target tokens/batch, trg/src length ratio: 1.08 (+-1.56)
[2023-11-29:18:40:58:INFO:sockeye.data_io:describe_data_and_buckets] Bucket (56, 56): 14 samples in 1 batches of 88, ~2028.9 target tokens/batch, trg/src length ratio: 0.35 (+-0.14)
[2023-11-29:18:40:58:INFO:sockeye.data_io:describe_data_and_buckets] Bucket (64, 64): 30 samples in 1 batches of 80, ~1973.1 target tokens/batch, trg/src length ratio: 0.44 (+-0.58)
[2023-11-29:18:40:58:INFO:sockeye.data_io:describe_data_and_buckets] Bucket (72, 72): 20 samples in 1 batches of 72, ~1961.3 target tokens/batch, trg/src length ratio: 0.66 (+-0.78)
[2023-11-29:18:40:58:INFO:sockeye.data_io:describe_data_and_buckets] Bucket (80, 80): 24 samples in 1 batches of 72, ~2134.8 target tokens/batch, trg/src length ratio: 0.38 (+-0.09)
[2023-11-29:18:40:58:INFO:sockeye.data_io:describe_data_and_buckets] Bucket (88, 88): 16 samples in 1 batches of 64, ~2058.0 target tokens/batch, trg/src length ratio: 0.37 (+-0.11)
[2023-11-29:18:40:58:INFO:sockeye.data_io:describe_data_and_buckets] Bucket (96, 96): 8 samples in 1 batches of 56, ~1962.4 target tokens/batch, trg/src length ratio: 0.36 (+-0.05)
[2023-11-29:18:40:58:INFO:sockeye.data_io:load] Created bucketed parallel data set. Introduced padding: source=22.4% target=61.8%)
[2023-11-29:18:40:58:INFO:__main__:train] Maximum source length determined by prepared data. Using 201 instead of 96
[2023-11-29:18:40:58:INFO:__main__:train] Maximum target length determined by prepared data. Using 201 instead of 96
[2023-11-29:18:40:58:INFO:sockeye.vocab:vocab_to_json] Vocabulary saved to "/shares/volk.cl.uzh/zifjia/signbank-plus/signbank_plus/nmt/signwriting-translation/models_new/expanded_cleaned/vocab.src.0.json"
[2023-11-29:18:40:58:INFO:sockeye.vocab:vocab_to_json] Vocabulary saved to "/shares/volk.cl.uzh/zifjia/signbank-plus/signbank_plus/nmt/signwriting-translation/models_new/expanded_cleaned/vocab.src.1.json"
[2023-11-29:18:40:58:INFO:sockeye.vocab:vocab_to_json] Vocabulary saved to "/shares/volk.cl.uzh/zifjia/signbank-plus/signbank_plus/nmt/signwriting-translation/models_new/expanded_cleaned/vocab.src.2.json"
[2023-11-29:18:40:58:INFO:sockeye.vocab:vocab_to_json] Vocabulary saved to "/shares/volk.cl.uzh/zifjia/signbank-plus/signbank_plus/nmt/signwriting-translation/models_new/expanded_cleaned/vocab.src.3.json"
[2023-11-29:18:40:58:INFO:sockeye.vocab:vocab_to_json] Vocabulary saved to "/shares/volk.cl.uzh/zifjia/signbank-plus/signbank_plus/nmt/signwriting-translation/models_new/expanded_cleaned/vocab.src.4.json"
[2023-11-29:18:40:58:INFO:sockeye.vocab:vocab_to_json] Vocabulary saved to "/shares/volk.cl.uzh/zifjia/signbank-plus/signbank_plus/nmt/signwriting-translation/models_new/expanded_cleaned/vocab.src.5.json"
[2023-11-29:18:40:58:INFO:sockeye.vocab:vocab_to_json] Vocabulary saved to "/shares/volk.cl.uzh/zifjia/signbank-plus/signbank_plus/nmt/signwriting-translation/models_new/expanded_cleaned/vocab.src.6.json"
[2023-11-29:18:40:58:INFO:sockeye.vocab:vocab_to_json] Vocabulary saved to "/shares/volk.cl.uzh/zifjia/signbank-plus/signbank_plus/nmt/signwriting-translation/models_new/expanded_cleaned/vocab.src.7.json"
[2023-11-29:18:40:58:INFO:sockeye.vocab:vocab_to_json] Vocabulary saved to "/shares/volk.cl.uzh/zifjia/signbank-plus/signbank_plus/nmt/signwriting-translation/models_new/expanded_cleaned/vocab.trg.0.json"
[2023-11-29:18:40:58:INFO:__main__:train] Vocabulary sizes: source=[15608|432|472|64|56|760|16|24] target=[4312]
[2023-11-29:18:40:58:INFO:__main__:get_num_embed] Source embedding size was not set it will automatically be adjusted to match the Transformer source model size (512).
[2023-11-29:18:40:58:INFO:__main__:get_num_embed] Target embedding size was not set it will automatically be adjusted to match the Transformer target model size (512).
[2023-11-29:18:40:58:INFO:__main__:create_encoder_config] Encoder transformer-model-size adjusted to account for source factor embeddings: 512 -> 624
[2023-11-29:18:40:58:INFO:__main__:create_optimizer_config] OptimizerConfig(name='adam', running_on_gpu=True, lr=0.0001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.0, momentum=0.0, gradient_clipping_type='none', gradient_clipping_threshold=1.0, update_interval=1)
[2023-11-29:18:40:58:INFO:__main__:create_optimizer_config] Gradient accumulation over 2 batch(es) by 1 worker(s). Effective batch size: 4096
[2023-11-29:18:40:58:INFO:sockeye.model:__init__] ModelConfig(config_data=DataConfig(data_statistics=DataStatistics(num_sents=354727, num_discarded=0, num_tokens_source=4201025, num_tokens_target=2057535, num_unks_source=167, num_unks_target=134832, max_observed_len_source=96, max_observed_len_target=192, size_vocab_source=15608, size_vocab_target=4312, length_ratio_mean=0.5463525593793229, length_ratio_std=0.4182879569055765, buckets=[(8, 8), (16, 16), (24, 24), (32, 32), (40, 40), (48, 48), (56, 56), (64, 64), (72, 72), (80, 80), (88, 88), (96, 96), (104, 104), (112, 112), (120, 120), (128, 128), (136, 136), (144, 144), (152, 152), (160, 160), (168, 168), (176, 176), (184, 184), (192, 192), (200, 200), (201, 201)], num_sents_per_bucket=[149216, 173121, 14560, 2725, 1750, 1479, 1589, 2145, 2565, 2457, 2308, 755, 12, 11, 7, 6, 5, 4, 0, 2, 0, 2, 3, 5, 0, 0], average_len_target_per_bucket=[4.140326774608547, 5.108531027431742, 7.958035714285693, 13.630458715596294, 19.282285714285702, 19.158215010142012, 23.056010069225927, 24.663869463869425, 27.240155945419072, 29.649572649572605, 32.15597920277298, 35.04238410596021, 100.0, 108.09090909090908, 116.57142857142857, 125.0, 131.4, 140.5, None, 158.5, None, 172.0, 179.0, 188.4, None, None], length_ratio_stats_per_bucket=[(0.5781455199493354, 0.20559989890762204), (0.5035479926045533, 0.31443786222138226), (0.5468974987608402, 0.6491670331407603), (0.91248259310849, 1.2010031762840225), (1.1660911955799533, 1.5511466062959094), (0.8187221782430846, 1.3467568304114719), (0.6641840418457079, 1.0642485351442208), (0.5038462754858931, 0.8043286661508741), (0.48069506489906394, 0.7869185585367217), (0.4669203885345722, 0.8762070208109582), (0.44644518448479037, 0.8098897847283998), (0.5449491437402029, 1.2820685486019436), (8.591055328871786, 3.9510322226387298), (7.201917367926802, 3.6758955236249236), (11.221567717996288, 2.1560031382015623), (6.269197912671014, 4.738615940455274), (16.691666666666666, 2.854761904761905), (16.043939393939393, 4.138753056115349), (None, None), (24.547619047619047, 2.1190476190476204), (None, None), (23.044642857142858, 1.6696428571428577), (19.53956228956229, 2.723775920544078), (20.744761904761905, 4.800873503286439), (None, None), (None, None)]), max_seq_len_source=201, max_seq_len_target=201, num_source_factors=8, num_target_factors=1, eop_id=-1), vocab_source_size=15608, vocab_target_size=4312, config_embed_source=EmbeddingConfig(vocab_size=15608, num_embed=512, dropout=0.5, num_factors=8, factor_configs=[FactorConfig(vocab_size=432, num_embed=16, combine='concat', share_embedding=False), FactorConfig(vocab_size=472, num_embed=16, combine='concat', share_embedding=False), FactorConfig(vocab_size=64, num_embed=16, combine='concat', share_embedding=False), FactorConfig(vocab_size=56, num_embed=16, combine='concat', share_embedding=False), FactorConfig(vocab_size=760, num_embed=16, combine='concat', share_embedding=False), FactorConfig(vocab_size=16, num_embed=16, combine='concat', share_embedding=False), FactorConfig(vocab_size=24, num_embed=16, combine='concat', share_embedding=False)], allow_sparse_grad=False), config_embed_target=EmbeddingConfig(vocab_size=4312, num_embed=512, dropout=0.5, num_factors=1, factor_configs=None, allow_sparse_grad=False), config_encoder=TransformerConfig(model_size=624, attention_heads=8, feed_forward_num_hidden=2048, act_type='relu', num_layers=6, dropout_attention=0.5, dropout_act=0.1, dropout_prepost=0.1, positional_embedding_type='fixed', preprocess_sequence='n', postprocess_sequence='dr', max_seq_len_source=201, max_seq_len_target=201, decoder_type='transformer', block_prepended_cross_attention=False, use_lhuc=False, depth_key_value=624, use_glu=False), config_decoder=TransformerConfig(model_size=512, attention_heads=8, feed_forward_num_hidden=2048, act_type='relu', num_layers=6, dropout_attention=0.5, dropout_act=0.1, dropout_prepost=0.1, positional_embedding_type='fixed', preprocess_sequence='n', postprocess_sequence='dr', max_seq_len_source=201, max_seq_len_target=201, decoder_type='transformer', block_prepended_cross_attention=False, use_lhuc=False, depth_key_value=624, use_glu=False), config_length_task=None, weight_tying_type='trg_softmax', lhuc=False, dtype='float32', neural_vocab_selection=None, neural_vocab_selection_block_loss=False)
[2023-11-29:18:41:03:INFO:sockeye.model:load_parameters] Loaded params from "models_new/expanded/params.best" to "cuda:0"
[2023-11-29:18:41:03:INFO:sockeye.utils:log_parameters] # of parameters: 61062344 | trainable: 60834008 (99.63%) | shared: 2212056 (3.62%) | fixed: 228336 (0.37%)
[2023-11-29:18:41:03:INFO:sockeye.utils:log_parameters] Trainable parameters: 
['embedding_source.embedding [(15608, 512), float32]',
 'embedding_source.factor_embeds.0 [(432, 16), float32]',
 'embedding_source.factor_embeds.1 [(472, 16), float32]',
 'embedding_source.factor_embeds.2 [(64, 16), float32]',
 'embedding_source.factor_embeds.3 [(56, 16), float32]',
 'embedding_source.factor_embeds.4 [(760, 16), float32]',
 'embedding_source.factor_embeds.5 [(16, 16), float32]',
 'embedding_source.factor_embeds.6 [(24, 16), float32]',
 'embedding_target.embedding [(4312, 512), float32]',
 'encoder.layers.0.pre_self_attention.layer_norm [(624,), float32]',
 'encoder.layers.0.pre_self_attention.layer_norm [(624,), float32]',
 'encoder.layers.0.self_attention.ff_out [(624, 624), float32]',
 'encoder.layers.0.self_attention.ff_in [(1872, 624), float32]',
 'encoder.layers.0.pre_ff.layer_norm [(624,), float32]',
 'encoder.layers.0.pre_ff.layer_norm [(624,), float32]',
 'encoder.layers.0.ff.ff1 [(2048, 624), float32]',
 'encoder.layers.0.ff.ff1 [(2048,), float32]',
 'encoder.layers.0.ff.ff2 [(624, 2048), float32]',
 'encoder.layers.0.ff.ff2 [(624,), float32]',
 'encoder.layers.1.pre_self_attention.layer_norm [(624,), float32]',
 'encoder.layers.1.pre_self_attention.layer_norm [(624,), float32]',
 'encoder.layers.1.self_attention.ff_out [(624, 624), float32]',
 'encoder.layers.1.self_attention.ff_in [(1872, 624), float32]',
 'encoder.layers.1.pre_ff.layer_norm [(624,), float32]',
 'encoder.layers.1.pre_ff.layer_norm [(624,), float32]',
 'encoder.layers.1.ff.ff1 [(2048, 624), float32]',
 'encoder.layers.1.ff.ff1 [(2048,), float32]',
 'encoder.layers.1.ff.ff2 [(624, 2048), float32]',
 'encoder.layers.1.ff.ff2 [(624,), float32]',
 'encoder.layers.2.pre_self_attention.layer_norm [(624,), float32]',
 'encoder.layers.2.pre_self_attention.layer_norm [(624,), float32]',
 'encoder.layers.2.self_attention.ff_out [(624, 624), float32]',
 'encoder.layers.2.self_attention.ff_in [(1872, 624), float32]',
 'encoder.layers.2.pre_ff.layer_norm [(624,), float32]',
 'encoder.layers.2.pre_ff.layer_norm [(624,), float32]',
 'encoder.layers.2.ff.ff1 [(2048, 624), float32]',
 'encoder.layers.2.ff.ff1 [(2048,), float32]',
 'encoder.layers.2.ff.ff2 [(624, 2048), float32]',
 'encoder.layers.2.ff.ff2 [(624,), float32]',
 'encoder.layers.3.pre_self_attention.layer_norm [(624,), float32]',
 'encoder.layers.3.pre_self_attention.layer_norm [(624,), float32]',
 'encoder.layers.3.self_attention.ff_out [(624, 624), float32]',
 'encoder.layers.3.self_attention.ff_in [(1872, 624), float32]',
 'encoder.layers.3.pre_ff.layer_norm [(624,), float32]',
 'encoder.layers.3.pre_ff.layer_norm [(624,), float32]',
 'encoder.layers.3.ff.ff1 [(2048, 624), float32]',
 'encoder.layers.3.ff.ff1 [(2048,), float32]',
 'encoder.layers.3.ff.ff2 [(624, 2048), float32]',
 'encoder.layers.3.ff.ff2 [(624,), float32]',
 'encoder.layers.4.pre_self_attention.layer_norm [(624,), float32]',
 'encoder.layers.4.pre_self_attention.layer_norm [(624,), float32]',
 'encoder.layers.4.self_attention.ff_out [(624, 624), float32]',
 'encoder.layers.4.self_attention.ff_in [(1872, 624), float32]',
 'encoder.layers.4.pre_ff.layer_norm [(624,), float32]',
 'encoder.layers.4.pre_ff.layer_norm [(624,), float32]',
 'encoder.layers.4.ff.ff1 [(2048, 624), float32]',
 'encoder.layers.4.ff.ff1 [(2048,), float32]',
 'encoder.layers.4.ff.ff2 [(624, 2048), float32]',
 'encoder.layers.4.ff.ff2 [(624,), float32]',
 'encoder.layers.5.pre_self_attention.layer_norm [(624,), float32]',
 'encoder.layers.5.pre_self_attention.layer_norm [(624,), float32]',
 'encoder.layers.5.self_attention.ff_out [(624, 624), float32]',
 'encoder.layers.5.self_attention.ff_in [(1872, 624), float32]',
 'encoder.layers.5.pre_ff.layer_norm [(624,), float32]',
 'encoder.layers.5.pre_ff.layer_norm [(624,), float32]',
 'encoder.layers.5.ff.ff1 [(2048, 624), float32]',
 'encoder.layers.5.ff.ff1 [(2048,), float32]',
 'encoder.layers.5.ff.ff2 [(624, 2048), float32]',
 'encoder.layers.5.ff.ff2 [(624,), float32]',
 'encoder.final_process.layer_norm [(624,), float32]',
 'encoder.final_process.layer_norm [(624,), float32]',
 'decoder.layers.0.autoregr_layer.ff_out [(512, 512), float32]',
 'decoder.layers.0.autoregr_layer.ff_in [(1536, 512), float32]',
 'decoder.layers.0.pre_autoregr_layer.layer_norm [(512,), float32]',
 'decoder.layers.0.pre_autoregr_layer.layer_norm [(512,), float32]',
 'decoder.layers.0.pre_enc_attention.layer_norm [(512,), float32]',
 'decoder.layers.0.pre_enc_attention.layer_norm [(512,), float32]',
 'decoder.layers.0.enc_attention.ff_out [(512, 512), float32]',
 'decoder.layers.0.enc_attention.ff_q [(512, 512), float32]',
 'decoder.layers.0.enc_attention.ff_kv [(1024, 624), float32]',
 'decoder.layers.0.pre_ff.layer_norm [(512,), float32]',
 'decoder.layers.0.pre_ff.layer_norm [(512,), float32]',
 'decoder.layers.0.ff.ff1 [(2048, 512), float32]',
 'decoder.layers.0.ff.ff1 [(2048,), float32]',
 'decoder.layers.0.ff.ff2 [(512, 2048), float32]',
 'decoder.layers.0.ff.ff2 [(512,), float32]',
 'decoder.layers.1.autoregr_layer.ff_out [(512, 512), float32]',
 'decoder.layers.1.autoregr_layer.ff_in [(1536, 512), float32]',
 'decoder.layers.1.pre_autoregr_layer.layer_norm [(512,), float32]',
 'decoder.layers.1.pre_autoregr_layer.layer_norm [(512,), float32]',
 'decoder.layers.1.pre_enc_attention.layer_norm [(512,), float32]',
 'decoder.layers.1.pre_enc_attention.layer_norm [(512,), float32]',
 'decoder.layers.1.enc_attention.ff_out [(512, 512), float32]',
 'decoder.layers.1.enc_attention.ff_q [(512, 512), float32]',
 'decoder.layers.1.enc_attention.ff_kv [(1024, 624), float32]',
 'decoder.layers.1.pre_ff.layer_norm [(512,), float32]',
 'decoder.layers.1.pre_ff.layer_norm [(512,), float32]',
 'decoder.layers.1.ff.ff1 [(2048, 512), float32]',
 'decoder.layers.1.ff.ff1 [(2048,), float32]',
 'decoder.layers.1.ff.ff2 [(512, 2048), float32]',
 'decoder.layers.1.ff.ff2 [(512,), float32]',
 'decoder.layers.2.autoregr_layer.ff_out [(512, 512), float32]',
 'decoder.layers.2.autoregr_layer.ff_in [(1536, 512), float32]',
 'decoder.layers.2.pre_autoregr_layer.layer_norm [(512,), float32]',
 'decoder.layers.2.pre_autoregr_layer.layer_norm [(512,), float32]',
 'decoder.layers.2.pre_enc_attention.layer_norm [(512,), float32]',
 'decoder.layers.2.pre_enc_attention.layer_norm [(512,), float32]',
 'decoder.layers.2.enc_attention.ff_out [(512, 512), float32]',
 'decoder.layers.2.enc_attention.ff_q [(512, 512), float32]',
 'decoder.layers.2.enc_attention.ff_kv [(1024, 624), float32]',
 'decoder.layers.2.pre_ff.layer_norm [(512,), float32]',
 'decoder.layers.2.pre_ff.layer_norm [(512,), float32]',
 'decoder.layers.2.ff.ff1 [(2048, 512), float32]',
 'decoder.layers.2.ff.ff1 [(2048,), float32]',
 'decoder.layers.2.ff.ff2 [(512, 2048), float32]',
 'decoder.layers.2.ff.ff2 [(512,), float32]',
 'decoder.layers.3.autoregr_layer.ff_out [(512, 512), float32]',
 'decoder.layers.3.autoregr_layer.ff_in [(1536, 512), float32]',
 'decoder.layers.3.pre_autoregr_layer.layer_norm [(512,), float32]',
 'decoder.layers.3.pre_autoregr_layer.layer_norm [(512,), float32]',
 'decoder.layers.3.pre_enc_attention.layer_norm [(512,), float32]',
 'decoder.layers.3.pre_enc_attention.layer_norm [(512,), float32]',
 'decoder.layers.3.enc_attention.ff_out [(512, 512), float32]',
 'decoder.layers.3.enc_attention.ff_q [(512, 512), float32]',
 'decoder.layers.3.enc_attention.ff_kv [(1024, 624), float32]',
 'decoder.layers.3.pre_ff.layer_norm [(512,), float32]',
 'decoder.layers.3.pre_ff.layer_norm [(512,), float32]',
 'decoder.layers.3.ff.ff1 [(2048, 512), float32]',
 'decoder.layers.3.ff.ff1 [(2048,), float32]',
 'decoder.layers.3.ff.ff2 [(512, 2048), float32]',
 'decoder.layers.3.ff.ff2 [(512,), float32]',
 'decoder.layers.4.autoregr_layer.ff_out [(512, 512), float32]',
 'decoder.layers.4.autoregr_layer.ff_in [(1536, 512), float32]',
 'decoder.layers.4.pre_autoregr_layer.layer_norm [(512,), float32]',
 'decoder.layers.4.pre_autoregr_layer.layer_norm [(512,), float32]',
 'decoder.layers.4.pre_enc_attention.layer_norm [(512,), float32]',
 'decoder.layers.4.pre_enc_attention.layer_norm [(512,), float32]',
 'decoder.layers.4.enc_attention.ff_out [(512, 512), float32]',
 'decoder.layers.4.enc_attention.ff_q [(512, 512), float32]',
 'decoder.layers.4.enc_attention.ff_kv [(1024, 624), float32]',
 'decoder.layers.4.pre_ff.layer_norm [(512,), float32]',
 'decoder.layers.4.pre_ff.layer_norm [(512,), float32]',
 'decoder.layers.4.ff.ff1 [(2048, 512), float32]',
 'decoder.layers.4.ff.ff1 [(2048,), float32]',
 'decoder.layers.4.ff.ff2 [(512, 2048), float32]',
 'decoder.layers.4.ff.ff2 [(512,), float32]',
 'decoder.layers.5.autoregr_layer.ff_out [(512, 512), float32]',
 'decoder.layers.5.autoregr_layer.ff_in [(1536, 512), float32]',
 'decoder.layers.5.pre_autoregr_layer.layer_norm [(512,), float32]',
 'decoder.layers.5.pre_autoregr_layer.layer_norm [(512,), float32]',
 'decoder.layers.5.pre_enc_attention.layer_norm [(512,), float32]',
 'decoder.layers.5.pre_enc_attention.layer_norm [(512,), float32]',
 'decoder.layers.5.enc_attention.ff_out [(512, 512), float32]',
 'decoder.layers.5.enc_attention.ff_q [(512, 512), float32]',
 'decoder.layers.5.enc_attention.ff_kv [(1024, 624), float32]',
 'decoder.layers.5.pre_ff.layer_norm [(512,), float32]',
 'decoder.layers.5.pre_ff.layer_norm [(512,), float32]',
 'decoder.layers.5.ff.ff1 [(2048, 512), float32]',
 'decoder.layers.5.ff.ff1 [(2048,), float32]',
 'decoder.layers.5.ff.ff2 [(512, 2048), float32]',
 'decoder.layers.5.ff.ff2 [(512,), float32]',
 'decoder.final_process.layer_norm [(512,), float32]',
 'decoder.final_process.layer_norm [(512,), float32]',
 'output_layer [(4312, 512), float32]',
 'output_layer [(4312,), float32]',
 'output_layer_module_cached [(4312, 512), float32]',
 'output_layer_module_cached [(4312,), float32]',
 'output_layer_script_cached [(4312, 512), float32]',
 'output_layer_script_cached [(4312,), float32]']
[2023-11-29:18:41:03:INFO:sockeye.utils:log_parameters] Shared parameters: 
['embedding_target.embedding.weight = output_layer.weight = output_layer_module_cached.weight = '
 'output_layer_script_cached.weight',
 'output_layer.bias = output_layer_module_cached.bias = output_layer_script_cached.bias']
[2023-11-29:18:41:03:INFO:sockeye.utils:log_parameters] Fixed parameters:
['encoder.pos_embedding [(201, 624), float32]',
 'decoder.pos_embedding [(201, 512), float32]']
[2023-11-29:18:41:03:INFO:sockeye.loss:__init__] Loss: cross-entropy | weight=1.00 | metric: perplexity (ppl) | output_name: 'logits' | label_name: 'target_label'
[2023-11-29:18:41:03:WARNING:sockeye.optimizers:get_optimizer] Cannot import NVIDIA Apex optimizers (FusedAdam, FusedSGD). Consider installing Apex for faster GPU training: https://github.com/NVIDIA/apex
[2023-11-29:18:41:03:INFO:sockeye.lr_scheduler:__init__] Will reduce the learning rate by a factor of 0.70 whenever the validation score doesn't improve 5 times.
[2023-11-29:18:41:03:INFO:__main__:train] Tracing SockeyeModel on a validation batch
[2023-11-29:18:41:06:INFO:sockeye.training:__init__] Logging training events for Tensorboard at 'models_new/expanded_cleaned/tensorboard'
[2023-11-29:18:41:06:INFO:sockeye.inference:__init__] Translator (1 model(s) beam_size=5 algorithm=BeamSearch, beam_search_stop=all max_input_length=200 nbest_size=1 ensemble_mode=None max_batch_size=16 dtype=torch.float32 skip_nvs=False nvs_thresh=0.5)
[2023-11-29:18:41:06:INFO:sockeye.checkpoint_decoder:__init__] Created CheckpointDecoder(max_input_len=-1, beam_size=5, num_sentences=500)
[2023-11-29:18:41:29:INFO:sockeye.training:fit] Early stopping by optimizing 'bleu'
[2023-11-29:18:41:29:INFO:sockeye.model:save_config] Saved model config to "models_new/expanded_cleaned/config"
[2023-11-29:18:41:29:INFO:root:save_parameters] Saved params/state_dict to "models_new/expanded_cleaned/params.00000"
[2023-11-29:18:41:29:INFO:sockeye.training:fit] Training started.
[2023-11-29:18:42:40:INFO:sockeye.training:__call__] E=0 B=50	s/sec=245.43 tok/sec=3703.73 u/sec=0.35	ppl=77.583306 
[2023-11-29:18:43:04:INFO:sockeye.training:__call__] E=0 B=100	s/sec=713.96 tok/sec=11279.93 u/sec=1.06	ppl=72.166757 
[2023-11-29:18:43:27:INFO:sockeye.training:__call__] E=0 B=150	s/sec=828.92 tok/sec=11391.96 u/sec=1.11	ppl=67.885000 
[2023-11-29:18:43:51:INFO:sockeye.training:__call__] E=0 B=200	s/sec=735.14 tok/sec=11264.09 u/sec=1.03	ppl=65.033709 
[2023-11-29:18:44:16:INFO:sockeye.training:__call__] E=0 B=250	s/sec=675.27 tok/sec=11138.34 u/sec=0.99	ppl=62.513170 
[2023-11-29:18:44:40:INFO:sockeye.training:__call__] E=0 B=300	s/sec=693.33 tok/sec=10947.73 u/sec=1.03	ppl=59.927572 
[2023-11-29:18:45:05:INFO:sockeye.training:__call__] E=0 B=350	s/sec=769.46 tok/sec=10903.88 u/sec=1.04	ppl=57.404241 
[2023-11-29:18:45:29:INFO:sockeye.training:__call__] E=0 B=400	s/sec=726.78 tok/sec=10893.08 u/sec=1.03	ppl=55.471706 
[2023-11-29:18:45:53:INFO:sockeye.training:__call__] E=0 B=450	s/sec=696.58 tok/sec=10917.51 u/sec=1.03	ppl=53.892195 
[2023-11-29:18:46:18:INFO:sockeye.training:__call__] E=0 B=500	s/sec=693.37 tok/sec=10955.25 u/sec=0.99	ppl=52.763889 
[2023-11-29:18:46:42:INFO:sockeye.training:__call__] E=0 B=550	s/sec=747.83 tok/sec=10886.65 u/sec=1.04	ppl=51.546537 
[2023-11-29:18:47:07:INFO:sockeye.training:__call__] E=0 B=600	s/sec=715.37 tok/sec=10861.91 u/sec=1.01	ppl=50.548160 
[2023-11-29:18:47:32:INFO:sockeye.training:__call__] E=0 B=650	s/sec=708.99 tok/sec=10901.88 u/sec=1.00	ppl=49.731080 
[2023-11-29:18:47:57:INFO:sockeye.training:__call__] E=0 B=700	s/sec=721.23 tok/sec=10929.26 u/sec=0.99	ppl=48.999723 
[2023-11-29:18:48:21:INFO:sockeye.training:__call__] E=0 B=750	s/sec=665.07 tok/sec=10832.63 u/sec=1.05	ppl=48.338089 
[2023-11-29:18:48:46:INFO:sockeye.training:__call__] E=0 B=800	s/sec=621.62 tok/sec=10867.82 u/sec=1.00	ppl=47.667302 
[2023-11-29:18:49:12:INFO:sockeye.training:__call__] E=0 B=850	s/sec=638.42 tok/sec=10911.81 u/sec=0.97	ppl=47.128913 
[2023-11-29:18:49:37:INFO:sockeye.training:__call__] E=0 B=900	s/sec=699.26 tok/sec=10854.27 u/sec=1.01	ppl=46.608893 
[2023-11-29:18:50:01:INFO:sockeye.training:__call__] E=0 B=950	s/sec=760.95 tok/sec=10929.38 u/sec=1.00	ppl=46.113327 
[2023-11-29:18:50:26:INFO:sockeye.training:__call__] E=0 B=1000	s/sec=705.43 tok/sec=10899.16 u/sec=1.01	ppl=45.626002 
[2023-11-29:18:50:51:INFO:sockeye.training:__call__] E=1 B=1050	s/sec=720.57 tok/sec=10796.59 u/sec=1.01	ppl=45.111476 
[2023-11-29:18:51:15:INFO:sockeye.training:__call__] E=1 B=1100	s/sec=744.42 tok/sec=10854.28 u/sec=1.05	ppl=44.580948 
[2023-11-29:18:51:39:INFO:sockeye.training:__call__] E=1 B=1150	s/sec=779.98 tok/sec=10894.69 u/sec=1.03	ppl=44.166280 
[2023-11-29:18:52:04:INFO:sockeye.training:__call__] E=1 B=1200	s/sec=699.20 tok/sec=10876.49 u/sec=1.01	ppl=43.718727 
[2023-11-29:18:52:29:INFO:sockeye.training:__call__] E=1 B=1250	s/sec=686.24 tok/sec=10909.01 u/sec=0.99	ppl=43.313690 
[2023-11-29:18:52:54:INFO:sockeye.training:__call__] E=1 B=1300	s/sec=679.43 tok/sec=10886.21 u/sec=1.00	ppl=42.928248 
[2023-11-29:18:53:18:INFO:sockeye.training:__call__] E=1 B=1350	s/sec=773.54 tok/sec=10852.24 u/sec=1.04	ppl=42.551724 
[2023-11-29:18:53:42:INFO:sockeye.training:__call__] E=1 B=1400	s/sec=754.77 tok/sec=10936.73 u/sec=1.03	ppl=42.288157 
[2023-11-29:18:54:06:INFO:sockeye.training:__call__] E=1 B=1450	s/sec=712.04 tok/sec=10861.81 u/sec=1.05	ppl=41.933845 
[2023-11-29:18:54:31:INFO:sockeye.training:__call__] E=1 B=1500	s/sec=744.00 tok/sec=10916.33 u/sec=1.00	ppl=41.640954 
[2023-11-29:18:54:57:INFO:sockeye.training:__call__] E=1 B=1550	s/sec=626.72 tok/sec=10908.74 u/sec=0.97	ppl=41.355265 
[2023-11-29:18:55:21:INFO:sockeye.training:__call__] E=1 B=1600	s/sec=796.51 tok/sec=10934.84 u/sec=1.03	ppl=41.124277 
[2023-11-29:18:55:46:INFO:sockeye.training:__call__] E=1 B=1650	s/sec=698.67 tok/sec=10899.02 u/sec=1.00	ppl=40.862207 
