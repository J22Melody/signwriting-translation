!ModelConfig
config_data: !DataConfig
  data_statistics: !DataStatistics
    average_len_target_per_bucket:
    - 4.560769632097103
    - 6.116364671431675
    - 13.678161824528276
    - 24.99246820595127
    - 30.49802801577584
    - 34.58581280788183
    - 30.352393617021267
    - 36.05333017302684
    - 36.97938860583601
    - 35.04584915897988
    - 34.59580291970818
    - 48.097156398104275
    - 99.95693779904305
    - 108.92777777777776
    - 115.93150684931507
    - 124.39361702127657
    - 132.3513513513514
    - 140.03999999999996
    - 148.3148148148148
    - 156.39215686274517
    - 164.15384615384616
    - 172.85294117647058
    - 181.60714285714286
    - 187.875
    - 196.81818181818178
    - 201.0
    buckets:
    - !!python/tuple
      - 8
      - 8
    - !!python/tuple
      - 16
      - 16
    - !!python/tuple
      - 24
      - 24
    - !!python/tuple
      - 32
      - 32
    - !!python/tuple
      - 40
      - 40
    - !!python/tuple
      - 48
      - 48
    - !!python/tuple
      - 56
      - 56
    - !!python/tuple
      - 64
      - 64
    - !!python/tuple
      - 72
      - 72
    - !!python/tuple
      - 80
      - 80
    - !!python/tuple
      - 88
      - 88
    - !!python/tuple
      - 96
      - 96
    - !!python/tuple
      - 104
      - 104
    - !!python/tuple
      - 112
      - 112
    - !!python/tuple
      - 120
      - 120
    - !!python/tuple
      - 128
      - 128
    - !!python/tuple
      - 136
      - 136
    - !!python/tuple
      - 144
      - 144
    - !!python/tuple
      - 152
      - 152
    - !!python/tuple
      - 160
      - 160
    - !!python/tuple
      - 168
      - 168
    - !!python/tuple
      - 176
      - 176
    - !!python/tuple
      - 184
      - 184
    - !!python/tuple
      - 192
      - 192
    - !!python/tuple
      - 200
      - 200
    - !!python/tuple
      - 201
      - 201
    length_ratio_mean: 0.8817726678584379
    length_ratio_stats_per_bucket:
    - !!python/tuple
      - 0.6360488074693902
      - 0.23141942350105316
    - !!python/tuple
      - 0.6284959092309799
      - 0.427861695012509
    - !!python/tuple
      - 1.3044594484301466
      - 1.0459474608347652
    - !!python/tuple
      - 2.5471913935186548
      - 1.2407721697645155
    - !!python/tuple
      - 2.944289167007739
      - 1.684712326385672
    - !!python/tuple
      - 3.1966584185031435
      - 2.3634426354487807
    - !!python/tuple
      - 2.2684539489378492
      - 2.707677461641542
    - !!python/tuple
      - 2.64940974077188
      - 3.171077650744171
    - !!python/tuple
      - 2.296919737603547
      - 3.2683886561015356
    - !!python/tuple
      - 1.6011553037571746
      - 2.9650444779127145
    - !!python/tuple
      - 1.2393492735315819
      - 2.790927670861211
    - !!python/tuple
      - 3.078445390065673
      - 4.690909759448575
    - !!python/tuple
      - 10.20045102468786
      - 4.473579667552742
    - !!python/tuple
      - 11.106200722356236
      - 3.853642106091223
    - !!python/tuple
      - 11.786693709653786
      - 3.957448693794891
    - !!python/tuple
      - 13.499236178411433
      - 4.396651652075544
    - !!python/tuple
      - 12.888173054572952
      - 5.304531294571493
    - !!python/tuple
      - 14.880456422318652
      - 4.091081890483121
    - !!python/tuple
      - 15.418422620379651
      - 5.821644534325687
    - !!python/tuple
      - 18.135535373251642
      - 5.5458424736469105
    - !!python/tuple
      - 19.57152693544096
      - 5.679287470674945
    - !!python/tuple
      - 19.51440871220283
      - 5.840272013854848
    - !!python/tuple
      - 19.960049099802006
      - 7.204426599571264
    - !!python/tuple
      - 18.04052907249985
      - 5.958775261003661
    - !!python/tuple
      - 18.21732402913095
      - 6.525886302161232
    - !!python/tuple
      - 10.578947368421053
      - 0.0
    length_ratio_std: 1.2007686809002125
    max_observed_len_source: 97
    max_observed_len_target: 201
    num_discarded: 91
    num_sents: 518462
    num_sents_per_bucket:
    - 174421
    - 256461
    - 38103
    - 16198
    - 7353
    - 5075
    - 3008
    - 4219
    - 4318
    - 3686
    - 3288
    - 1266
    - 209
    - 180
    - 146
    - 94
    - 74
    - 75
    - 54
    - 51
    - 39
    - 34
    - 28
    - 48
    - 33
    - 1
    num_tokens_source: 6064118
    num_tokens_target: 4535269
    num_unks_source: 0
    num_unks_target: 0
    size_vocab_source: 16312
    size_vocab_target: 3872
  eop_id: -1
  max_seq_len_source: 201
  max_seq_len_target: 201
  num_source_factors: 8
  num_target_factors: 1
config_decoder: !TransformerConfig
  act_type: relu
  attention_heads: 8
  block_prepended_cross_attention: false
  decoder_type: transformer
  depth_key_value: 624
  dropout_act: 0.1
  dropout_attention: 0.5
  dropout_prepost: 0.1
  feed_forward_num_hidden: 2048
  max_seq_len_source: 201
  max_seq_len_target: 201
  model_size: 512
  num_layers: 6
  positional_embedding_type: fixed
  postprocess_sequence: dr
  preprocess_sequence: n
  use_glu: false
  use_lhuc: false
config_embed_source: !EmbeddingConfig
  allow_sparse_grad: false
  dropout: 0.5
  factor_configs:
  - !FactorConfig
    combine: concat
    num_embed: 16
    share_embedding: false
    vocab_size: 432
  - !FactorConfig
    combine: concat
    num_embed: 16
    share_embedding: false
    vocab_size: 480
  - !FactorConfig
    combine: concat
    num_embed: 16
    share_embedding: false
    vocab_size: 64
  - !FactorConfig
    combine: concat
    num_embed: 16
    share_embedding: false
    vocab_size: 56
  - !FactorConfig
    combine: concat
    num_embed: 16
    share_embedding: false
    vocab_size: 768
  - !FactorConfig
    combine: concat
    num_embed: 16
    share_embedding: false
    vocab_size: 16
  - !FactorConfig
    combine: concat
    num_embed: 16
    share_embedding: false
    vocab_size: 24
  num_embed: 512
  num_factors: 8
  vocab_size: 16312
config_embed_target: !EmbeddingConfig
  allow_sparse_grad: false
  dropout: 0.5
  factor_configs: null
  num_embed: 512
  num_factors: 1
  vocab_size: 3872
config_encoder: !TransformerConfig
  act_type: relu
  attention_heads: 8
  block_prepended_cross_attention: false
  decoder_type: transformer
  depth_key_value: 624
  dropout_act: 0.1
  dropout_attention: 0.5
  dropout_prepost: 0.1
  feed_forward_num_hidden: 2048
  max_seq_len_source: 201
  max_seq_len_target: 201
  model_size: 624
  num_layers: 6
  positional_embedding_type: fixed
  postprocess_sequence: dr
  preprocess_sequence: n
  use_glu: false
  use_lhuc: false
config_length_task: null
dtype: float32
lhuc: false
neural_vocab_selection: null
neural_vocab_selection_block_loss: false
vocab_source_size: 16312
vocab_target_size: 3872
weight_tying_type: trg_softmax
