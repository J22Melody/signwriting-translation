!ModelConfig
config_data: !DataConfig
  data_statistics: !DataStatistics
    average_len_target_per_bucket:
    - 4.135334972327465
    - 4.828063512662262
    - 6.719319619511736
    - 10.901819430140748
    - 15.063989108236939
    - 19.350240489724552
    - 21.124265702666065
    - 23.9830935251798
    - 27.085289256198276
    - 29.634281748785504
    - 32.692785104732316
    - 36.42265529841655
    - 100.43333333333332
    - 108.6923076923077
    - 116.25
    - 125.0
    - 132.10000000000002
    - 140.83333333333331
    - 148.0
    - 155.5
    - 162.5
    - 172.0
    - 180.33333333333334
    - 188.5
    - 194.5
    - null
    buckets:
    - !!python/tuple
      - 8
      - 8
    - !!python/tuple
      - 16
      - 16
    - !!python/tuple
      - 24
      - 24
    - !!python/tuple
      - 32
      - 32
    - !!python/tuple
      - 40
      - 40
    - !!python/tuple
      - 48
      - 48
    - !!python/tuple
      - 56
      - 56
    - !!python/tuple
      - 64
      - 64
    - !!python/tuple
      - 72
      - 72
    - !!python/tuple
      - 80
      - 80
    - !!python/tuple
      - 88
      - 88
    - !!python/tuple
      - 96
      - 96
    - !!python/tuple
      - 104
      - 104
    - !!python/tuple
      - 112
      - 112
    - !!python/tuple
      - 120
      - 120
    - !!python/tuple
      - 128
      - 128
    - !!python/tuple
      - 136
      - 136
    - !!python/tuple
      - 144
      - 144
    - !!python/tuple
      - 152
      - 152
    - !!python/tuple
      - 160
      - 160
    - !!python/tuple
      - 168
      - 168
    - !!python/tuple
      - 176
      - 176
    - !!python/tuple
      - 184
      - 184
    - !!python/tuple
      - 192
      - 192
    - !!python/tuple
      - 200
      - 200
    - !!python/tuple
      - 201
      - 201
    length_ratio_mean: 0.5186323896452135
    length_ratio_stats_per_bucket:
    - !!python/tuple
      - 0.5743623539489789
      - 0.20104410991666757
    - !!python/tuple
      - 0.47035279153532145
      - 0.27928194684761937
    - !!python/tuple
      - 0.4412524134371495
      - 0.5539074058401151
    - !!python/tuple
      - 0.6966882124391869
      - 1.084042180233606
    - !!python/tuple
      - 0.8326263581144604
      - 1.3483938006916854
    - !!python/tuple
      - 0.9673584898436735
      - 1.5824764061641485
    - !!python/tuple
      - 0.6466763175948788
      - 1.1761397022444244
    - !!python/tuple
      - 0.5263895420628171
      - 0.8972160157748982
    - !!python/tuple
      - 0.49829128091250935
      - 0.9283455282499427
    - !!python/tuple
      - 0.509314659994052
      - 1.0776079585192622
    - !!python/tuple
      - 0.49088849177952265
      - 1.0571204269381713
    - !!python/tuple
      - 0.6757536295125441
      - 1.6419251974397497
    - !!python/tuple
      - 9.39581347857291
      - 3.2462817357485196
    - !!python/tuple
      - 8.822836591719108
      - 4.477978162406543
    - !!python/tuple
      - 10.956883330485304
      - 3.292236881457392
    - !!python/tuple
      - 9.332937638135423
      - 5.926444735884293
    - !!python/tuple
      - 15.34
      - 4.717874275476605
    - !!python/tuple
      - 15.866666666666667
      - 2.7801253968521316
    - !!python/tuple
      - 24.666666666666668
      - 0.0
    - !!python/tuple
      - 17.358405483405484
      - 4.273141659431777
    - !!python/tuple
      - 25.154761904761905
      - 2.0119047619047628
    - !!python/tuple
      - 19.82638888888889
      - 1.8016116113491685
    - !!python/tuple
      - 24.100288600288604
      - 5.78348398763817
    - !!python/tuple
      - 25.232142857142858
      - 1.4821428571428577
    - !!python/tuple
      - 22.041666666666664
      - 5.958333333333335
    - !!python/tuple
      - null
      - null
    length_ratio_std: 0.3554390833055887
    max_observed_len_source: 96
    max_observed_len_target: 196
    num_discarded: 2
    num_sents: 1024500
    num_sents_per_bucket:
    - 432379
    - 523990
    - 42682
    - 5826
    - 2938
    - 2287
    - 2213
    - 2780
    - 3025
    - 2882
    - 2578
    - 821
    - 30
    - 13
    - 8
    - 10
    - 10
    - 6
    - 1
    - 6
    - 2
    - 4
    - 3
    - 2
    - 4
    - 0
    num_tokens_source: 10901580
    num_tokens_target: 5164216
    num_unks_source: 0
    num_unks_target: 0
    size_vocab_source: 15608
    size_vocab_target: 4312
  eop_id: -1
  max_seq_len_source: 201
  max_seq_len_target: 201
  num_source_factors: 8
  num_target_factors: 1
config_decoder: !TransformerConfig
  act_type: relu
  attention_heads: 8
  block_prepended_cross_attention: false
  decoder_type: transformer
  depth_key_value: 624
  dropout_act: 0.1
  dropout_attention: 0.5
  dropout_prepost: 0.1
  feed_forward_num_hidden: 2048
  max_seq_len_source: 201
  max_seq_len_target: 201
  model_size: 512
  num_layers: 6
  positional_embedding_type: fixed
  postprocess_sequence: dr
  preprocess_sequence: n
  use_glu: false
  use_lhuc: false
config_embed_source: !EmbeddingConfig
  allow_sparse_grad: false
  dropout: 0.5
  factor_configs:
  - !FactorConfig
    combine: concat
    num_embed: 16
    share_embedding: false
    vocab_size: 432
  - !FactorConfig
    combine: concat
    num_embed: 16
    share_embedding: false
    vocab_size: 472
  - !FactorConfig
    combine: concat
    num_embed: 16
    share_embedding: false
    vocab_size: 64
  - !FactorConfig
    combine: concat
    num_embed: 16
    share_embedding: false
    vocab_size: 56
  - !FactorConfig
    combine: concat
    num_embed: 16
    share_embedding: false
    vocab_size: 760
  - !FactorConfig
    combine: concat
    num_embed: 16
    share_embedding: false
    vocab_size: 16
  - !FactorConfig
    combine: concat
    num_embed: 16
    share_embedding: false
    vocab_size: 24
  num_embed: 512
  num_factors: 8
  vocab_size: 15608
config_embed_target: !EmbeddingConfig
  allow_sparse_grad: false
  dropout: 0.5
  factor_configs: null
  num_embed: 512
  num_factors: 1
  vocab_size: 4312
config_encoder: !TransformerConfig
  act_type: relu
  attention_heads: 8
  block_prepended_cross_attention: false
  decoder_type: transformer
  depth_key_value: 624
  dropout_act: 0.1
  dropout_attention: 0.5
  dropout_prepost: 0.1
  feed_forward_num_hidden: 2048
  max_seq_len_source: 201
  max_seq_len_target: 201
  model_size: 624
  num_layers: 6
  positional_embedding_type: fixed
  postprocess_sequence: dr
  preprocess_sequence: n
  use_glu: false
  use_lhuc: false
config_length_task: null
dtype: float32
lhuc: false
neural_vocab_selection: null
neural_vocab_selection_block_loss: false
vocab_source_size: 15608
vocab_target_size: 4312
weight_tying_type: trg_softmax
