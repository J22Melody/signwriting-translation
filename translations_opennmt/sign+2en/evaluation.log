Requirement already satisfied: OpenNMT-py in /net/cephfs/home/zifjia/.local/lib/python3.8/site-packages (2.2.0)
Requirement already satisfied: flask in /net/cephfs/data/zifjia/condaenvs/opennmt/lib/python3.8/site-packages (from OpenNMT-py) (2.0.2)
Requirement already satisfied: tensorboard>=2.3 in /net/cephfs/home/zifjia/.local/lib/python3.8/site-packages (from OpenNMT-py) (2.6.0)
Requirement already satisfied: pyyaml in /net/cephfs/data/zifjia/condaenvs/opennmt/lib/python3.8/site-packages (from OpenNMT-py) (6.0)
Requirement already satisfied: torchtext==0.5.0 in /net/cephfs/home/zifjia/.local/lib/python3.8/site-packages (from OpenNMT-py) (0.5.0)
Requirement already satisfied: torch>=1.6.0 in /net/cephfs/home/zifjia/.local/lib/python3.8/site-packages (from OpenNMT-py) (1.10.0)
Requirement already satisfied: configargparse in /net/cephfs/home/zifjia/.local/lib/python3.8/site-packages (from OpenNMT-py) (1.5.3)
Requirement already satisfied: pyonmttok<2,>=1.23 in /net/cephfs/home/zifjia/.local/lib/python3.8/site-packages (from OpenNMT-py) (1.29.0)
Requirement already satisfied: waitress in /net/cephfs/home/zifjia/.local/lib/python3.8/site-packages (from OpenNMT-py) (2.0.0)
Requirement already satisfied: numpy in /net/cephfs/home/zifjia/.local/lib/python3.8/site-packages (from torchtext==0.5.0->OpenNMT-py) (1.19.5)
Requirement already satisfied: tqdm in /net/cephfs/home/zifjia/.local/lib/python3.8/site-packages (from torchtext==0.5.0->OpenNMT-py) (4.62.2)
Requirement already satisfied: six in /net/cephfs/home/zifjia/.local/lib/python3.8/site-packages (from torchtext==0.5.0->OpenNMT-py) (1.12.0)
Requirement already satisfied: requests in /net/cephfs/home/zifjia/.local/lib/python3.8/site-packages (from torchtext==0.5.0->OpenNMT-py) (2.26.0)
Requirement already satisfied: sentencepiece in /net/cephfs/home/zifjia/.local/lib/python3.8/site-packages (from torchtext==0.5.0->OpenNMT-py) (0.1.96)
Requirement already satisfied: absl-py>=0.4 in /net/cephfs/home/zifjia/.local/lib/python3.8/site-packages (from tensorboard>=2.3->OpenNMT-py) (0.12.0)
Requirement already satisfied: wheel>=0.26 in /net/cephfs/data/zifjia/condaenvs/opennmt/lib/python3.8/site-packages (from tensorboard>=2.3->OpenNMT-py) (0.37.0)
Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /net/cephfs/home/zifjia/.local/lib/python3.8/site-packages (from tensorboard>=2.3->OpenNMT-py) (0.4.6)
Requirement already satisfied: markdown>=2.6.8 in /net/cephfs/home/zifjia/.local/lib/python3.8/site-packages (from tensorboard>=2.3->OpenNMT-py) (3.3.4)
Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /net/cephfs/home/zifjia/.local/lib/python3.8/site-packages (from tensorboard>=2.3->OpenNMT-py) (0.6.1)
Requirement already satisfied: werkzeug>=0.11.15 in /net/cephfs/home/zifjia/.local/lib/python3.8/site-packages (from tensorboard>=2.3->OpenNMT-py) (2.0.1)
Requirement already satisfied: google-auth<2,>=1.6.3 in /net/cephfs/home/zifjia/.local/lib/python3.8/site-packages (from tensorboard>=2.3->OpenNMT-py) (1.35.0)
Requirement already satisfied: grpcio>=1.24.3 in /net/cephfs/home/zifjia/.local/lib/python3.8/site-packages (from tensorboard>=2.3->OpenNMT-py) (1.40.0)
Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /net/cephfs/home/zifjia/.local/lib/python3.8/site-packages (from tensorboard>=2.3->OpenNMT-py) (1.8.0)
Requirement already satisfied: setuptools>=41.0.0 in /net/cephfs/data/zifjia/condaenvs/opennmt/lib/python3.8/site-packages (from tensorboard>=2.3->OpenNMT-py) (58.2.0)
Requirement already satisfied: protobuf>=3.6.0 in /net/cephfs/home/zifjia/.local/lib/python3.8/site-packages (from tensorboard>=2.3->OpenNMT-py) (3.18.0)
Requirement already satisfied: typing-extensions in /net/cephfs/data/zifjia/condaenvs/opennmt/lib/python3.8/site-packages (from torch>=1.6.0->OpenNMT-py) (3.10.0.2)
Requirement already satisfied: Jinja2>=3.0 in /net/cephfs/data/zifjia/condaenvs/opennmt/lib/python3.8/site-packages (from flask->OpenNMT-py) (3.0.2)
Requirement already satisfied: click>=7.1.2 in /net/cephfs/home/zifjia/.local/lib/python3.8/site-packages (from flask->OpenNMT-py) (8.0.1)
Requirement already satisfied: itsdangerous>=2.0 in /net/cephfs/data/zifjia/condaenvs/opennmt/lib/python3.8/site-packages (from flask->OpenNMT-py) (2.0.1)
Requirement already satisfied: pyasn1-modules>=0.2.1 in /net/cephfs/home/zifjia/.local/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (0.2.8)
Requirement already satisfied: rsa<5,>=3.1.4 in /net/cephfs/home/zifjia/.local/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (4.7.2)
Requirement already satisfied: cachetools<5.0,>=2.0.0 in /net/cephfs/home/zifjia/.local/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (4.2.2)
Requirement already satisfied: requests-oauthlib>=0.7.0 in /net/cephfs/home/zifjia/.local/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.3->OpenNMT-py) (1.3.0)
Requirement already satisfied: MarkupSafe>=2.0 in /net/cephfs/data/zifjia/condaenvs/opennmt/lib/python3.8/site-packages (from Jinja2>=3.0->flask->OpenNMT-py) (2.0.1)
Requirement already satisfied: certifi>=2017.4.17 in /net/cephfs/home/zifjia/.local/lib/python3.8/site-packages (from requests->torchtext==0.5.0->OpenNMT-py) (2021.5.30)
Requirement already satisfied: charset-normalizer~=2.0.0 in /net/cephfs/home/zifjia/.local/lib/python3.8/site-packages (from requests->torchtext==0.5.0->OpenNMT-py) (2.0.6)
Requirement already satisfied: urllib3<1.27,>=1.21.1 in /net/cephfs/home/zifjia/.local/lib/python3.8/site-packages (from requests->torchtext==0.5.0->OpenNMT-py) (1.26.6)
Requirement already satisfied: idna<4,>=2.5 in /net/cephfs/home/zifjia/.local/lib/python3.8/site-packages (from requests->torchtext==0.5.0->OpenNMT-py) (3.2)
Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /net/cephfs/home/zifjia/.local/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (0.4.8)
Requirement already satisfied: oauthlib>=3.0.0 in /net/cephfs/home/zifjia/.local/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.3->OpenNMT-py) (3.1.1)
# Translating with checkpoint /net/cephfs/home/zifjia/signwriting-translation/./scripts_opennmt/../models_opennmt/sign+2en/model_step_54000.pt
/home/cluster/zifjia/.local/lib/python3.8/site-packages/onmt/modules/embeddings.py:195: UserWarning: Not merging with sum and positive feat_vec_size, but got non-default feat_vec_exponent. It will be unused.
  warnings.warn("Not merging with sum and positive "
[2021-10-27 12:12:44,897 INFO] Translating shard 0.
/home/cluster/zifjia/.local/lib/python3.8/site-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  self._batch_index = self.topk_ids // vocab_size
[2021-10-27 12:12:57,607 INFO] PRED AVG SCORE: -0.1193, PRED PPL: 1.1267
compute BLEU with sacrebleu ... 
sacreBLEU: That's 100 lines that end in a tokenized period ('.')
sacreBLEU: It looks like you forgot to detokenize your test data, which may hurt your score.
sacreBLEU: If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
{
 "name": "BLEU",
 "score": 1.4,
 "signature": "nrefs:1|case:lc|eff:no|tok:13a|smooth:exp|version:2.0.0",
 "verbose_score": "19.4/2.1/0.7/0.4 (BP = 0.798 ratio = 0.816 hyp_len = 23650 ref_len = 28974)",
 "nrefs": "1",
 "case": "lc",
 "eff": "no",
 "tok": "13a",
 "smooth": "exp",
 "version": "2.0.0"
}
[0m# Translating with checkpoint /net/cephfs/home/zifjia/signwriting-translation/./scripts_opennmt/../models_opennmt/sign+2en/model_step_57000.pt
/home/cluster/zifjia/.local/lib/python3.8/site-packages/onmt/modules/embeddings.py:195: UserWarning: Not merging with sum and positive feat_vec_size, but got non-default feat_vec_exponent. It will be unused.
  warnings.warn("Not merging with sum and positive "
[2021-10-27 12:13:06,648 INFO] Translating shard 0.
/home/cluster/zifjia/.local/lib/python3.8/site-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  self._batch_index = self.topk_ids // vocab_size
[2021-10-27 12:13:19,449 INFO] PRED AVG SCORE: -0.1213, PRED PPL: 1.1290
compute BLEU with sacrebleu ... 
sacreBLEU: That's 100 lines that end in a tokenized period ('.')
sacreBLEU: It looks like you forgot to detokenize your test data, which may hurt your score.
sacreBLEU: If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
{
 "name": "BLEU",
 "score": 1.5,
 "signature": "nrefs:1|case:lc|eff:no|tok:13a|smooth:exp|version:2.0.0",
 "verbose_score": "19.2/2.1/0.7/0.4 (BP = 0.793 ratio = 0.812 hyp_len = 23522 ref_len = 28974)",
 "nrefs": "1",
 "case": "lc",
 "eff": "no",
 "tok": "13a",
 "smooth": "exp",
 "version": "2.0.0"
}
[0m# Translating with checkpoint /net/cephfs/home/zifjia/signwriting-translation/./scripts_opennmt/../models_opennmt/sign+2en/model_step_60000.pt
/home/cluster/zifjia/.local/lib/python3.8/site-packages/onmt/modules/embeddings.py:195: UserWarning: Not merging with sum and positive feat_vec_size, but got non-default feat_vec_exponent. It will be unused.
  warnings.warn("Not merging with sum and positive "
[2021-10-27 12:13:28,738 INFO] Translating shard 0.
/home/cluster/zifjia/.local/lib/python3.8/site-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  self._batch_index = self.topk_ids // vocab_size
[2021-10-27 12:13:41,156 INFO] PRED AVG SCORE: -0.1226, PRED PPL: 1.1305
compute BLEU with sacrebleu ... 
sacreBLEU: That's 100 lines that end in a tokenized period ('.')
sacreBLEU: It looks like you forgot to detokenize your test data, which may hurt your score.
sacreBLEU: If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
{
 "name": "BLEU",
 "score": 1.4,
 "signature": "nrefs:1|case:lc|eff:no|tok:13a|smooth:exp|version:2.0.0",
 "verbose_score": "19.0/2.0/0.7/0.4 (BP = 0.786 ratio = 0.806 hyp_len = 23356 ref_len = 28974)",
 "nrefs": "1",
 "case": "lc",
 "eff": "no",
 "tok": "13a",
 "smooth": "exp",
 "version": "2.0.0"
}
[0m# Translating with checkpoint /net/cephfs/home/zifjia/signwriting-translation/./scripts_opennmt/../models_opennmt/sign+2en/model_step_63000.pt
/home/cluster/zifjia/.local/lib/python3.8/site-packages/onmt/modules/embeddings.py:195: UserWarning: Not merging with sum and positive feat_vec_size, but got non-default feat_vec_exponent. It will be unused.
  warnings.warn("Not merging with sum and positive "
[2021-10-27 12:13:50,990 INFO] Translating shard 0.
/home/cluster/zifjia/.local/lib/python3.8/site-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  self._batch_index = self.topk_ids // vocab_size
[2021-10-27 12:14:03,361 INFO] PRED AVG SCORE: -0.1221, PRED PPL: 1.1298
compute BLEU with sacrebleu ... 
sacreBLEU: That's 100 lines that end in a tokenized period ('.')
sacreBLEU: It looks like you forgot to detokenize your test data, which may hurt your score.
sacreBLEU: If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
{
 "name": "BLEU",
 "score": 1.4,
 "signature": "nrefs:1|case:lc|eff:no|tok:13a|smooth:exp|version:2.0.0",
 "verbose_score": "19.2/2.0/0.7/0.4 (BP = 0.794 ratio = 0.813 hyp_len = 23548 ref_len = 28974)",
 "nrefs": "1",
 "case": "lc",
 "eff": "no",
 "tok": "13a",
 "smooth": "exp",
 "version": "2.0.0"
}
[0m# Translating with checkpoint /net/cephfs/home/zifjia/signwriting-translation/./scripts_opennmt/../models_opennmt/sign+2en/model_step_66000.pt
/home/cluster/zifjia/.local/lib/python3.8/site-packages/onmt/modules/embeddings.py:195: UserWarning: Not merging with sum and positive feat_vec_size, but got non-default feat_vec_exponent. It will be unused.
  warnings.warn("Not merging with sum and positive "
[2021-10-27 12:14:13,233 INFO] Translating shard 0.
/home/cluster/zifjia/.local/lib/python3.8/site-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  self._batch_index = self.topk_ids // vocab_size
[2021-10-27 12:14:25,447 INFO] PRED AVG SCORE: -0.1232, PRED PPL: 1.1311
compute BLEU with sacrebleu ... 
sacreBLEU: That's 100 lines that end in a tokenized period ('.')
sacreBLEU: It looks like you forgot to detokenize your test data, which may hurt your score.
sacreBLEU: If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
{
 "name": "BLEU",
 "score": 1.4,
 "signature": "nrefs:1|case:lc|eff:no|tok:13a|smooth:exp|version:2.0.0",
 "verbose_score": "19.1/2.0/0.8/0.4 (BP = 0.782 ratio = 0.802 hyp_len = 23248 ref_len = 28974)",
 "nrefs": "1",
 "case": "lc",
 "eff": "no",
 "tok": "13a",
 "smooth": "exp",
 "version": "2.0.0"
}
[0m# Translating with checkpoint /net/cephfs/home/zifjia/signwriting-translation/./scripts_opennmt/../models_opennmt/sign+2en/model_step_69000.pt
/home/cluster/zifjia/.local/lib/python3.8/site-packages/onmt/modules/embeddings.py:195: UserWarning: Not merging with sum and positive feat_vec_size, but got non-default feat_vec_exponent. It will be unused.
  warnings.warn("Not merging with sum and positive "
[2021-10-27 12:14:34,711 INFO] Translating shard 0.
/home/cluster/zifjia/.local/lib/python3.8/site-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  self._batch_index = self.topk_ids // vocab_size
[2021-10-27 12:14:47,246 INFO] PRED AVG SCORE: -0.1232, PRED PPL: 1.1311
compute BLEU with sacrebleu ... 
sacreBLEU: That's 100 lines that end in a tokenized period ('.')
sacreBLEU: It looks like you forgot to detokenize your test data, which may hurt your score.
sacreBLEU: If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
{
 "name": "BLEU",
 "score": 1.4,
 "signature": "nrefs:1|case:lc|eff:no|tok:13a|smooth:exp|version:2.0.0",
 "verbose_score": "19.1/2.1/0.7/0.4 (BP = 0.784 ratio = 0.804 hyp_len = 23309 ref_len = 28974)",
 "nrefs": "1",
 "case": "lc",
 "eff": "no",
 "tok": "13a",
 "smooth": "exp",
 "version": "2.0.0"
}
[0m# Translating with checkpoint /net/cephfs/home/zifjia/signwriting-translation/./scripts_opennmt/../models_opennmt/sign+2en/model_step_72000.pt
/home/cluster/zifjia/.local/lib/python3.8/site-packages/onmt/modules/embeddings.py:195: UserWarning: Not merging with sum and positive feat_vec_size, but got non-default feat_vec_exponent. It will be unused.
  warnings.warn("Not merging with sum and positive "
[2021-10-27 12:14:57,009 INFO] Translating shard 0.
/home/cluster/zifjia/.local/lib/python3.8/site-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  self._batch_index = self.topk_ids // vocab_size
[2021-10-27 12:15:09,226 INFO] PRED AVG SCORE: -0.1245, PRED PPL: 1.1326
compute BLEU with sacrebleu ... 
sacreBLEU: That's 100 lines that end in a tokenized period ('.')
sacreBLEU: It looks like you forgot to detokenize your test data, which may hurt your score.
sacreBLEU: If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
{
 "name": "BLEU",
 "score": 2.1,
 "signature": "nrefs:1|case:lc|eff:no|tok:13a|smooth:exp|version:2.0.0",
 "verbose_score": "19.7/2.5/1.2/0.8 (BP = 0.774 ratio = 0.796 hyp_len = 23073 ref_len = 28974)",
 "nrefs": "1",
 "case": "lc",
 "eff": "no",
 "tok": "13a",
 "smooth": "exp",
 "version": "2.0.0"
}
[0m# Translating with checkpoint /net/cephfs/home/zifjia/signwriting-translation/./scripts_opennmt/../models_opennmt/sign+2en/model_step_75000.pt
/home/cluster/zifjia/.local/lib/python3.8/site-packages/onmt/modules/embeddings.py:195: UserWarning: Not merging with sum and positive feat_vec_size, but got non-default feat_vec_exponent. It will be unused.
  warnings.warn("Not merging with sum and positive "
[2021-10-27 12:15:18,890 INFO] Translating shard 0.
/home/cluster/zifjia/.local/lib/python3.8/site-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  self._batch_index = self.topk_ids // vocab_size
[2021-10-27 12:15:30,978 INFO] PRED AVG SCORE: -0.1232, PRED PPL: 1.1311
compute BLEU with sacrebleu ... 
sacreBLEU: That's 100 lines that end in a tokenized period ('.')
sacreBLEU: It looks like you forgot to detokenize your test data, which may hurt your score.
sacreBLEU: If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
{
 "name": "BLEU",
 "score": 1.4,
 "signature": "nrefs:1|case:lc|eff:no|tok:13a|smooth:exp|version:2.0.0",
 "verbose_score": "19.2/2.2/0.7/0.4 (BP = 0.793 ratio = 0.812 hyp_len = 23528 ref_len = 28974)",
 "nrefs": "1",
 "case": "lc",
 "eff": "no",
 "tok": "13a",
 "smooth": "exp",
 "version": "2.0.0"
}
[0m# Translating with checkpoint /net/cephfs/home/zifjia/signwriting-translation/./scripts_opennmt/../models_opennmt/sign+2en/model_step_78000.pt
/home/cluster/zifjia/.local/lib/python3.8/site-packages/onmt/modules/embeddings.py:195: UserWarning: Not merging with sum and positive feat_vec_size, but got non-default feat_vec_exponent. It will be unused.
  warnings.warn("Not merging with sum and positive "
[2021-10-27 12:15:41,116 INFO] Translating shard 0.
/home/cluster/zifjia/.local/lib/python3.8/site-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  self._batch_index = self.topk_ids // vocab_size
[2021-10-27 12:15:53,886 INFO] PRED AVG SCORE: -0.1232, PRED PPL: 1.1311
compute BLEU with sacrebleu ... 
sacreBLEU: That's 100 lines that end in a tokenized period ('.')
sacreBLEU: It looks like you forgot to detokenize your test data, which may hurt your score.
sacreBLEU: If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
{
 "name": "BLEU",
 "score": 1.3,
 "signature": "nrefs:1|case:lc|eff:no|tok:13a|smooth:exp|version:2.0.0",
 "verbose_score": "19.1/2.0/0.7/0.3 (BP = 0.794 ratio = 0.813 hyp_len = 23551 ref_len = 28974)",
 "nrefs": "1",
 "case": "lc",
 "eff": "no",
 "tok": "13a",
 "smooth": "exp",
 "version": "2.0.0"
}
[0m# Translating with checkpoint /net/cephfs/home/zifjia/signwriting-translation/./scripts_opennmt/../models_opennmt/sign+2en/model_step_80000.pt
/home/cluster/zifjia/.local/lib/python3.8/site-packages/onmt/modules/embeddings.py:195: UserWarning: Not merging with sum and positive feat_vec_size, but got non-default feat_vec_exponent. It will be unused.
  warnings.warn("Not merging with sum and positive "
[2021-10-27 12:16:02,906 INFO] Translating shard 0.
/home/cluster/zifjia/.local/lib/python3.8/site-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  self._batch_index = self.topk_ids // vocab_size
[2021-10-27 12:16:15,253 INFO] PRED AVG SCORE: -0.1240, PRED PPL: 1.1320
compute BLEU with sacrebleu ... 
sacreBLEU: That's 100 lines that end in a tokenized period ('.')
sacreBLEU: It looks like you forgot to detokenize your test data, which may hurt your score.
sacreBLEU: If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
{
 "name": "BLEU",
 "score": 1.5,
 "signature": "nrefs:1|case:lc|eff:no|tok:13a|smooth:exp|version:2.0.0",
 "verbose_score": "19.5/2.2/0.8/0.4 (BP = 0.797 ratio = 0.815 hyp_len = 23624 ref_len = 28974)",
 "nrefs": "1",
 "case": "lc",
 "eff": "no",
 "tok": "13a",
 "smooth": "exp",
 "version": "2.0.0"
}
[0mslurmstepd: Exceeded job memory limit at some point.
slurmstepd: Exceeded job memory limit at some point.
