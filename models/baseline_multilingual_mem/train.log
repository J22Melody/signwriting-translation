2021-11-23 13:10:28,739 - INFO - root - Hello! This is Joey-NMT (version 1.3).
2021-11-23 13:10:28,883 - INFO - joeynmt.data - Loading training data...
2021-11-23 13:10:35,029 - INFO - joeynmt.data - Building vocabulary...
2021-11-23 13:10:39,649 - INFO - joeynmt.data - Loading dev data...
2021-11-23 13:10:39,735 - INFO - joeynmt.data - Loading test data...
2021-11-23 13:10:39,799 - INFO - joeynmt.data - Data loaded.
2021-11-23 13:10:39,799 - INFO - joeynmt.model - Building an encoder-decoder model...
2021-11-23 13:10:40,564 - INFO - joeynmt.model - Enc-dec model built.
2021-11-23 13:10:43,328 - DEBUG - tensorflow - Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.
2021-11-23 13:10:43,761 - DEBUG - h5py._conv - Creating converter from 7 to 5
2021-11-23 13:10:43,761 - DEBUG - h5py._conv - Creating converter from 5 to 7
2021-11-23 13:10:43,761 - DEBUG - h5py._conv - Creating converter from 7 to 5
2021-11-23 13:10:43,761 - DEBUG - h5py._conv - Creating converter from 5 to 7
2021-11-23 13:10:46,617 - INFO - joeynmt.training - Total params: 49881760
2021-11-23 13:10:46,619 - DEBUG - joeynmt.training - Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'encoder.layers.4.feed_forward.layer_norm.bias', 'encoder.layers.4.feed_forward.layer_norm.weight', 'encoder.layers.4.feed_forward.pwff_layer.0.bias', 'encoder.layers.4.feed_forward.pwff_layer.0.weight', 'encoder.layers.4.feed_forward.pwff_layer.3.bias', 'encoder.layers.4.feed_forward.pwff_layer.3.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.4.src_src_att.k_layer.bias', 'encoder.layers.4.src_src_att.k_layer.weight', 'encoder.layers.4.src_src_att.output_layer.bias', 'encoder.layers.4.src_src_att.output_layer.weight', 'encoder.layers.4.src_src_att.q_layer.bias', 'encoder.layers.4.src_src_att.q_layer.weight', 'encoder.layers.4.src_src_att.v_layer.bias', 'encoder.layers.4.src_src_att.v_layer.weight', 'encoder.layers.5.feed_forward.layer_norm.bias', 'encoder.layers.5.feed_forward.layer_norm.weight', 'encoder.layers.5.feed_forward.pwff_layer.0.bias', 'encoder.layers.5.feed_forward.pwff_layer.0.weight', 'encoder.layers.5.feed_forward.pwff_layer.3.bias', 'encoder.layers.5.feed_forward.pwff_layer.3.weight', 'encoder.layers.5.layer_norm.bias', 'encoder.layers.5.layer_norm.weight', 'encoder.layers.5.src_src_att.k_layer.bias', 'encoder.layers.5.src_src_att.k_layer.weight', 'encoder.layers.5.src_src_att.output_layer.bias', 'encoder.layers.5.src_src_att.output_layer.weight', 'encoder.layers.5.src_src_att.q_layer.bias', 'encoder.layers.5.src_src_att.q_layer.weight', 'encoder.layers.5.src_src_att.v_layer.bias', 'encoder.layers.5.src_src_att.v_layer.weight', 'factor_embeds.0.lut.weight', 'factor_embeds.1.lut.weight', 'factor_embeds.2.lut.weight', 'factor_embeds.3.lut.weight', 'factor_embeds.4.lut.weight', 'factor_embeds.5.lut.weight', 'factor_embeds.6.lut.weight', 'src_embed.lut.weight', 'trg_embed.lut.weight']
2021-11-23 13:10:46,621 - WARNING - joeynmt.training - `keep_last_ckpts` option is outdated. Please use `keep_best_ckpts`, instead.
2021-11-23 13:10:54,688 - INFO - joeynmt.helpers - cfg.name                           : baseline_multilingual_mem
2021-11-23 13:10:54,700 - INFO - joeynmt.helpers - cfg.data.src                       : sign
2021-11-23 13:10:54,700 - INFO - joeynmt.helpers - cfg.data.trg                       : spm.spoken
2021-11-23 13:10:54,700 - INFO - joeynmt.helpers - cfg.data.factors                   : ['sign+', 'feat_col', 'feat_row', 'feat_x', 'feat_y', 'feat_x_rel', 'feat_y_rel']
2021-11-23 13:10:54,701 - INFO - joeynmt.helpers - cfg.data.train                     : data/train
2021-11-23 13:10:54,701 - INFO - joeynmt.helpers - cfg.data.dev                       : data/dev
2021-11-23 13:10:54,701 - INFO - joeynmt.helpers - cfg.data.test                      : data/test
2021-11-23 13:10:54,701 - INFO - joeynmt.helpers - cfg.data.level                     : word
2021-11-23 13:10:54,702 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False
2021-11-23 13:10:54,702 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 200
2021-11-23 13:10:54,702 - INFO - joeynmt.helpers - cfg.data.factor_voc_limit          : 10000
2021-11-23 13:10:54,702 - INFO - joeynmt.helpers - cfg.data.factor_voc_min_freq       : 1
2021-11-23 13:10:54,702 - INFO - joeynmt.helpers - cfg.data.use_factor                : True
2021-11-23 13:10:54,702 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5
2021-11-23 13:10:54,703 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0
2021-11-23 13:10:54,703 - INFO - joeynmt.helpers - cfg.testing.postprocess            : False
2021-11-23 13:10:54,703 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42
2021-11-23 13:10:54,703 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam
2021-11-23 13:10:54,703 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens
2021-11-23 13:10:54,703 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]
2021-11-23 13:10:54,703 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau
2021-11-23 13:10:54,703 - INFO - joeynmt.helpers - cfg.training.patience              : 7
2021-11-23 13:10:54,704 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7
2021-11-23 13:10:54,704 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy
2021-11-23 13:10:54,704 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0001
2021-11-23 13:10:54,704 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08
2021-11-23 13:10:54,704 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0
2021-11-23 13:10:54,704 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.2
2021-11-23 13:10:54,704 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096
2021-11-23 13:10:54,704 - INFO - joeynmt.helpers - cfg.training.batch_type            : token
2021-11-23 13:10:54,704 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 2048
2021-11-23 13:10:54,704 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token
2021-11-23 13:10:54,705 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1
2021-11-23 13:10:54,705 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : eval_metric
2021-11-23 13:10:54,705 - INFO - joeynmt.helpers - cfg.training.epochs                : 200
2021-11-23 13:10:54,705 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 1000
2021-11-23 13:10:54,705 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 100
2021-11-23 13:10:54,705 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu
2021-11-23 13:10:54,705 - INFO - joeynmt.helpers - cfg.training.overwrite             : True
2021-11-23 13:10:54,705 - INFO - joeynmt.helpers - cfg.training.shuffle               : True
2021-11-23 13:10:54,705 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True
2021-11-23 13:10:54,705 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 200
2021-11-23 13:10:54,705 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3, 6]
2021-11-23 13:10:54,705 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 1
2021-11-23 13:10:54,705 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/baseline_multilingual_mem
2021-11-23 13:10:54,705 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier
2021-11-23 13:10:54,705 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros
2021-11-23 13:10:54,706 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0
2021-11-23 13:10:54,706 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier
2021-11-23 13:10:54,706 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0
2021-11-23 13:10:54,706 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True
2021-11-23 13:10:54,706 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer
2021-11-23 13:10:54,706 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6
2021-11-23 13:10:54,706 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 8
2021-11-23 13:10:54,706 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 400
2021-11-23 13:10:54,706 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True
2021-11-23 13:10:54,706 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.5
2021-11-23 13:10:54,706 - INFO - joeynmt.helpers - cfg.model.encoder.factor_embeddings.embedding_dim : 16
2021-11-23 13:10:54,706 - INFO - joeynmt.helpers - cfg.model.encoder.factor_embeddings.scale : True
2021-11-23 13:10:54,707 - INFO - joeynmt.helpers - cfg.model.encoder.factor_embeddings.dropout : 0.5
2021-11-23 13:10:54,707 - INFO - joeynmt.helpers - cfg.model.encoder.factor_combine   : concatenate
2021-11-23 13:10:54,707 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 512
2021-11-23 13:10:54,707 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 2048
2021-11-23 13:10:54,707 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.5
2021-11-23 13:10:54,707 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer
2021-11-23 13:10:54,707 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6
2021-11-23 13:10:54,707 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 8
2021-11-23 13:10:54,707 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 512
2021-11-23 13:10:54,707 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True
2021-11-23 13:10:54,707 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.5
2021-11-23 13:10:54,707 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 512
2021-11-23 13:10:54,707 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 2048
2021-11-23 13:10:54,707 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.5
2021-11-23 13:10:54,708 - INFO - joeynmt.helpers - Data set sizes: 
	train 108087,
	valid 3426,
	test 2286
2021-11-23 13:10:54,709 - INFO - joeynmt.helpers - First training example:
	[SRC] <2pt> <4br> <dict> M S1dc02 S17610 S1f502 S20320
	[SRC_FACTOR0] <2pt> <4br> <dict> M S1dc S176 S1f5 S203
	[TRG] ▁2 01 8
2021-11-23 13:10:54,709 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) M (5) <dict> (6) P (7) S20500 (8) S2ff00 (9) <2pt>
2021-11-23 13:10:54,712 - INFO - joeynmt.helpers - First 10 words (src_factor0): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) M (5) S15a (6) <dict> (7) S100 (8) P (9) S265
2021-11-23 13:10:54,713 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) ▁the (6) . (7) ▁and (8) - (9) s
2021-11-23 13:10:54,713 - INFO - joeynmt.helpers - Number of Src words (types): 11765
2021-11-23 13:10:54,734 - INFO - joeynmt.helpers - Number of Src_factor0 words (types): 653
2021-11-23 13:10:54,734 - INFO - joeynmt.helpers - Number of Trg words (types): 1969
2021-11-23 13:10:54,736 - INFO - joeynmt.training - Model(
	encoder=TransformerEncoder(num_layers=6, num_heads=8),
	decoder=TransformerDecoder(num_layers=6, num_heads=8),
	src_embed=Embeddings(embedding_dim=400, vocab_size=11765),
	factor_embeds=ModuleList(
  (0): Embeddings(embedding_dim=16, vocab_size=653)
  (1): Embeddings(embedding_dim=16, vocab_size=11)
  (2): Embeddings(embedding_dim=16, vocab_size=21)
  (3): Embeddings(embedding_dim=16, vocab_size=394)
  (4): Embeddings(embedding_dim=16, vocab_size=457)
  (5): Embeddings(embedding_dim=16, vocab_size=83)
  (6): Embeddings(embedding_dim=16, vocab_size=74)
),
	trg_embed=Embeddings(embedding_dim=512, vocab_size=1969))
2021-11-23 13:10:54,769 - INFO - joeynmt.training - Train stats:
	device: cuda
	n_gpu: 1
	16-bits training: False
	gradient accumulation: 1
	batch size per device: 4096
	total batch size (w. parallel & accumulation): 4096
2021-11-23 13:10:54,770 - INFO - joeynmt.training - EPOCH 1
2021-11-23 13:11:07,980 - INFO - joeynmt.training - Epoch   1, Step:      100, Batch Loss:     4.462044, Tokens per Sec:     2153, Lr: 0.000100
2021-11-23 13:11:20,741 - INFO - joeynmt.training - Epoch   1, Step:      200, Batch Loss:     4.590358, Tokens per Sec:     2342, Lr: 0.000100
2021-11-23 13:11:33,442 - INFO - joeynmt.training - Epoch   1, Step:      300, Batch Loss:     4.368107, Tokens per Sec:     2262, Lr: 0.000100
2021-11-23 13:11:46,111 - INFO - joeynmt.training - Epoch   1, Step:      400, Batch Loss:     4.312573, Tokens per Sec:     2311, Lr: 0.000100
2021-11-23 13:11:58,606 - INFO - joeynmt.training - Epoch   1, Step:      500, Batch Loss:     4.278621, Tokens per Sec:     2212, Lr: 0.000100
2021-11-23 13:12:11,030 - INFO - joeynmt.training - Epoch   1, Step:      600, Batch Loss:     4.313378, Tokens per Sec:     2308, Lr: 0.000100
2021-11-23 13:12:23,706 - INFO - joeynmt.training - Epoch   1, Step:      700, Batch Loss:     4.292613, Tokens per Sec:     2208, Lr: 0.000100
2021-11-23 13:12:36,402 - INFO - joeynmt.training - Epoch   1, Step:      800, Batch Loss:     4.181169, Tokens per Sec:     2237, Lr: 0.000100
2021-11-23 13:12:48,970 - INFO - joeynmt.training - Epoch   1, Step:      900, Batch Loss:     4.290770, Tokens per Sec:     2302, Lr: 0.000100
2021-11-23 13:13:01,658 - INFO - joeynmt.training - Epoch   1, Step:     1000, Batch Loss:     4.283733, Tokens per Sec:     2224, Lr: 0.000100
2021-11-23 13:21:03,019 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-11-23 13:21:04,738 - INFO - joeynmt.training - Example #0
2021-11-23 13:21:04,739 - DEBUG - joeynmt.training - 	Raw source:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a07', 'S1f010', 'S26507', 'M', 'S11e20', 'S1a520', 'P', 'S38800', 'M', 'S15a56', 'S15a41', 'S2b700', 'S37c06', 'S36d01', 'S30d00', 'M', 'S10040', 'P', 'S38700', 'M', 'S37806', 'S10050', 'S20356', 'S36d01', 'M', 'S15a37', 'S1ce51', 'S26a07', 'P', 'S38700', 'M', 'S19220', 'S2a20c', 'S11520', 'S10120', 'S1f720', 'S20320', 'S36d01', 'M', 'S10018', 'S26505', 'S10641', 'M', 'S1f000', 'S1f720', 'S1dc20', 'S19220', 'S14a20', 'S1dc20', 'S14a20', 'P', 'S38800', 'M', 'S10041', 'S30d00', 'S36d01', 'M', 'S15a31', 'S10018', 'S26503', 'M', 'S2e74c', 'S14220', 'S2e700', 'S14228', 'M', 'S26521', 'S1f548', 'S1f540', 'M', 'S10041', 'P', 'S38700', 'M', 'S10021', 'S10029', 'S22a07', 'S22a11', 'S36d01', 'M', 'S2ea3e', 'S10000', 'M', 'S15a18', 'S10051', 'S26601', 'M', 'S10058', 'S26526', 'S10050', 'S20500', 'S20500', 'P', 'S38900', 'M', 'S15a37', 'S15a01', 'S2e900', 'S36d01', 'M', 'S15a20', 'S26501', 'M', 'S26521', 'S23b0a', 'S1f548', 'S15a40', 'S15a48', 'S23b1a', 'S1f540', 'M', 'S14c57', 'S14c5f', 'S26511', 'S26507', 'S22527', 'S20314', 'S22521', 'S2031c', 'P', 'S38800']
2021-11-23 13:21:04,739 - DEBUG - joeynmt.training - 	Raw factor:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a', 'S1f0', 'S265', 'M', 'S11e', 'S1a5', 'P', 'S388', 'M', 'S15a', 'S15a', 'S2b7', 'S37c', 'S36d', 'S30d', 'M', 'S100', 'P', 'S387', 'M', 'S378', 'S100', 'S203', 'S36d', 'M', 'S15a', 'S1ce', 'S26a', 'P', 'S387', 'M', 'S192', 'S2a2', 'S115', 'S101', 'S1f7', 'S203', 'S36d', 'M', 'S100', 'S265', 'S106', 'M', 'S1f0', 'S1f7', 'S1dc', 'S192', 'S14a', 'S1dc', 'S14a', 'P', 'S388', 'M', 'S100', 'S30d', 'S36d', 'M', 'S15a', 'S100', 'S265', 'M', 'S2e7', 'S142', 'S2e7', 'S142', 'M', 'S265', 'S1f5', 'S1f5', 'M', 'S100', 'P', 'S387', 'M', 'S100', 'S100', 'S22a', 'S22a', 'S36d', 'M', 'S2ea', 'S100', 'M', 'S15a', 'S100', 'S266', 'M', 'S100', 'S265', 'S100', 'S205', 'S205', 'P', 'S389', 'M', 'S15a', 'S15a', 'S2e9', 'S36d', 'M', 'S15a', 'S265', 'M', 'S265', 'S23b', 'S1f5', 'S15a', 'S15a', 'S23b', 'S1f5', 'M', 'S14c', 'S14c', 'S265', 'S265', 'S225', 'S203', 'S225', 'S203', 'P', 'S388']
2021-11-23 13:21:04,739 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse']
2021-11-23 13:21:04,739 - INFO - joeynmt.training - 	Source:     <2en> <4us> <sent> M S15a07 S1f010 S26507 M S11e20 S1a520 P S38800 M S15a56 S15a41 S2b700 S37c06 S36d01 S30d00 M S10040 P S38700 M S37806 S10050 S20356 S36d01 M S15a37 S1ce51 S26a07 P S38700 M S19220 S2a20c S11520 S10120 S1f720 S20320 S36d01 M S10018 S26505 S10641 M S1f000 S1f720 S1dc20 S19220 S14a20 S1dc20 S14a20 P S38800 M S10041 S30d00 S36d01 M S15a31 S10018 S26503 M S2e74c S14220 S2e700 S14228 M S26521 S1f548 S1f540 M S10041 P S38700 M S10021 S10029 S22a07 S22a11 S36d01 M S2ea3e S10000 M S15a18 S10051 S26601 M S10058 S26526 S10050 S20500 S20500 P S38900 M S15a37 S15a01 S2e900 S36d01 M S15a20 S26501 M S26521 S23b0a S1f548 S15a40 S15a48 S23b1a S1f540 M S14c57 S14c5f S26511 S26507 S22527 S20314 S22521 S2031c P S38800
2021-11-23 13:21:04,739 - INFO - joeynmt.training - 	Factor:     <2en> <4us> <sent> M S15a S1f0 S265 M S11e S1a5 P S388 M S15a S15a S2b7 S37c S36d S30d M S100 P S387 M S378 S100 S203 S36d M S15a S1ce S26a P S387 M S192 S2a2 S115 S101 S1f7 S203 S36d M S100 S265 S106 M S1f0 S1f7 S1dc S192 S14a S1dc S14a P S388 M S100 S30d S36d M S15a S100 S265 M S2e7 S142 S2e7 S142 M S265 S1f5 S1f5 M S100 P S387 M S100 S100 S22a S22a S36d M S2ea S100 M S15a S100 S266 M S100 S265 S100 S205 S205 P S389 M S15a S15a S2e9 S36d M S15a S265 M S265 S23b S1f5 S15a S15a S23b S1f5 M S14c S14c S265 S265 S225 S203 S225 S203 P S388
2021-11-23 13:21:04,739 - INFO - joeynmt.training - 	Reference:  ▁Verse ▁3 7. ▁After ▁him , ▁at ▁the ▁time ▁of ▁the ▁c ens us , ▁there ▁was ▁Jud as ▁of ▁Gal ile e . ▁He ▁g ot ▁people ▁to ▁follow ▁him , ▁but ▁he ▁was ▁k illed , ▁too , ▁and ▁all ▁his ▁follow ers ▁were ▁sc atter ed .
2021-11-23 13:21:04,739 - INFO - joeynmt.training - 	Hypothesis: ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse
2021-11-23 13:21:04,739 - INFO - joeynmt.training - Example #1
2021-11-23 13:21:04,739 - DEBUG - joeynmt.training - 	Raw source:     ['<2pt>', '<4br>', '<dict>', 'M', 'S34700', 'S21100', 'S1f410']
2021-11-23 13:21:04,739 - DEBUG - joeynmt.training - 	Raw factor:     ['<2pt>', '<4br>', '<dict>', 'M', 'S347', 'S211', 'S1f4']
2021-11-23 13:21:04,739 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁rwth', '200']
2021-11-23 13:21:04,739 - INFO - joeynmt.training - 	Source:     <2pt> <4br> <dict> M S34700 S21100 S1f410
2021-11-23 13:21:04,739 - INFO - joeynmt.training - 	Factor:     <2pt> <4br> <dict> M S347 S211 S1f4
2021-11-23 13:21:04,739 - INFO - joeynmt.training - 	Reference:  ▁L á p is
2021-11-23 13:21:04,739 - INFO - joeynmt.training - 	Hypothesis: ▁rwth 200
2021-11-23 13:21:04,740 - INFO - joeynmt.training - Example #2
2021-11-23 13:21:04,740 - DEBUG - joeynmt.training - 	Raw source:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a07', 'S1f010', 'S26507', 'M', 'S10e00', 'P', 'S38800', 'M', 'S1dc0a', 'S10003', 'S20500', 'S20500', 'S30a00', 'M', 'S20302', 'S2030a', 'S20340', 'S20348', 'S30e00', 'S30124', 'M', 'S20500', 'S10013', 'M', 'S30124', 'S10010', 'S26500', 'M', 'S22f10', 'S20500', 'S15a0a', 'S22f00', 'S15a02', 'S20500', 'P', 'S38900', 'M', 'S17911', 'S17919', 'S2c300', 'S2c311', 'S30a00', 'P', 'S38700', 'M', 'S15a06', 'S15a41', 'S23d04', 'M', 'S2ff00', 'S10050', 'S10058', 'S10011', 'S22a04', 'M', 'S1f522', 'S2ea46', 'S2ea02', 'S1f50a', 'P', 'S38700', 'R', 'S3770b', 'S20305', 'S20500', 'S20303', 'S20500', 'S37713', 'S30e00', 'S30124', 'R', 'S1f522', 'S2ea46', 'S2ea02', 'S1f50a', 'P', 'S38700', 'M', 'S20356', 'S20350', 'S37706', 'S22f04', 'S30e00', 'S30124', 'M', 'S1f740', 'S1f748', 'S26a20', 'M', 'S1814b', 'S20500', 'S20500', 'S18143', 'R', 'S10000', 'S26500', 'R', 'S10011', 'S2ff00', 'P', 'S38700', 'M', 'S10000', 'S26500', 'M', 'S15a18', 'S15a18', 'S10e22', 'S10e02', 'P', 'S38800']
2021-11-23 13:21:04,740 - DEBUG - joeynmt.training - 	Raw factor:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a', 'S1f0', 'S265', 'M', 'S10e', 'P', 'S388', 'M', 'S1dc', 'S100', 'S205', 'S205', 'S30a', 'M', 'S203', 'S203', 'S203', 'S203', 'S30e', 'S301', 'M', 'S205', 'S100', 'M', 'S301', 'S100', 'S265', 'M', 'S22f', 'S205', 'S15a', 'S22f', 'S15a', 'S205', 'P', 'S389', 'M', 'S179', 'S179', 'S2c3', 'S2c3', 'S30a', 'P', 'S387', 'M', 'S15a', 'S15a', 'S23d', 'M', 'S2ff', 'S100', 'S100', 'S100', 'S22a', 'M', 'S1f5', 'S2ea', 'S2ea', 'S1f5', 'P', 'S387', 'R', 'S377', 'S203', 'S205', 'S203', 'S205', 'S377', 'S30e', 'S301', 'R', 'S1f5', 'S2ea', 'S2ea', 'S1f5', 'P', 'S387', 'M', 'S203', 'S203', 'S377', 'S22f', 'S30e', 'S301', 'M', 'S1f7', 'S1f7', 'S26a', 'M', 'S181', 'S205', 'S205', 'S181', 'R', 'S100', 'S265', 'R', 'S100', 'S2ff', 'P', 'S387', 'M', 'S100', 'S265', 'M', 'S15a', 'S15a', 'S10e', 'S10e', 'P', 'S388']
2021-11-23 13:21:04,740 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse']
2021-11-23 13:21:04,740 - INFO - joeynmt.training - 	Source:     <2en> <4us> <sent> M S15a07 S1f010 S26507 M S10e00 P S38800 M S1dc0a S10003 S20500 S20500 S30a00 M S20302 S2030a S20340 S20348 S30e00 S30124 M S20500 S10013 M S30124 S10010 S26500 M S22f10 S20500 S15a0a S22f00 S15a02 S20500 P S38900 M S17911 S17919 S2c300 S2c311 S30a00 P S38700 M S15a06 S15a41 S23d04 M S2ff00 S10050 S10058 S10011 S22a04 M S1f522 S2ea46 S2ea02 S1f50a P S38700 R S3770b S20305 S20500 S20303 S20500 S37713 S30e00 S30124 R S1f522 S2ea46 S2ea02 S1f50a P S38700 M S20356 S20350 S37706 S22f04 S30e00 S30124 M S1f740 S1f748 S26a20 M S1814b S20500 S20500 S18143 R S10000 S26500 R S10011 S2ff00 P S38700 M S10000 S26500 M S15a18 S15a18 S10e22 S10e02 P S38800
2021-11-23 13:21:04,740 - INFO - joeynmt.training - 	Factor:     <2en> <4us> <sent> M S15a S1f0 S265 M S10e P S388 M S1dc S100 S205 S205 S30a M S203 S203 S203 S203 S30e S301 M S205 S100 M S301 S100 S265 M S22f S205 S15a S22f S15a S205 P S389 M S179 S179 S2c3 S2c3 S30a P S387 M S15a S15a S23d M S2ff S100 S100 S100 S22a M S1f5 S2ea S2ea S1f5 P S387 R S377 S203 S205 S203 S205 S377 S30e S301 R S1f5 S2ea S2ea S1f5 P S387 M S203 S203 S377 S22f S30e S301 M S1f7 S1f7 S26a M S181 S205 S205 S181 R S100 S265 R S100 S2ff P S387 M S100 S265 M S15a S15a S10e S10e P S388
2021-11-23 13:21:04,740 - INFO - joeynmt.training - 	Reference:  ▁Verse ▁2. ▁Then ▁make ▁me ▁tr ul y ▁hap p y ▁by ▁ag ree ing ▁whole he art ed ly ▁with ▁each ▁other , ▁lo v ing ▁one ▁another , ▁and ▁work ing ▁together ▁with ▁one ▁m ind ▁and ▁p ur p ose .
2021-11-23 13:21:04,740 - INFO - joeynmt.training - 	Hypothesis: ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse
2021-11-23 13:21:04,740 - INFO - joeynmt.training - Example #3
2021-11-23 13:21:04,740 - DEBUG - joeynmt.training - 	Raw source:     ['<2pt>', '<4br>', '<dict>', 'M', 'S10e40', 'S11040', 'S26504', 'S2fb04']
2021-11-23 13:21:04,740 - DEBUG - joeynmt.training - 	Raw factor:     ['<2pt>', '<4br>', '<dict>', 'M', 'S10e', 'S110', 'S265', 'S2fb']
2021-11-23 13:21:04,740 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁rwth', '200']
2021-11-23 13:21:04,740 - INFO - joeynmt.training - 	Source:     <2pt> <4br> <dict> M S10e40 S11040 S26504 S2fb04
2021-11-23 13:21:04,740 - INFO - joeynmt.training - 	Factor:     <2pt> <4br> <dict> M S10e S110 S265 S2fb
2021-11-23 13:21:04,740 - INFO - joeynmt.training - 	Reference:  ▁expl or ar
2021-11-23 13:21:04,740 - INFO - joeynmt.training - 	Hypothesis: ▁rwth 200
2021-11-23 13:21:04,740 - INFO - joeynmt.training - Example #6
2021-11-23 13:21:04,740 - DEBUG - joeynmt.training - 	Raw source:     ['<2en>', '<4us>', '<dict>', 'M', 'S20340', 'S21800']
2021-11-23 13:21:04,740 - DEBUG - joeynmt.training - 	Raw factor:     ['<2en>', '<4us>', '<dict>', 'M', 'S203', 'S218']
2021-11-23 13:21:04,740 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁Verse', '▁I']
2021-11-23 13:21:04,740 - INFO - joeynmt.training - 	Source:     <2en> <4us> <dict> M S20340 S21800
2021-11-23 13:21:04,740 - INFO - joeynmt.training - 	Factor:     <2en> <4us> <dict> M S203 S218
2021-11-23 13:21:04,741 - INFO - joeynmt.training - 	Reference:  ▁m il k
2021-11-23 13:21:04,741 - INFO - joeynmt.training - 	Hypothesis: ▁Verse ▁I
2021-11-23 13:21:04,741 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step     1000: bleu:   0.01, loss: 146263.4688, ppl:  75.0955, duration: 483.0822s
2021-11-23 13:21:17,331 - INFO - joeynmt.training - Epoch   1, Step:     1100, Batch Loss:     4.107805, Tokens per Sec:     2274, Lr: 0.000100
2021-11-23 13:21:29,871 - INFO - joeynmt.training - Epoch   1, Step:     1200, Batch Loss:     4.202862, Tokens per Sec:     2278, Lr: 0.000100
2021-11-23 13:21:42,557 - INFO - joeynmt.training - Epoch   1, Step:     1300, Batch Loss:     4.299277, Tokens per Sec:     2242, Lr: 0.000100
2021-11-23 13:21:55,107 - INFO - joeynmt.training - Epoch   1, Step:     1400, Batch Loss:     4.433462, Tokens per Sec:     2296, Lr: 0.000100
2021-11-23 13:22:07,773 - INFO - joeynmt.training - Epoch   1, Step:     1500, Batch Loss:     4.157005, Tokens per Sec:     2299, Lr: 0.000100
2021-11-23 13:22:20,490 - INFO - joeynmt.training - Epoch   1, Step:     1600, Batch Loss:     3.937958, Tokens per Sec:     2216, Lr: 0.000100
2021-11-23 13:22:33,128 - INFO - joeynmt.training - Epoch   1, Step:     1700, Batch Loss:     4.188608, Tokens per Sec:     2272, Lr: 0.000100
2021-11-23 13:22:45,520 - INFO - joeynmt.training - Epoch   1, Step:     1800, Batch Loss:     4.045748, Tokens per Sec:     2326, Lr: 0.000100
2021-11-23 13:22:58,168 - INFO - joeynmt.training - Epoch   1, Step:     1900, Batch Loss:     3.957309, Tokens per Sec:     2314, Lr: 0.000100
2021-11-23 13:23:10,989 - INFO - joeynmt.training - Epoch   1, Step:     2000, Batch Loss:     3.869528, Tokens per Sec:     2270, Lr: 0.000100
2021-11-23 13:32:12,235 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-11-23 13:32:13,786 - INFO - joeynmt.helpers - delete models/baseline_multilingual_mem/1000.ckpt
2021-11-23 13:32:13,787 - INFO - joeynmt.helpers - delete /net/cephfs/home/zifjia/signwriting-translation/models/baseline_multilingual_mem/1000.ckpt
2021-11-23 13:32:13,788 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /net/cephfs/home/zifjia/signwriting-translation/models/baseline_multilingual_mem/1000.ckpt but file does not exist. ([Errno 2] No such file or directory: '/net/cephfs/home/zifjia/signwriting-translation/models/baseline_multilingual_mem/1000.ckpt')
2021-11-23 13:32:13,794 - INFO - joeynmt.training - Example #0
2021-11-23 13:32:13,794 - DEBUG - joeynmt.training - 	Raw source:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a07', 'S1f010', 'S26507', 'M', 'S11e20', 'S1a520', 'P', 'S38800', 'M', 'S15a56', 'S15a41', 'S2b700', 'S37c06', 'S36d01', 'S30d00', 'M', 'S10040', 'P', 'S38700', 'M', 'S37806', 'S10050', 'S20356', 'S36d01', 'M', 'S15a37', 'S1ce51', 'S26a07', 'P', 'S38700', 'M', 'S19220', 'S2a20c', 'S11520', 'S10120', 'S1f720', 'S20320', 'S36d01', 'M', 'S10018', 'S26505', 'S10641', 'M', 'S1f000', 'S1f720', 'S1dc20', 'S19220', 'S14a20', 'S1dc20', 'S14a20', 'P', 'S38800', 'M', 'S10041', 'S30d00', 'S36d01', 'M', 'S15a31', 'S10018', 'S26503', 'M', 'S2e74c', 'S14220', 'S2e700', 'S14228', 'M', 'S26521', 'S1f548', 'S1f540', 'M', 'S10041', 'P', 'S38700', 'M', 'S10021', 'S10029', 'S22a07', 'S22a11', 'S36d01', 'M', 'S2ea3e', 'S10000', 'M', 'S15a18', 'S10051', 'S26601', 'M', 'S10058', 'S26526', 'S10050', 'S20500', 'S20500', 'P', 'S38900', 'M', 'S15a37', 'S15a01', 'S2e900', 'S36d01', 'M', 'S15a20', 'S26501', 'M', 'S26521', 'S23b0a', 'S1f548', 'S15a40', 'S15a48', 'S23b1a', 'S1f540', 'M', 'S14c57', 'S14c5f', 'S26511', 'S26507', 'S22527', 'S20314', 'S22521', 'S2031c', 'P', 'S38800']
2021-11-23 13:32:13,795 - DEBUG - joeynmt.training - 	Raw factor:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a', 'S1f0', 'S265', 'M', 'S11e', 'S1a5', 'P', 'S388', 'M', 'S15a', 'S15a', 'S2b7', 'S37c', 'S36d', 'S30d', 'M', 'S100', 'P', 'S387', 'M', 'S378', 'S100', 'S203', 'S36d', 'M', 'S15a', 'S1ce', 'S26a', 'P', 'S387', 'M', 'S192', 'S2a2', 'S115', 'S101', 'S1f7', 'S203', 'S36d', 'M', 'S100', 'S265', 'S106', 'M', 'S1f0', 'S1f7', 'S1dc', 'S192', 'S14a', 'S1dc', 'S14a', 'P', 'S388', 'M', 'S100', 'S30d', 'S36d', 'M', 'S15a', 'S100', 'S265', 'M', 'S2e7', 'S142', 'S2e7', 'S142', 'M', 'S265', 'S1f5', 'S1f5', 'M', 'S100', 'P', 'S387', 'M', 'S100', 'S100', 'S22a', 'S22a', 'S36d', 'M', 'S2ea', 'S100', 'M', 'S15a', 'S100', 'S266', 'M', 'S100', 'S265', 'S100', 'S205', 'S205', 'P', 'S389', 'M', 'S15a', 'S15a', 'S2e9', 'S36d', 'M', 'S15a', 'S265', 'M', 'S265', 'S23b', 'S1f5', 'S15a', 'S15a', 'S23b', 'S1f5', 'M', 'S14c', 'S14c', 'S265', 'S265', 'S225', 'S203', 'S225', 'S203', 'P', 'S388']
2021-11-23 13:32:13,795 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁Verse', '▁3', '▁3', '▁3', '▁And', '▁the', '▁Verse', '▁the', '▁Verse', '▁the', '▁Verse', '▁the', '▁Verse', '▁the', '▁Verse', '▁the', '▁Verse', '▁the', '▁Verse', '▁And', '▁the', '▁Verse', '▁The', '▁the', '▁Verse', '▁And', '▁the', '▁Verse', '▁The', '▁the', '▁Verse', '▁And', '▁the', '▁Verse', '▁And', '▁the', '▁Verse', '▁And', '▁the', '▁Verse', '▁And', '▁the', '▁Verse', '▁And', '▁the', '▁Verse', '▁And', '▁the', '▁Verse', '▁3', 't', '▁the', '▁Verse', '▁3', 't', '▁the', '▁Verse', '▁3', 't', '▁the', '▁Verse', '▁3', ':1', 't', '▁the', '▁Verse', '▁3', 't', '▁the', '▁Verse', '▁3', ':1', 't', '▁the', '▁Verse', '▁Verse', '▁3', ':1', 't', '▁the', '▁Verse', '▁3', ':1', 't', '▁the', '▁Verse', '▁3', ':1', 't', '▁the', '▁Verse', '▁3', '▁And', '▁the', '▁Verse', '▁Verse', '▁Verse', '▁3', ':1', 'ed', '▁the', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁3', '▁And', '▁the', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁3', '▁And', '▁the', '▁Verse', '▁3', ':1', 't', '▁the', '▁Verse', '▁3', ':1', 'ed', '▁the', '▁Verse', '▁3', '▁And', '▁the', '▁Verse', '▁Verse', '▁Verse', '▁3', '▁And', '▁the', '▁Verse', '▁Verse', '▁Verse', '▁3', '▁And', '▁the', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁3', '▁And', '▁the', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse']
2021-11-23 13:32:13,795 - INFO - joeynmt.training - 	Source:     <2en> <4us> <sent> M S15a07 S1f010 S26507 M S11e20 S1a520 P S38800 M S15a56 S15a41 S2b700 S37c06 S36d01 S30d00 M S10040 P S38700 M S37806 S10050 S20356 S36d01 M S15a37 S1ce51 S26a07 P S38700 M S19220 S2a20c S11520 S10120 S1f720 S20320 S36d01 M S10018 S26505 S10641 M S1f000 S1f720 S1dc20 S19220 S14a20 S1dc20 S14a20 P S38800 M S10041 S30d00 S36d01 M S15a31 S10018 S26503 M S2e74c S14220 S2e700 S14228 M S26521 S1f548 S1f540 M S10041 P S38700 M S10021 S10029 S22a07 S22a11 S36d01 M S2ea3e S10000 M S15a18 S10051 S26601 M S10058 S26526 S10050 S20500 S20500 P S38900 M S15a37 S15a01 S2e900 S36d01 M S15a20 S26501 M S26521 S23b0a S1f548 S15a40 S15a48 S23b1a S1f540 M S14c57 S14c5f S26511 S26507 S22527 S20314 S22521 S2031c P S38800
2021-11-23 13:32:13,795 - INFO - joeynmt.training - 	Factor:     <2en> <4us> <sent> M S15a S1f0 S265 M S11e S1a5 P S388 M S15a S15a S2b7 S37c S36d S30d M S100 P S387 M S378 S100 S203 S36d M S15a S1ce S26a P S387 M S192 S2a2 S115 S101 S1f7 S203 S36d M S100 S265 S106 M S1f0 S1f7 S1dc S192 S14a S1dc S14a P S388 M S100 S30d S36d M S15a S100 S265 M S2e7 S142 S2e7 S142 M S265 S1f5 S1f5 M S100 P S387 M S100 S100 S22a S22a S36d M S2ea S100 M S15a S100 S266 M S100 S265 S100 S205 S205 P S389 M S15a S15a S2e9 S36d M S15a S265 M S265 S23b S1f5 S15a S15a S23b S1f5 M S14c S14c S265 S265 S225 S203 S225 S203 P S388
2021-11-23 13:32:13,795 - INFO - joeynmt.training - 	Reference:  ▁Verse ▁3 7. ▁After ▁him , ▁at ▁the ▁time ▁of ▁the ▁c ens us , ▁there ▁was ▁Jud as ▁of ▁Gal ile e . ▁He ▁g ot ▁people ▁to ▁follow ▁him , ▁but ▁he ▁was ▁k illed , ▁too , ▁and ▁all ▁his ▁follow ers ▁were ▁sc atter ed .
2021-11-23 13:32:13,795 - INFO - joeynmt.training - 	Hypothesis: ▁Verse ▁3 ▁3 ▁3 ▁And ▁the ▁Verse ▁the ▁Verse ▁the ▁Verse ▁the ▁Verse ▁the ▁Verse ▁the ▁Verse ▁the ▁Verse ▁And ▁the ▁Verse ▁The ▁the ▁Verse ▁And ▁the ▁Verse ▁The ▁the ▁Verse ▁And ▁the ▁Verse ▁And ▁the ▁Verse ▁And ▁the ▁Verse ▁And ▁the ▁Verse ▁And ▁the ▁Verse ▁And ▁the ▁Verse ▁3 t ▁the ▁Verse ▁3 t ▁the ▁Verse ▁3 t ▁the ▁Verse ▁3 :1 t ▁the ▁Verse ▁3 t ▁the ▁Verse ▁3 :1 t ▁the ▁Verse ▁Verse ▁3 :1 t ▁the ▁Verse ▁3 :1 t ▁the ▁Verse ▁3 :1 t ▁the ▁Verse ▁3 ▁And ▁the ▁Verse ▁Verse ▁Verse ▁3 :1 ed ▁the ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁3 ▁And ▁the ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁3 ▁And ▁the ▁Verse ▁3 :1 t ▁the ▁Verse ▁3 :1 ed ▁the ▁Verse ▁3 ▁And ▁the ▁Verse ▁Verse ▁Verse ▁3 ▁And ▁the ▁Verse ▁Verse ▁Verse ▁3 ▁And ▁the ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁3 ▁And ▁the ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse
2021-11-23 13:32:13,795 - INFO - joeynmt.training - Example #1
2021-11-23 13:32:13,795 - DEBUG - joeynmt.training - 	Raw source:     ['<2pt>', '<4br>', '<dict>', 'M', 'S34700', 'S21100', 'S1f410']
2021-11-23 13:32:13,795 - DEBUG - joeynmt.training - 	Raw factor:     ['<2pt>', '<4br>', '<dict>', 'M', 'S347', 'S211', 'S1f4']
2021-11-23 13:32:13,795 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁Verse', '▁3', '1']
2021-11-23 13:32:13,795 - INFO - joeynmt.training - 	Source:     <2pt> <4br> <dict> M S34700 S21100 S1f410
2021-11-23 13:32:13,795 - INFO - joeynmt.training - 	Factor:     <2pt> <4br> <dict> M S347 S211 S1f4
2021-11-23 13:32:13,795 - INFO - joeynmt.training - 	Reference:  ▁L á p is
2021-11-23 13:32:13,795 - INFO - joeynmt.training - 	Hypothesis: ▁Verse ▁3 1
2021-11-23 13:32:13,795 - INFO - joeynmt.training - Example #2
2021-11-23 13:32:13,795 - DEBUG - joeynmt.training - 	Raw source:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a07', 'S1f010', 'S26507', 'M', 'S10e00', 'P', 'S38800', 'M', 'S1dc0a', 'S10003', 'S20500', 'S20500', 'S30a00', 'M', 'S20302', 'S2030a', 'S20340', 'S20348', 'S30e00', 'S30124', 'M', 'S20500', 'S10013', 'M', 'S30124', 'S10010', 'S26500', 'M', 'S22f10', 'S20500', 'S15a0a', 'S22f00', 'S15a02', 'S20500', 'P', 'S38900', 'M', 'S17911', 'S17919', 'S2c300', 'S2c311', 'S30a00', 'P', 'S38700', 'M', 'S15a06', 'S15a41', 'S23d04', 'M', 'S2ff00', 'S10050', 'S10058', 'S10011', 'S22a04', 'M', 'S1f522', 'S2ea46', 'S2ea02', 'S1f50a', 'P', 'S38700', 'R', 'S3770b', 'S20305', 'S20500', 'S20303', 'S20500', 'S37713', 'S30e00', 'S30124', 'R', 'S1f522', 'S2ea46', 'S2ea02', 'S1f50a', 'P', 'S38700', 'M', 'S20356', 'S20350', 'S37706', 'S22f04', 'S30e00', 'S30124', 'M', 'S1f740', 'S1f748', 'S26a20', 'M', 'S1814b', 'S20500', 'S20500', 'S18143', 'R', 'S10000', 'S26500', 'R', 'S10011', 'S2ff00', 'P', 'S38700', 'M', 'S10000', 'S26500', 'M', 'S15a18', 'S15a18', 'S10e22', 'S10e02', 'P', 'S38800']
2021-11-23 13:32:13,795 - DEBUG - joeynmt.training - 	Raw factor:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a', 'S1f0', 'S265', 'M', 'S10e', 'P', 'S388', 'M', 'S1dc', 'S100', 'S205', 'S205', 'S30a', 'M', 'S203', 'S203', 'S203', 'S203', 'S30e', 'S301', 'M', 'S205', 'S100', 'M', 'S301', 'S100', 'S265', 'M', 'S22f', 'S205', 'S15a', 'S22f', 'S15a', 'S205', 'P', 'S389', 'M', 'S179', 'S179', 'S2c3', 'S2c3', 'S30a', 'P', 'S387', 'M', 'S15a', 'S15a', 'S23d', 'M', 'S2ff', 'S100', 'S100', 'S100', 'S22a', 'M', 'S1f5', 'S2ea', 'S2ea', 'S1f5', 'P', 'S387', 'R', 'S377', 'S203', 'S205', 'S203', 'S205', 'S377', 'S30e', 'S301', 'R', 'S1f5', 'S2ea', 'S2ea', 'S1f5', 'P', 'S387', 'M', 'S203', 'S203', 'S377', 'S22f', 'S30e', 'S301', 'M', 'S1f7', 'S1f7', 'S26a', 'M', 'S181', 'S205', 'S205', 'S181', 'R', 'S100', 'S265', 'R', 'S100', 'S2ff', 'P', 'S387', 'M', 'S100', 'S265', 'M', 'S15a', 'S15a', 'S10e', 'S10e', 'P', 'S388']
2021-11-23 13:32:13,795 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁Verse', '▁3', '▁3', '▁3', '▁The', '▁the', '▁Verse', ',', '▁"', 't', '▁the', '▁Verse', 'ed', '▁the', '▁Verse', 'ed', '▁the', '▁Verse', 'ed', '▁the', '▁Verse', 'ed', '▁the', '▁Verse', 'ed', '▁the', '▁Verse', '▁the', '▁Verse', '▁the', '▁Verse', '▁And', '▁the', '▁Verse', '▁And', '▁the', '▁Verse', '▁the', '▁Verse', '▁And', '▁the', '▁Verse', '▁But', '▁the', '▁Verse', '▁But', '▁the', '▁Verse', '▁"', 't', '▁the', '▁Verse', '▁And', '▁the', '▁Verse', '▁"', 't', '▁the', '▁Verse', '▁3', 't', '▁the', '▁Verse', '▁3', 't', '▁the', '▁Verse', '▁"', 't', '▁the', '▁Verse', '▁3', 't', '▁the', '▁Verse', '▁Verse', '▁Verse', '▁3', ':1', 't', '▁the', '▁Verse', '▁3', 't', '▁the', '▁Verse', '▁3', ':1', 't', '▁the', '▁Verse', '▁3', ':1', '▁of', '▁the', '▁Verse', '▁Verse', '▁Verse', '▁3', ':1', ',', '▁the', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁3', ':1', 't', '▁the', '▁Verse', '▁3', ':1', 't', '▁the', '▁Verse', '▁3', ':1', ',', '▁the', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse', '▁Verse']
2021-11-23 13:32:13,795 - INFO - joeynmt.training - 	Source:     <2en> <4us> <sent> M S15a07 S1f010 S26507 M S10e00 P S38800 M S1dc0a S10003 S20500 S20500 S30a00 M S20302 S2030a S20340 S20348 S30e00 S30124 M S20500 S10013 M S30124 S10010 S26500 M S22f10 S20500 S15a0a S22f00 S15a02 S20500 P S38900 M S17911 S17919 S2c300 S2c311 S30a00 P S38700 M S15a06 S15a41 S23d04 M S2ff00 S10050 S10058 S10011 S22a04 M S1f522 S2ea46 S2ea02 S1f50a P S38700 R S3770b S20305 S20500 S20303 S20500 S37713 S30e00 S30124 R S1f522 S2ea46 S2ea02 S1f50a P S38700 M S20356 S20350 S37706 S22f04 S30e00 S30124 M S1f740 S1f748 S26a20 M S1814b S20500 S20500 S18143 R S10000 S26500 R S10011 S2ff00 P S38700 M S10000 S26500 M S15a18 S15a18 S10e22 S10e02 P S38800
2021-11-23 13:32:13,795 - INFO - joeynmt.training - 	Factor:     <2en> <4us> <sent> M S15a S1f0 S265 M S10e P S388 M S1dc S100 S205 S205 S30a M S203 S203 S203 S203 S30e S301 M S205 S100 M S301 S100 S265 M S22f S205 S15a S22f S15a S205 P S389 M S179 S179 S2c3 S2c3 S30a P S387 M S15a S15a S23d M S2ff S100 S100 S100 S22a M S1f5 S2ea S2ea S1f5 P S387 R S377 S203 S205 S203 S205 S377 S30e S301 R S1f5 S2ea S2ea S1f5 P S387 M S203 S203 S377 S22f S30e S301 M S1f7 S1f7 S26a M S181 S205 S205 S181 R S100 S265 R S100 S2ff P S387 M S100 S265 M S15a S15a S10e S10e P S388
2021-11-23 13:32:13,795 - INFO - joeynmt.training - 	Reference:  ▁Verse ▁2. ▁Then ▁make ▁me ▁tr ul y ▁hap p y ▁by ▁ag ree ing ▁whole he art ed ly ▁with ▁each ▁other , ▁lo v ing ▁one ▁another , ▁and ▁work ing ▁together ▁with ▁one ▁m ind ▁and ▁p ur p ose .
2021-11-23 13:32:13,795 - INFO - joeynmt.training - 	Hypothesis: ▁Verse ▁3 ▁3 ▁3 ▁The ▁the ▁Verse , ▁" t ▁the ▁Verse ed ▁the ▁Verse ed ▁the ▁Verse ed ▁the ▁Verse ed ▁the ▁Verse ed ▁the ▁Verse ▁the ▁Verse ▁the ▁Verse ▁And ▁the ▁Verse ▁And ▁the ▁Verse ▁the ▁Verse ▁And ▁the ▁Verse ▁But ▁the ▁Verse ▁But ▁the ▁Verse ▁" t ▁the ▁Verse ▁And ▁the ▁Verse ▁" t ▁the ▁Verse ▁3 t ▁the ▁Verse ▁3 t ▁the ▁Verse ▁" t ▁the ▁Verse ▁3 t ▁the ▁Verse ▁Verse ▁Verse ▁3 :1 t ▁the ▁Verse ▁3 t ▁the ▁Verse ▁3 :1 t ▁the ▁Verse ▁3 :1 ▁of ▁the ▁Verse ▁Verse ▁Verse ▁3 :1 , ▁the ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁3 :1 t ▁the ▁Verse ▁3 :1 t ▁the ▁Verse ▁3 :1 , ▁the ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse ▁Verse
2021-11-23 13:32:13,795 - INFO - joeynmt.training - Example #3
2021-11-23 13:32:13,795 - DEBUG - joeynmt.training - 	Raw source:     ['<2pt>', '<4br>', '<dict>', 'M', 'S10e40', 'S11040', 'S26504', 'S2fb04']
2021-11-23 13:32:13,796 - DEBUG - joeynmt.training - 	Raw factor:     ['<2pt>', '<4br>', '<dict>', 'M', 'S10e', 'S110', 'S265', 'S2fb']
2021-11-23 13:32:13,796 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁Verse', '▁3', '1']
2021-11-23 13:32:13,796 - INFO - joeynmt.training - 	Source:     <2pt> <4br> <dict> M S10e40 S11040 S26504 S2fb04
2021-11-23 13:32:13,796 - INFO - joeynmt.training - 	Factor:     <2pt> <4br> <dict> M S10e S110 S265 S2fb
2021-11-23 13:32:13,796 - INFO - joeynmt.training - 	Reference:  ▁expl or ar
2021-11-23 13:32:13,796 - INFO - joeynmt.training - 	Hypothesis: ▁Verse ▁3 1
2021-11-23 13:32:13,796 - INFO - joeynmt.training - Example #6
2021-11-23 13:32:13,796 - DEBUG - joeynmt.training - 	Raw source:     ['<2en>', '<4us>', '<dict>', 'M', 'S20340', 'S21800']
2021-11-23 13:32:13,796 - DEBUG - joeynmt.training - 	Raw factor:     ['<2en>', '<4us>', '<dict>', 'M', 'S203', 'S218']
2021-11-23 13:32:13,796 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁Verse', '6']
2021-11-23 13:32:13,796 - INFO - joeynmt.training - 	Source:     <2en> <4us> <dict> M S20340 S21800
2021-11-23 13:32:13,796 - INFO - joeynmt.training - 	Factor:     <2en> <4us> <dict> M S203 S218
2021-11-23 13:32:13,796 - INFO - joeynmt.training - 	Reference:  ▁m il k
2021-11-23 13:32:13,796 - INFO - joeynmt.training - 	Hypothesis: ▁Verse 6
2021-11-23 13:32:13,796 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step     2000: bleu:   0.03, loss: 136109.1250, ppl:  55.6416, duration: 542.8065s
2021-11-23 13:32:26,538 - INFO - joeynmt.training - Epoch   1, Step:     2100, Batch Loss:     4.192682, Tokens per Sec:     2261, Lr: 0.000100
2021-11-23 13:32:39,160 - INFO - joeynmt.training - Epoch   1, Step:     2200, Batch Loss:     3.785859, Tokens per Sec:     2301, Lr: 0.000100
2021-11-23 13:32:51,770 - INFO - joeynmt.training - Epoch   1, Step:     2300, Batch Loss:     3.881624, Tokens per Sec:     2288, Lr: 0.000100
2021-11-23 13:33:04,584 - INFO - joeynmt.training - Epoch   1, Step:     2400, Batch Loss:     3.823195, Tokens per Sec:     2250, Lr: 0.000100
2021-11-23 13:33:17,382 - INFO - joeynmt.training - Epoch   1, Step:     2500, Batch Loss:     3.649605, Tokens per Sec:     2391, Lr: 0.000100
2021-11-23 13:33:29,993 - INFO - joeynmt.training - Epoch   1, Step:     2600, Batch Loss:     3.921522, Tokens per Sec:     2263, Lr: 0.000100
2021-11-23 13:33:42,752 - INFO - joeynmt.training - Epoch   1, Step:     2700, Batch Loss:     3.874722, Tokens per Sec:     2318, Lr: 0.000100
2021-11-23 13:33:55,379 - INFO - joeynmt.training - Epoch   1, Step:     2800, Batch Loss:     3.907628, Tokens per Sec:     2311, Lr: 0.000100
2021-11-23 13:34:08,111 - INFO - joeynmt.training - Epoch   1, Step:     2900, Batch Loss:     3.802755, Tokens per Sec:     2235, Lr: 0.000100
2021-11-23 13:34:20,828 - INFO - joeynmt.training - Epoch   1, Step:     3000, Batch Loss:     3.809938, Tokens per Sec:     2272, Lr: 0.000100
2021-11-23 13:40:13,072 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-11-23 13:40:14,535 - INFO - joeynmt.helpers - delete models/baseline_multilingual_mem/2000.ckpt
2021-11-23 13:40:14,536 - INFO - joeynmt.helpers - delete /net/cephfs/home/zifjia/signwriting-translation/models/baseline_multilingual_mem/2000.ckpt
2021-11-23 13:40:14,536 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /net/cephfs/home/zifjia/signwriting-translation/models/baseline_multilingual_mem/2000.ckpt but file does not exist. ([Errno 2] No such file or directory: '/net/cephfs/home/zifjia/signwriting-translation/models/baseline_multilingual_mem/2000.ckpt')
2021-11-23 13:40:14,543 - INFO - joeynmt.training - Example #0
2021-11-23 13:40:14,543 - DEBUG - joeynmt.training - 	Raw source:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a07', 'S1f010', 'S26507', 'M', 'S11e20', 'S1a520', 'P', 'S38800', 'M', 'S15a56', 'S15a41', 'S2b700', 'S37c06', 'S36d01', 'S30d00', 'M', 'S10040', 'P', 'S38700', 'M', 'S37806', 'S10050', 'S20356', 'S36d01', 'M', 'S15a37', 'S1ce51', 'S26a07', 'P', 'S38700', 'M', 'S19220', 'S2a20c', 'S11520', 'S10120', 'S1f720', 'S20320', 'S36d01', 'M', 'S10018', 'S26505', 'S10641', 'M', 'S1f000', 'S1f720', 'S1dc20', 'S19220', 'S14a20', 'S1dc20', 'S14a20', 'P', 'S38800', 'M', 'S10041', 'S30d00', 'S36d01', 'M', 'S15a31', 'S10018', 'S26503', 'M', 'S2e74c', 'S14220', 'S2e700', 'S14228', 'M', 'S26521', 'S1f548', 'S1f540', 'M', 'S10041', 'P', 'S38700', 'M', 'S10021', 'S10029', 'S22a07', 'S22a11', 'S36d01', 'M', 'S2ea3e', 'S10000', 'M', 'S15a18', 'S10051', 'S26601', 'M', 'S10058', 'S26526', 'S10050', 'S20500', 'S20500', 'P', 'S38900', 'M', 'S15a37', 'S15a01', 'S2e900', 'S36d01', 'M', 'S15a20', 'S26501', 'M', 'S26521', 'S23b0a', 'S1f548', 'S15a40', 'S15a48', 'S23b1a', 'S1f540', 'M', 'S14c57', 'S14c5f', 'S26511', 'S26507', 'S22527', 'S20314', 'S22521', 'S2031c', 'P', 'S38800']
2021-11-23 13:40:14,543 - DEBUG - joeynmt.training - 	Raw factor:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a', 'S1f0', 'S265', 'M', 'S11e', 'S1a5', 'P', 'S388', 'M', 'S15a', 'S15a', 'S2b7', 'S37c', 'S36d', 'S30d', 'M', 'S100', 'P', 'S387', 'M', 'S378', 'S100', 'S203', 'S36d', 'M', 'S15a', 'S1ce', 'S26a', 'P', 'S387', 'M', 'S192', 'S2a2', 'S115', 'S101', 'S1f7', 'S203', 'S36d', 'M', 'S100', 'S265', 'S106', 'M', 'S1f0', 'S1f7', 'S1dc', 'S192', 'S14a', 'S1dc', 'S14a', 'P', 'S388', 'M', 'S100', 'S30d', 'S36d', 'M', 'S15a', 'S100', 'S265', 'M', 'S2e7', 'S142', 'S2e7', 'S142', 'M', 'S265', 'S1f5', 'S1f5', 'M', 'S100', 'P', 'S387', 'M', 'S100', 'S100', 'S22a', 'S22a', 'S36d', 'M', 'S2ea', 'S100', 'M', 'S15a', 'S100', 'S266', 'M', 'S100', 'S265', 'S100', 'S205', 'S205', 'P', 'S389', 'M', 'S15a', 'S15a', 'S2e9', 'S36d', 'M', 'S15a', 'S265', 'M', 'S265', 'S23b', 'S1f5', 'S15a', 'S15a', 'S23b', 'S1f5', 'M', 'S14c', 'S14c', 'S265', 'S265', 'S225', 'S203', 'S225', 'S203', 'P', 'S388']
2021-11-23 13:40:14,543 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁Verse', '▁3', '▁3', '7', '▁¶', '▁¶', '▁¶', '▁"', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', '▁"', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T']
2021-11-23 13:40:14,543 - INFO - joeynmt.training - 	Source:     <2en> <4us> <sent> M S15a07 S1f010 S26507 M S11e20 S1a520 P S38800 M S15a56 S15a41 S2b700 S37c06 S36d01 S30d00 M S10040 P S38700 M S37806 S10050 S20356 S36d01 M S15a37 S1ce51 S26a07 P S38700 M S19220 S2a20c S11520 S10120 S1f720 S20320 S36d01 M S10018 S26505 S10641 M S1f000 S1f720 S1dc20 S19220 S14a20 S1dc20 S14a20 P S38800 M S10041 S30d00 S36d01 M S15a31 S10018 S26503 M S2e74c S14220 S2e700 S14228 M S26521 S1f548 S1f540 M S10041 P S38700 M S10021 S10029 S22a07 S22a11 S36d01 M S2ea3e S10000 M S15a18 S10051 S26601 M S10058 S26526 S10050 S20500 S20500 P S38900 M S15a37 S15a01 S2e900 S36d01 M S15a20 S26501 M S26521 S23b0a S1f548 S15a40 S15a48 S23b1a S1f540 M S14c57 S14c5f S26511 S26507 S22527 S20314 S22521 S2031c P S38800
2021-11-23 13:40:14,543 - INFO - joeynmt.training - 	Factor:     <2en> <4us> <sent> M S15a S1f0 S265 M S11e S1a5 P S388 M S15a S15a S2b7 S37c S36d S30d M S100 P S387 M S378 S100 S203 S36d M S15a S1ce S26a P S387 M S192 S2a2 S115 S101 S1f7 S203 S36d M S100 S265 S106 M S1f0 S1f7 S1dc S192 S14a S1dc S14a P S388 M S100 S30d S36d M S15a S100 S265 M S2e7 S142 S2e7 S142 M S265 S1f5 S1f5 M S100 P S387 M S100 S100 S22a S22a S36d M S2ea S100 M S15a S100 S266 M S100 S265 S100 S205 S205 P S389 M S15a S15a S2e9 S36d M S15a S265 M S265 S23b S1f5 S15a S15a S23b S1f5 M S14c S14c S265 S265 S225 S203 S225 S203 P S388
2021-11-23 13:40:14,543 - INFO - joeynmt.training - 	Reference:  ▁Verse ▁3 7. ▁After ▁him , ▁at ▁the ▁time ▁of ▁the ▁c ens us , ▁there ▁was ▁Jud as ▁of ▁Gal ile e . ▁He ▁g ot ▁people ▁to ▁follow ▁him , ▁but ▁he ▁was ▁k illed , ▁too , ▁and ▁all ▁his ▁follow ers ▁were ▁sc atter ed .
2021-11-23 13:40:14,543 - INFO - joeynmt.training - 	Hypothesis: ▁Verse ▁3 ▁3 7 ▁¶ ▁¶ ▁¶ ▁" I I I I I I I I I ▁" T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T
2021-11-23 13:40:14,543 - INFO - joeynmt.training - Example #1
2021-11-23 13:40:14,543 - DEBUG - joeynmt.training - 	Raw source:     ['<2pt>', '<4br>', '<dict>', 'M', 'S34700', 'S21100', 'S1f410']
2021-11-23 13:40:14,543 - DEBUG - joeynmt.training - 	Raw factor:     ['<2pt>', '<4br>', '<dict>', 'M', 'S347', 'S211', 'S1f4']
2021-11-23 13:40:14,543 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁P', 'a']
2021-11-23 13:40:14,543 - INFO - joeynmt.training - 	Source:     <2pt> <4br> <dict> M S34700 S21100 S1f410
2021-11-23 13:40:14,543 - INFO - joeynmt.training - 	Factor:     <2pt> <4br> <dict> M S347 S211 S1f4
2021-11-23 13:40:14,543 - INFO - joeynmt.training - 	Reference:  ▁L á p is
2021-11-23 13:40:14,543 - INFO - joeynmt.training - 	Hypothesis: ▁P a
2021-11-23 13:40:14,543 - INFO - joeynmt.training - Example #2
2021-11-23 13:40:14,543 - DEBUG - joeynmt.training - 	Raw source:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a07', 'S1f010', 'S26507', 'M', 'S10e00', 'P', 'S38800', 'M', 'S1dc0a', 'S10003', 'S20500', 'S20500', 'S30a00', 'M', 'S20302', 'S2030a', 'S20340', 'S20348', 'S30e00', 'S30124', 'M', 'S20500', 'S10013', 'M', 'S30124', 'S10010', 'S26500', 'M', 'S22f10', 'S20500', 'S15a0a', 'S22f00', 'S15a02', 'S20500', 'P', 'S38900', 'M', 'S17911', 'S17919', 'S2c300', 'S2c311', 'S30a00', 'P', 'S38700', 'M', 'S15a06', 'S15a41', 'S23d04', 'M', 'S2ff00', 'S10050', 'S10058', 'S10011', 'S22a04', 'M', 'S1f522', 'S2ea46', 'S2ea02', 'S1f50a', 'P', 'S38700', 'R', 'S3770b', 'S20305', 'S20500', 'S20303', 'S20500', 'S37713', 'S30e00', 'S30124', 'R', 'S1f522', 'S2ea46', 'S2ea02', 'S1f50a', 'P', 'S38700', 'M', 'S20356', 'S20350', 'S37706', 'S22f04', 'S30e00', 'S30124', 'M', 'S1f740', 'S1f748', 'S26a20', 'M', 'S1814b', 'S20500', 'S20500', 'S18143', 'R', 'S10000', 'S26500', 'R', 'S10011', 'S2ff00', 'P', 'S38700', 'M', 'S10000', 'S26500', 'M', 'S15a18', 'S15a18', 'S10e22', 'S10e02', 'P', 'S38800']
2021-11-23 13:40:14,543 - DEBUG - joeynmt.training - 	Raw factor:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a', 'S1f0', 'S265', 'M', 'S10e', 'P', 'S388', 'M', 'S1dc', 'S100', 'S205', 'S205', 'S30a', 'M', 'S203', 'S203', 'S203', 'S203', 'S30e', 'S301', 'M', 'S205', 'S100', 'M', 'S301', 'S100', 'S265', 'M', 'S22f', 'S205', 'S15a', 'S22f', 'S15a', 'S205', 'P', 'S389', 'M', 'S179', 'S179', 'S2c3', 'S2c3', 'S30a', 'P', 'S387', 'M', 'S15a', 'S15a', 'S23d', 'M', 'S2ff', 'S100', 'S100', 'S100', 'S22a', 'M', 'S1f5', 'S2ea', 'S2ea', 'S1f5', 'P', 'S387', 'R', 'S377', 'S203', 'S205', 'S203', 'S205', 'S377', 'S30e', 'S301', 'R', 'S1f5', 'S2ea', 'S2ea', 'S1f5', 'P', 'S387', 'M', 'S203', 'S203', 'S377', 'S22f', 'S30e', 'S301', 'M', 'S1f7', 'S1f7', 'S26a', 'M', 'S181', 'S205', 'S205', 'S181', 'R', 'S100', 'S265', 'R', 'S100', 'S2ff', 'P', 'S387', 'M', 'S100', 'S265', 'M', 'S15a', 'S15a', 'S10e', 'S10e', 'P', 'S388']
2021-11-23 13:40:14,543 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁Verse', '▁3', '▁3', '7', '▁¶', '▁¶', '▁"', 'I', 'I', 'I', 'I', '▁¶', '▁¶', '▁"', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T']
2021-11-23 13:40:14,543 - INFO - joeynmt.training - 	Source:     <2en> <4us> <sent> M S15a07 S1f010 S26507 M S10e00 P S38800 M S1dc0a S10003 S20500 S20500 S30a00 M S20302 S2030a S20340 S20348 S30e00 S30124 M S20500 S10013 M S30124 S10010 S26500 M S22f10 S20500 S15a0a S22f00 S15a02 S20500 P S38900 M S17911 S17919 S2c300 S2c311 S30a00 P S38700 M S15a06 S15a41 S23d04 M S2ff00 S10050 S10058 S10011 S22a04 M S1f522 S2ea46 S2ea02 S1f50a P S38700 R S3770b S20305 S20500 S20303 S20500 S37713 S30e00 S30124 R S1f522 S2ea46 S2ea02 S1f50a P S38700 M S20356 S20350 S37706 S22f04 S30e00 S30124 M S1f740 S1f748 S26a20 M S1814b S20500 S20500 S18143 R S10000 S26500 R S10011 S2ff00 P S38700 M S10000 S26500 M S15a18 S15a18 S10e22 S10e02 P S38800
2021-11-23 13:40:14,543 - INFO - joeynmt.training - 	Factor:     <2en> <4us> <sent> M S15a S1f0 S265 M S10e P S388 M S1dc S100 S205 S205 S30a M S203 S203 S203 S203 S30e S301 M S205 S100 M S301 S100 S265 M S22f S205 S15a S22f S15a S205 P S389 M S179 S179 S2c3 S2c3 S30a P S387 M S15a S15a S23d M S2ff S100 S100 S100 S22a M S1f5 S2ea S2ea S1f5 P S387 R S377 S203 S205 S203 S205 S377 S30e S301 R S1f5 S2ea S2ea S1f5 P S387 M S203 S203 S377 S22f S30e S301 M S1f7 S1f7 S26a M S181 S205 S205 S181 R S100 S265 R S100 S2ff P S387 M S100 S265 M S15a S15a S10e S10e P S388
2021-11-23 13:40:14,543 - INFO - joeynmt.training - 	Reference:  ▁Verse ▁2. ▁Then ▁make ▁me ▁tr ul y ▁hap p y ▁by ▁ag ree ing ▁whole he art ed ly ▁with ▁each ▁other , ▁lo v ing ▁one ▁another , ▁and ▁work ing ▁together ▁with ▁one ▁m ind ▁and ▁p ur p ose .
2021-11-23 13:40:14,543 - INFO - joeynmt.training - 	Hypothesis: ▁Verse ▁3 ▁3 7 ▁¶ ▁¶ ▁" I I I I ▁¶ ▁¶ ▁" T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T
2021-11-23 13:40:14,543 - INFO - joeynmt.training - Example #3
2021-11-23 13:40:14,544 - DEBUG - joeynmt.training - 	Raw source:     ['<2pt>', '<4br>', '<dict>', 'M', 'S10e40', 'S11040', 'S26504', 'S2fb04']
2021-11-23 13:40:14,544 - DEBUG - joeynmt.training - 	Raw factor:     ['<2pt>', '<4br>', '<dict>', 'M', 'S10e', 'S110', 'S265', 'S2fb']
2021-11-23 13:40:14,544 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁P', 'a']
2021-11-23 13:40:14,544 - INFO - joeynmt.training - 	Source:     <2pt> <4br> <dict> M S10e40 S11040 S26504 S2fb04
2021-11-23 13:40:14,544 - INFO - joeynmt.training - 	Factor:     <2pt> <4br> <dict> M S10e S110 S265 S2fb
2021-11-23 13:40:14,544 - INFO - joeynmt.training - 	Reference:  ▁expl or ar
2021-11-23 13:40:14,544 - INFO - joeynmt.training - 	Hypothesis: ▁P a
2021-11-23 13:40:14,544 - INFO - joeynmt.training - Example #6
2021-11-23 13:40:14,544 - DEBUG - joeynmt.training - 	Raw source:     ['<2en>', '<4us>', '<dict>', 'M', 'S20340', 'S21800']
2021-11-23 13:40:14,544 - DEBUG - joeynmt.training - 	Raw factor:     ['<2en>', '<4us>', '<dict>', 'M', 'S203', 'S218']
2021-11-23 13:40:14,544 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁p', 'er']
2021-11-23 13:40:14,544 - INFO - joeynmt.training - 	Source:     <2en> <4us> <dict> M S20340 S21800
2021-11-23 13:40:14,544 - INFO - joeynmt.training - 	Factor:     <2en> <4us> <dict> M S203 S218
2021-11-23 13:40:14,544 - INFO - joeynmt.training - 	Reference:  ▁m il k
2021-11-23 13:40:14,544 - INFO - joeynmt.training - 	Hypothesis: ▁p er
2021-11-23 13:40:14,544 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step     3000: bleu:   0.08, loss: 130465.8203, ppl:  47.1012, duration: 353.7154s
2021-11-23 13:40:27,197 - INFO - joeynmt.training - Epoch   1, Step:     3100, Batch Loss:     3.765104, Tokens per Sec:     2275, Lr: 0.000100
2021-11-23 13:40:39,817 - INFO - joeynmt.training - Epoch   1, Step:     3200, Batch Loss:     3.820812, Tokens per Sec:     2234, Lr: 0.000100
2021-11-23 13:40:52,302 - INFO - joeynmt.training - Epoch   1, Step:     3300, Batch Loss:     3.796361, Tokens per Sec:     2208, Lr: 0.000100
2021-11-23 13:41:05,066 - INFO - joeynmt.training - Epoch   1, Step:     3400, Batch Loss:     3.903196, Tokens per Sec:     2301, Lr: 0.000100
2021-11-23 13:41:17,855 - INFO - joeynmt.training - Epoch   1, Step:     3500, Batch Loss:     3.835144, Tokens per Sec:     2231, Lr: 0.000100
2021-11-23 13:41:30,592 - INFO - joeynmt.training - Epoch   1, Step:     3600, Batch Loss:     3.840024, Tokens per Sec:     2341, Lr: 0.000100
2021-11-23 13:41:30,945 - INFO - joeynmt.training - Epoch   1: total training loss 14856.17
2021-11-23 13:41:30,945 - INFO - joeynmt.training - EPOCH 2
2021-11-23 13:41:43,366 - INFO - joeynmt.training - Epoch   2, Step:     3700, Batch Loss:     3.975380, Tokens per Sec:     2229, Lr: 0.000100
2021-11-23 13:41:55,859 - INFO - joeynmt.training - Epoch   2, Step:     3800, Batch Loss:     3.888113, Tokens per Sec:     2268, Lr: 0.000100
2021-11-23 13:42:08,675 - INFO - joeynmt.training - Epoch   2, Step:     3900, Batch Loss:     3.902678, Tokens per Sec:     2261, Lr: 0.000100
2021-11-23 13:42:21,304 - INFO - joeynmt.training - Epoch   2, Step:     4000, Batch Loss:     3.775831, Tokens per Sec:     2284, Lr: 0.000100
2021-11-23 13:44:22,066 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-11-23 13:44:22,066 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-11-23 13:44:22,066 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-11-23 13:44:22,075 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-11-23 13:44:23,550 - INFO - joeynmt.helpers - delete models/baseline_multilingual_mem/3000.ckpt
2021-11-23 13:44:23,551 - INFO - joeynmt.helpers - delete /net/cephfs/home/zifjia/signwriting-translation/models/baseline_multilingual_mem/3000.ckpt
2021-11-23 13:44:23,551 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /net/cephfs/home/zifjia/signwriting-translation/models/baseline_multilingual_mem/3000.ckpt but file does not exist. ([Errno 2] No such file or directory: '/net/cephfs/home/zifjia/signwriting-translation/models/baseline_multilingual_mem/3000.ckpt')
2021-11-23 13:44:23,563 - INFO - joeynmt.training - Example #0
2021-11-23 13:44:23,563 - DEBUG - joeynmt.training - 	Raw source:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a07', 'S1f010', 'S26507', 'M', 'S11e20', 'S1a520', 'P', 'S38800', 'M', 'S15a56', 'S15a41', 'S2b700', 'S37c06', 'S36d01', 'S30d00', 'M', 'S10040', 'P', 'S38700', 'M', 'S37806', 'S10050', 'S20356', 'S36d01', 'M', 'S15a37', 'S1ce51', 'S26a07', 'P', 'S38700', 'M', 'S19220', 'S2a20c', 'S11520', 'S10120', 'S1f720', 'S20320', 'S36d01', 'M', 'S10018', 'S26505', 'S10641', 'M', 'S1f000', 'S1f720', 'S1dc20', 'S19220', 'S14a20', 'S1dc20', 'S14a20', 'P', 'S38800', 'M', 'S10041', 'S30d00', 'S36d01', 'M', 'S15a31', 'S10018', 'S26503', 'M', 'S2e74c', 'S14220', 'S2e700', 'S14228', 'M', 'S26521', 'S1f548', 'S1f540', 'M', 'S10041', 'P', 'S38700', 'M', 'S10021', 'S10029', 'S22a07', 'S22a11', 'S36d01', 'M', 'S2ea3e', 'S10000', 'M', 'S15a18', 'S10051', 'S26601', 'M', 'S10058', 'S26526', 'S10050', 'S20500', 'S20500', 'P', 'S38900', 'M', 'S15a37', 'S15a01', 'S2e900', 'S36d01', 'M', 'S15a20', 'S26501', 'M', 'S26521', 'S23b0a', 'S1f548', 'S15a40', 'S15a48', 'S23b1a', 'S1f540', 'M', 'S14c57', 'S14c5f', 'S26511', 'S26507', 'S22527', 'S20314', 'S22521', 'S2031c', 'P', 'S38800']
2021-11-23 13:44:23,563 - DEBUG - joeynmt.training - 	Raw factor:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a', 'S1f0', 'S265', 'M', 'S11e', 'S1a5', 'P', 'S388', 'M', 'S15a', 'S15a', 'S2b7', 'S37c', 'S36d', 'S30d', 'M', 'S100', 'P', 'S387', 'M', 'S378', 'S100', 'S203', 'S36d', 'M', 'S15a', 'S1ce', 'S26a', 'P', 'S387', 'M', 'S192', 'S2a2', 'S115', 'S101', 'S1f7', 'S203', 'S36d', 'M', 'S100', 'S265', 'S106', 'M', 'S1f0', 'S1f7', 'S1dc', 'S192', 'S14a', 'S1dc', 'S14a', 'P', 'S388', 'M', 'S100', 'S30d', 'S36d', 'M', 'S15a', 'S100', 'S265', 'M', 'S2e7', 'S142', 'S2e7', 'S142', 'M', 'S265', 'S1f5', 'S1f5', 'M', 'S100', 'P', 'S387', 'M', 'S100', 'S100', 'S22a', 'S22a', 'S36d', 'M', 'S2ea', 'S100', 'M', 'S15a', 'S100', 'S266', 'M', 'S100', 'S265', 'S100', 'S205', 'S205', 'P', 'S389', 'M', 'S15a', 'S15a', 'S2e9', 'S36d', 'M', 'S15a', 'S265', 'M', 'S265', 'S23b', 'S1f5', 'S15a', 'S15a', 'S23b', 'S1f5', 'M', 'S14c', 'S14c', 'S265', 'S265', 'S225', 'S203', 'S225', 'S203', 'P', 'S388']
2021-11-23 13:44:23,564 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁Verse', '▁3', '▁And', '▁the', '▁LORD', ',', '▁and', '▁the', '▁LORD', ',', '▁and', '▁the', '▁LORD', ',', '▁and', '▁the', '▁LORD', ',', '▁and', '▁the', '▁LORD', ',', '▁and', '▁the', '▁LORD', ',', '▁and', '▁the', '▁LORD', '.']
2021-11-23 13:44:23,564 - INFO - joeynmt.training - 	Source:     <2en> <4us> <sent> M S15a07 S1f010 S26507 M S11e20 S1a520 P S38800 M S15a56 S15a41 S2b700 S37c06 S36d01 S30d00 M S10040 P S38700 M S37806 S10050 S20356 S36d01 M S15a37 S1ce51 S26a07 P S38700 M S19220 S2a20c S11520 S10120 S1f720 S20320 S36d01 M S10018 S26505 S10641 M S1f000 S1f720 S1dc20 S19220 S14a20 S1dc20 S14a20 P S38800 M S10041 S30d00 S36d01 M S15a31 S10018 S26503 M S2e74c S14220 S2e700 S14228 M S26521 S1f548 S1f540 M S10041 P S38700 M S10021 S10029 S22a07 S22a11 S36d01 M S2ea3e S10000 M S15a18 S10051 S26601 M S10058 S26526 S10050 S20500 S20500 P S38900 M S15a37 S15a01 S2e900 S36d01 M S15a20 S26501 M S26521 S23b0a S1f548 S15a40 S15a48 S23b1a S1f540 M S14c57 S14c5f S26511 S26507 S22527 S20314 S22521 S2031c P S38800
2021-11-23 13:44:23,564 - INFO - joeynmt.training - 	Factor:     <2en> <4us> <sent> M S15a S1f0 S265 M S11e S1a5 P S388 M S15a S15a S2b7 S37c S36d S30d M S100 P S387 M S378 S100 S203 S36d M S15a S1ce S26a P S387 M S192 S2a2 S115 S101 S1f7 S203 S36d M S100 S265 S106 M S1f0 S1f7 S1dc S192 S14a S1dc S14a P S388 M S100 S30d S36d M S15a S100 S265 M S2e7 S142 S2e7 S142 M S265 S1f5 S1f5 M S100 P S387 M S100 S100 S22a S22a S36d M S2ea S100 M S15a S100 S266 M S100 S265 S100 S205 S205 P S389 M S15a S15a S2e9 S36d M S15a S265 M S265 S23b S1f5 S15a S15a S23b S1f5 M S14c S14c S265 S265 S225 S203 S225 S203 P S388
2021-11-23 13:44:23,564 - INFO - joeynmt.training - 	Reference:  ▁Verse ▁3 7. ▁After ▁him , ▁at ▁the ▁time ▁of ▁the ▁c ens us , ▁there ▁was ▁Jud as ▁of ▁Gal ile e . ▁He ▁g ot ▁people ▁to ▁follow ▁him , ▁but ▁he ▁was ▁k illed , ▁too , ▁and ▁all ▁his ▁follow ers ▁were ▁sc atter ed .
2021-11-23 13:44:23,564 - INFO - joeynmt.training - 	Hypothesis: ▁Verse ▁3 ▁And ▁the ▁LORD , ▁and ▁the ▁LORD , ▁and ▁the ▁LORD , ▁and ▁the ▁LORD , ▁and ▁the ▁LORD , ▁and ▁the ▁LORD , ▁and ▁the ▁LORD .
2021-11-23 13:44:23,564 - INFO - joeynmt.training - Example #1
2021-11-23 13:44:23,564 - DEBUG - joeynmt.training - 	Raw source:     ['<2pt>', '<4br>', '<dict>', 'M', 'S34700', 'S21100', 'S1f410']
2021-11-23 13:44:23,564 - DEBUG - joeynmt.training - 	Raw factor:     ['<2pt>', '<4br>', '<dict>', 'M', 'S347', 'S211', 'S1f4']
2021-11-23 13:44:23,564 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁C', 'A']
2021-11-23 13:44:23,564 - INFO - joeynmt.training - 	Source:     <2pt> <4br> <dict> M S34700 S21100 S1f410
2021-11-23 13:44:23,564 - INFO - joeynmt.training - 	Factor:     <2pt> <4br> <dict> M S347 S211 S1f4
2021-11-23 13:44:23,564 - INFO - joeynmt.training - 	Reference:  ▁L á p is
2021-11-23 13:44:23,564 - INFO - joeynmt.training - 	Hypothesis: ▁C A
2021-11-23 13:44:23,564 - INFO - joeynmt.training - Example #2
2021-11-23 13:44:23,564 - DEBUG - joeynmt.training - 	Raw source:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a07', 'S1f010', 'S26507', 'M', 'S10e00', 'P', 'S38800', 'M', 'S1dc0a', 'S10003', 'S20500', 'S20500', 'S30a00', 'M', 'S20302', 'S2030a', 'S20340', 'S20348', 'S30e00', 'S30124', 'M', 'S20500', 'S10013', 'M', 'S30124', 'S10010', 'S26500', 'M', 'S22f10', 'S20500', 'S15a0a', 'S22f00', 'S15a02', 'S20500', 'P', 'S38900', 'M', 'S17911', 'S17919', 'S2c300', 'S2c311', 'S30a00', 'P', 'S38700', 'M', 'S15a06', 'S15a41', 'S23d04', 'M', 'S2ff00', 'S10050', 'S10058', 'S10011', 'S22a04', 'M', 'S1f522', 'S2ea46', 'S2ea02', 'S1f50a', 'P', 'S38700', 'R', 'S3770b', 'S20305', 'S20500', 'S20303', 'S20500', 'S37713', 'S30e00', 'S30124', 'R', 'S1f522', 'S2ea46', 'S2ea02', 'S1f50a', 'P', 'S38700', 'M', 'S20356', 'S20350', 'S37706', 'S22f04', 'S30e00', 'S30124', 'M', 'S1f740', 'S1f748', 'S26a20', 'M', 'S1814b', 'S20500', 'S20500', 'S18143', 'R', 'S10000', 'S26500', 'R', 'S10011', 'S2ff00', 'P', 'S38700', 'M', 'S10000', 'S26500', 'M', 'S15a18', 'S15a18', 'S10e22', 'S10e02', 'P', 'S38800']
2021-11-23 13:44:23,564 - DEBUG - joeynmt.training - 	Raw factor:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a', 'S1f0', 'S265', 'M', 'S10e', 'P', 'S388', 'M', 'S1dc', 'S100', 'S205', 'S205', 'S30a', 'M', 'S203', 'S203', 'S203', 'S203', 'S30e', 'S301', 'M', 'S205', 'S100', 'M', 'S301', 'S100', 'S265', 'M', 'S22f', 'S205', 'S15a', 'S22f', 'S15a', 'S205', 'P', 'S389', 'M', 'S179', 'S179', 'S2c3', 'S2c3', 'S30a', 'P', 'S387', 'M', 'S15a', 'S15a', 'S23d', 'M', 'S2ff', 'S100', 'S100', 'S100', 'S22a', 'M', 'S1f5', 'S2ea', 'S2ea', 'S1f5', 'P', 'S387', 'R', 'S377', 'S203', 'S205', 'S203', 'S205', 'S377', 'S30e', 'S301', 'R', 'S1f5', 'S2ea', 'S2ea', 'S1f5', 'P', 'S387', 'M', 'S203', 'S203', 'S377', 'S22f', 'S30e', 'S301', 'M', 'S1f7', 'S1f7', 'S26a', 'M', 'S181', 'S205', 'S205', 'S181', 'R', 'S100', 'S265', 'R', 'S100', 'S2ff', 'P', 'S387', 'M', 'S100', 'S265', 'M', 'S15a', 'S15a', 'S10e', 'S10e', 'P', 'S388']
2021-11-23 13:44:23,564 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁Verse', '▁3', '▁And', '▁the', '▁LORD', ',', '▁"', 'I', "'", 't', ',', '▁and', '▁the', '▁LORD', ',', '▁and', '▁the', '▁LORD', ',', '▁and', '▁the', '▁LORD', ',', '▁and', '▁the', '▁LORD', ',', '▁and', '▁the', '▁LORD', '.']
2021-11-23 13:44:23,564 - INFO - joeynmt.training - 	Source:     <2en> <4us> <sent> M S15a07 S1f010 S26507 M S10e00 P S38800 M S1dc0a S10003 S20500 S20500 S30a00 M S20302 S2030a S20340 S20348 S30e00 S30124 M S20500 S10013 M S30124 S10010 S26500 M S22f10 S20500 S15a0a S22f00 S15a02 S20500 P S38900 M S17911 S17919 S2c300 S2c311 S30a00 P S38700 M S15a06 S15a41 S23d04 M S2ff00 S10050 S10058 S10011 S22a04 M S1f522 S2ea46 S2ea02 S1f50a P S38700 R S3770b S20305 S20500 S20303 S20500 S37713 S30e00 S30124 R S1f522 S2ea46 S2ea02 S1f50a P S38700 M S20356 S20350 S37706 S22f04 S30e00 S30124 M S1f740 S1f748 S26a20 M S1814b S20500 S20500 S18143 R S10000 S26500 R S10011 S2ff00 P S38700 M S10000 S26500 M S15a18 S15a18 S10e22 S10e02 P S38800
2021-11-23 13:44:23,564 - INFO - joeynmt.training - 	Factor:     <2en> <4us> <sent> M S15a S1f0 S265 M S10e P S388 M S1dc S100 S205 S205 S30a M S203 S203 S203 S203 S30e S301 M S205 S100 M S301 S100 S265 M S22f S205 S15a S22f S15a S205 P S389 M S179 S179 S2c3 S2c3 S30a P S387 M S15a S15a S23d M S2ff S100 S100 S100 S22a M S1f5 S2ea S2ea S1f5 P S387 R S377 S203 S205 S203 S205 S377 S30e S301 R S1f5 S2ea S2ea S1f5 P S387 M S203 S203 S377 S22f S30e S301 M S1f7 S1f7 S26a M S181 S205 S205 S181 R S100 S265 R S100 S2ff P S387 M S100 S265 M S15a S15a S10e S10e P S388
2021-11-23 13:44:23,564 - INFO - joeynmt.training - 	Reference:  ▁Verse ▁2. ▁Then ▁make ▁me ▁tr ul y ▁hap p y ▁by ▁ag ree ing ▁whole he art ed ly ▁with ▁each ▁other , ▁lo v ing ▁one ▁another , ▁and ▁work ing ▁together ▁with ▁one ▁m ind ▁and ▁p ur p ose .
2021-11-23 13:44:23,565 - INFO - joeynmt.training - 	Hypothesis: ▁Verse ▁3 ▁And ▁the ▁LORD , ▁" I ' t , ▁and ▁the ▁LORD , ▁and ▁the ▁LORD , ▁and ▁the ▁LORD , ▁and ▁the ▁LORD , ▁and ▁the ▁LORD .
2021-11-23 13:44:23,565 - INFO - joeynmt.training - Example #3
2021-11-23 13:44:23,565 - DEBUG - joeynmt.training - 	Raw source:     ['<2pt>', '<4br>', '<dict>', 'M', 'S10e40', 'S11040', 'S26504', 'S2fb04']
2021-11-23 13:44:23,565 - DEBUG - joeynmt.training - 	Raw factor:     ['<2pt>', '<4br>', '<dict>', 'M', 'S10e', 'S110', 'S265', 'S2fb']
2021-11-23 13:44:23,565 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁C', 'A']
2021-11-23 13:44:23,565 - INFO - joeynmt.training - 	Source:     <2pt> <4br> <dict> M S10e40 S11040 S26504 S2fb04
2021-11-23 13:44:23,565 - INFO - joeynmt.training - 	Factor:     <2pt> <4br> <dict> M S10e S110 S265 S2fb
2021-11-23 13:44:23,565 - INFO - joeynmt.training - 	Reference:  ▁expl or ar
2021-11-23 13:44:23,565 - INFO - joeynmt.training - 	Hypothesis: ▁C A
2021-11-23 13:44:23,565 - INFO - joeynmt.training - Example #6
2021-11-23 13:44:23,565 - DEBUG - joeynmt.training - 	Raw source:     ['<2en>', '<4us>', '<dict>', 'M', 'S20340', 'S21800']
2021-11-23 13:44:23,565 - DEBUG - joeynmt.training - 	Raw factor:     ['<2en>', '<4us>', '<dict>', 'M', 'S203', 'S218']
2021-11-23 13:44:23,565 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁b', 'at']
2021-11-23 13:44:23,565 - INFO - joeynmt.training - 	Source:     <2en> <4us> <dict> M S20340 S21800
2021-11-23 13:44:23,565 - INFO - joeynmt.training - 	Factor:     <2en> <4us> <dict> M S203 S218
2021-11-23 13:44:23,565 - INFO - joeynmt.training - 	Reference:  ▁m il k
2021-11-23 13:44:23,565 - INFO - joeynmt.training - 	Hypothesis: ▁b at
2021-11-23 13:44:23,565 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step     4000: bleu:   0.60, loss: 127675.3516, ppl:  43.3759, duration: 122.2606s
2021-11-23 13:44:36,351 - INFO - joeynmt.training - Epoch   2, Step:     4100, Batch Loss:     3.728872, Tokens per Sec:     2353, Lr: 0.000100
2021-11-23 13:44:48,893 - INFO - joeynmt.training - Epoch   2, Step:     4200, Batch Loss:     3.918212, Tokens per Sec:     2303, Lr: 0.000100
2021-11-23 13:45:01,409 - INFO - joeynmt.training - Epoch   2, Step:     4300, Batch Loss:     3.765236, Tokens per Sec:     2286, Lr: 0.000100
2021-11-23 13:45:13,876 - INFO - joeynmt.training - Epoch   2, Step:     4400, Batch Loss:     3.572428, Tokens per Sec:     2248, Lr: 0.000100
2021-11-23 13:45:26,541 - INFO - joeynmt.training - Epoch   2, Step:     4500, Batch Loss:     3.766511, Tokens per Sec:     2276, Lr: 0.000100
2021-11-23 13:45:39,300 - INFO - joeynmt.training - Epoch   2, Step:     4600, Batch Loss:     3.884321, Tokens per Sec:     2304, Lr: 0.000100
2021-11-23 13:45:51,999 - INFO - joeynmt.training - Epoch   2, Step:     4700, Batch Loss:     3.801060, Tokens per Sec:     2247, Lr: 0.000100
2021-11-23 13:46:04,686 - INFO - joeynmt.training - Epoch   2, Step:     4800, Batch Loss:     3.875112, Tokens per Sec:     2324, Lr: 0.000100
2021-11-23 13:46:17,336 - INFO - joeynmt.training - Epoch   2, Step:     4900, Batch Loss:     3.744943, Tokens per Sec:     2297, Lr: 0.000100
2021-11-23 13:46:29,970 - INFO - joeynmt.training - Epoch   2, Step:     5000, Batch Loss:     3.786514, Tokens per Sec:     2278, Lr: 0.000100
2021-11-23 13:48:41,232 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-11-23 13:48:41,232 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-11-23 13:48:41,232 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-11-23 13:48:41,241 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-11-23 13:48:46,703 - INFO - joeynmt.helpers - delete models/baseline_multilingual_mem/4000.ckpt
2021-11-23 13:48:46,705 - INFO - joeynmt.helpers - delete /net/cephfs/home/zifjia/signwriting-translation/models/baseline_multilingual_mem/4000.ckpt
2021-11-23 13:48:46,705 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /net/cephfs/home/zifjia/signwriting-translation/models/baseline_multilingual_mem/4000.ckpt but file does not exist. ([Errno 2] No such file or directory: '/net/cephfs/home/zifjia/signwriting-translation/models/baseline_multilingual_mem/4000.ckpt')
2021-11-23 13:48:46,720 - INFO - joeynmt.training - Example #0
2021-11-23 13:48:46,720 - DEBUG - joeynmt.training - 	Raw source:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a07', 'S1f010', 'S26507', 'M', 'S11e20', 'S1a520', 'P', 'S38800', 'M', 'S15a56', 'S15a41', 'S2b700', 'S37c06', 'S36d01', 'S30d00', 'M', 'S10040', 'P', 'S38700', 'M', 'S37806', 'S10050', 'S20356', 'S36d01', 'M', 'S15a37', 'S1ce51', 'S26a07', 'P', 'S38700', 'M', 'S19220', 'S2a20c', 'S11520', 'S10120', 'S1f720', 'S20320', 'S36d01', 'M', 'S10018', 'S26505', 'S10641', 'M', 'S1f000', 'S1f720', 'S1dc20', 'S19220', 'S14a20', 'S1dc20', 'S14a20', 'P', 'S38800', 'M', 'S10041', 'S30d00', 'S36d01', 'M', 'S15a31', 'S10018', 'S26503', 'M', 'S2e74c', 'S14220', 'S2e700', 'S14228', 'M', 'S26521', 'S1f548', 'S1f540', 'M', 'S10041', 'P', 'S38700', 'M', 'S10021', 'S10029', 'S22a07', 'S22a11', 'S36d01', 'M', 'S2ea3e', 'S10000', 'M', 'S15a18', 'S10051', 'S26601', 'M', 'S10058', 'S26526', 'S10050', 'S20500', 'S20500', 'P', 'S38900', 'M', 'S15a37', 'S15a01', 'S2e900', 'S36d01', 'M', 'S15a20', 'S26501', 'M', 'S26521', 'S23b0a', 'S1f548', 'S15a40', 'S15a48', 'S23b1a', 'S1f540', 'M', 'S14c57', 'S14c5f', 'S26511', 'S26507', 'S22527', 'S20314', 'S22521', 'S2031c', 'P', 'S38800']
2021-11-23 13:48:46,720 - DEBUG - joeynmt.training - 	Raw factor:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a', 'S1f0', 'S265', 'M', 'S11e', 'S1a5', 'P', 'S388', 'M', 'S15a', 'S15a', 'S2b7', 'S37c', 'S36d', 'S30d', 'M', 'S100', 'P', 'S387', 'M', 'S378', 'S100', 'S203', 'S36d', 'M', 'S15a', 'S1ce', 'S26a', 'P', 'S387', 'M', 'S192', 'S2a2', 'S115', 'S101', 'S1f7', 'S203', 'S36d', 'M', 'S100', 'S265', 'S106', 'M', 'S1f0', 'S1f7', 'S1dc', 'S192', 'S14a', 'S1dc', 'S14a', 'P', 'S388', 'M', 'S100', 'S30d', 'S36d', 'M', 'S15a', 'S100', 'S265', 'M', 'S2e7', 'S142', 'S2e7', 'S142', 'M', 'S265', 'S1f5', 'S1f5', 'M', 'S100', 'P', 'S387', 'M', 'S100', 'S100', 'S22a', 'S22a', 'S36d', 'M', 'S2ea', 'S100', 'M', 'S15a', 'S100', 'S266', 'M', 'S100', 'S265', 'S100', 'S205', 'S205', 'P', 'S389', 'M', 'S15a', 'S15a', 'S2e9', 'S36d', 'M', 'S15a', 'S265', 'M', 'S265', 'S23b', 'S1f5', 'S15a', 'S15a', 'S23b', 'S1f5', 'M', 'S14c', 'S14c', 'S265', 'S265', 'S225', 'S203', 'S225', 'S203', 'P', 'S388']
2021-11-23 13:48:46,720 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁Verse', '▁3', '1.', '▁The', '▁LORD', ',', '▁"', 'I', "'", 't', '▁was', '▁the', '▁LORD', ',', '▁and', '▁the', '▁LORD', ',', '▁and', '▁the', '▁LORD', ',', '▁and', '▁the', '▁LORD', ',', '▁and', '▁the', '▁LORD', '.']
2021-11-23 13:48:46,720 - INFO - joeynmt.training - 	Source:     <2en> <4us> <sent> M S15a07 S1f010 S26507 M S11e20 S1a520 P S38800 M S15a56 S15a41 S2b700 S37c06 S36d01 S30d00 M S10040 P S38700 M S37806 S10050 S20356 S36d01 M S15a37 S1ce51 S26a07 P S38700 M S19220 S2a20c S11520 S10120 S1f720 S20320 S36d01 M S10018 S26505 S10641 M S1f000 S1f720 S1dc20 S19220 S14a20 S1dc20 S14a20 P S38800 M S10041 S30d00 S36d01 M S15a31 S10018 S26503 M S2e74c S14220 S2e700 S14228 M S26521 S1f548 S1f540 M S10041 P S38700 M S10021 S10029 S22a07 S22a11 S36d01 M S2ea3e S10000 M S15a18 S10051 S26601 M S10058 S26526 S10050 S20500 S20500 P S38900 M S15a37 S15a01 S2e900 S36d01 M S15a20 S26501 M S26521 S23b0a S1f548 S15a40 S15a48 S23b1a S1f540 M S14c57 S14c5f S26511 S26507 S22527 S20314 S22521 S2031c P S38800
2021-11-23 13:48:46,720 - INFO - joeynmt.training - 	Factor:     <2en> <4us> <sent> M S15a S1f0 S265 M S11e S1a5 P S388 M S15a S15a S2b7 S37c S36d S30d M S100 P S387 M S378 S100 S203 S36d M S15a S1ce S26a P S387 M S192 S2a2 S115 S101 S1f7 S203 S36d M S100 S265 S106 M S1f0 S1f7 S1dc S192 S14a S1dc S14a P S388 M S100 S30d S36d M S15a S100 S265 M S2e7 S142 S2e7 S142 M S265 S1f5 S1f5 M S100 P S387 M S100 S100 S22a S22a S36d M S2ea S100 M S15a S100 S266 M S100 S265 S100 S205 S205 P S389 M S15a S15a S2e9 S36d M S15a S265 M S265 S23b S1f5 S15a S15a S23b S1f5 M S14c S14c S265 S265 S225 S203 S225 S203 P S388
2021-11-23 13:48:46,721 - INFO - joeynmt.training - 	Reference:  ▁Verse ▁3 7. ▁After ▁him , ▁at ▁the ▁time ▁of ▁the ▁c ens us , ▁there ▁was ▁Jud as ▁of ▁Gal ile e . ▁He ▁g ot ▁people ▁to ▁follow ▁him , ▁but ▁he ▁was ▁k illed , ▁too , ▁and ▁all ▁his ▁follow ers ▁were ▁sc atter ed .
2021-11-23 13:48:46,721 - INFO - joeynmt.training - 	Hypothesis: ▁Verse ▁3 1. ▁The ▁LORD , ▁" I ' t ▁was ▁the ▁LORD , ▁and ▁the ▁LORD , ▁and ▁the ▁LORD , ▁and ▁the ▁LORD , ▁and ▁the ▁LORD .
2021-11-23 13:48:46,721 - INFO - joeynmt.training - Example #1
2021-11-23 13:48:46,721 - DEBUG - joeynmt.training - 	Raw source:     ['<2pt>', '<4br>', '<dict>', 'M', 'S34700', 'S21100', 'S1f410']
2021-11-23 13:48:46,721 - DEBUG - joeynmt.training - 	Raw factor:     ['<2pt>', '<4br>', '<dict>', 'M', 'S347', 'S211', 'S1f4']
2021-11-23 13:48:46,721 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁P', 'A']
2021-11-23 13:48:46,721 - INFO - joeynmt.training - 	Source:     <2pt> <4br> <dict> M S34700 S21100 S1f410
2021-11-23 13:48:46,721 - INFO - joeynmt.training - 	Factor:     <2pt> <4br> <dict> M S347 S211 S1f4
2021-11-23 13:48:46,721 - INFO - joeynmt.training - 	Reference:  ▁L á p is
2021-11-23 13:48:46,721 - INFO - joeynmt.training - 	Hypothesis: ▁P A
2021-11-23 13:48:46,721 - INFO - joeynmt.training - Example #2
2021-11-23 13:48:46,721 - DEBUG - joeynmt.training - 	Raw source:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a07', 'S1f010', 'S26507', 'M', 'S10e00', 'P', 'S38800', 'M', 'S1dc0a', 'S10003', 'S20500', 'S20500', 'S30a00', 'M', 'S20302', 'S2030a', 'S20340', 'S20348', 'S30e00', 'S30124', 'M', 'S20500', 'S10013', 'M', 'S30124', 'S10010', 'S26500', 'M', 'S22f10', 'S20500', 'S15a0a', 'S22f00', 'S15a02', 'S20500', 'P', 'S38900', 'M', 'S17911', 'S17919', 'S2c300', 'S2c311', 'S30a00', 'P', 'S38700', 'M', 'S15a06', 'S15a41', 'S23d04', 'M', 'S2ff00', 'S10050', 'S10058', 'S10011', 'S22a04', 'M', 'S1f522', 'S2ea46', 'S2ea02', 'S1f50a', 'P', 'S38700', 'R', 'S3770b', 'S20305', 'S20500', 'S20303', 'S20500', 'S37713', 'S30e00', 'S30124', 'R', 'S1f522', 'S2ea46', 'S2ea02', 'S1f50a', 'P', 'S38700', 'M', 'S20356', 'S20350', 'S37706', 'S22f04', 'S30e00', 'S30124', 'M', 'S1f740', 'S1f748', 'S26a20', 'M', 'S1814b', 'S20500', 'S20500', 'S18143', 'R', 'S10000', 'S26500', 'R', 'S10011', 'S2ff00', 'P', 'S38700', 'M', 'S10000', 'S26500', 'M', 'S15a18', 'S15a18', 'S10e22', 'S10e02', 'P', 'S38800']
2021-11-23 13:48:46,721 - DEBUG - joeynmt.training - 	Raw factor:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a', 'S1f0', 'S265', 'M', 'S10e', 'P', 'S388', 'M', 'S1dc', 'S100', 'S205', 'S205', 'S30a', 'M', 'S203', 'S203', 'S203', 'S203', 'S30e', 'S301', 'M', 'S205', 'S100', 'M', 'S301', 'S100', 'S265', 'M', 'S22f', 'S205', 'S15a', 'S22f', 'S15a', 'S205', 'P', 'S389', 'M', 'S179', 'S179', 'S2c3', 'S2c3', 'S30a', 'P', 'S387', 'M', 'S15a', 'S15a', 'S23d', 'M', 'S2ff', 'S100', 'S100', 'S100', 'S22a', 'M', 'S1f5', 'S2ea', 'S2ea', 'S1f5', 'P', 'S387', 'R', 'S377', 'S203', 'S205', 'S203', 'S205', 'S377', 'S30e', 'S301', 'R', 'S1f5', 'S2ea', 'S2ea', 'S1f5', 'P', 'S387', 'M', 'S203', 'S203', 'S377', 'S22f', 'S30e', 'S301', 'M', 'S1f7', 'S1f7', 'S26a', 'M', 'S181', 'S205', 'S205', 'S181', 'R', 'S100', 'S265', 'R', 'S100', 'S2ff', 'P', 'S387', 'M', 'S100', 'S265', 'M', 'S15a', 'S15a', 'S10e', 'S10e', 'P', 'S388']
2021-11-23 13:48:46,721 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁Verse', '▁3', '1.', '▁"', 'I', "'", 't', '▁have', '▁be', '▁the', '▁Lord', ',', '▁and', '▁you', '▁will', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '.']
2021-11-23 13:48:46,721 - INFO - joeynmt.training - 	Source:     <2en> <4us> <sent> M S15a07 S1f010 S26507 M S10e00 P S38800 M S1dc0a S10003 S20500 S20500 S30a00 M S20302 S2030a S20340 S20348 S30e00 S30124 M S20500 S10013 M S30124 S10010 S26500 M S22f10 S20500 S15a0a S22f00 S15a02 S20500 P S38900 M S17911 S17919 S2c300 S2c311 S30a00 P S38700 M S15a06 S15a41 S23d04 M S2ff00 S10050 S10058 S10011 S22a04 M S1f522 S2ea46 S2ea02 S1f50a P S38700 R S3770b S20305 S20500 S20303 S20500 S37713 S30e00 S30124 R S1f522 S2ea46 S2ea02 S1f50a P S38700 M S20356 S20350 S37706 S22f04 S30e00 S30124 M S1f740 S1f748 S26a20 M S1814b S20500 S20500 S18143 R S10000 S26500 R S10011 S2ff00 P S38700 M S10000 S26500 M S15a18 S15a18 S10e22 S10e02 P S38800
2021-11-23 13:48:46,721 - INFO - joeynmt.training - 	Factor:     <2en> <4us> <sent> M S15a S1f0 S265 M S10e P S388 M S1dc S100 S205 S205 S30a M S203 S203 S203 S203 S30e S301 M S205 S100 M S301 S100 S265 M S22f S205 S15a S22f S15a S205 P S389 M S179 S179 S2c3 S2c3 S30a P S387 M S15a S15a S23d M S2ff S100 S100 S100 S22a M S1f5 S2ea S2ea S1f5 P S387 R S377 S203 S205 S203 S205 S377 S30e S301 R S1f5 S2ea S2ea S1f5 P S387 M S203 S203 S377 S22f S30e S301 M S1f7 S1f7 S26a M S181 S205 S205 S181 R S100 S265 R S100 S2ff P S387 M S100 S265 M S15a S15a S10e S10e P S388
2021-11-23 13:48:46,721 - INFO - joeynmt.training - 	Reference:  ▁Verse ▁2. ▁Then ▁make ▁me ▁tr ul y ▁hap p y ▁by ▁ag ree ing ▁whole he art ed ly ▁with ▁each ▁other , ▁lo v ing ▁one ▁another , ▁and ▁work ing ▁together ▁with ▁one ▁m ind ▁and ▁p ur p ose .
2021-11-23 13:48:46,721 - INFO - joeynmt.training - 	Hypothesis: ▁Verse ▁3 1. ▁" I ' t ▁have ▁be ▁the ▁Lord , ▁and ▁you ▁will ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be .
2021-11-23 13:48:46,721 - INFO - joeynmt.training - Example #3
2021-11-23 13:48:46,721 - DEBUG - joeynmt.training - 	Raw source:     ['<2pt>', '<4br>', '<dict>', 'M', 'S10e40', 'S11040', 'S26504', 'S2fb04']
2021-11-23 13:48:46,721 - DEBUG - joeynmt.training - 	Raw factor:     ['<2pt>', '<4br>', '<dict>', 'M', 'S10e', 'S110', 'S265', 'S2fb']
2021-11-23 13:48:46,721 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁P', 'A']
2021-11-23 13:48:46,721 - INFO - joeynmt.training - 	Source:     <2pt> <4br> <dict> M S10e40 S11040 S26504 S2fb04
2021-11-23 13:48:46,721 - INFO - joeynmt.training - 	Factor:     <2pt> <4br> <dict> M S10e S110 S265 S2fb
2021-11-23 13:48:46,721 - INFO - joeynmt.training - 	Reference:  ▁expl or ar
2021-11-23 13:48:46,721 - INFO - joeynmt.training - 	Hypothesis: ▁P A
2021-11-23 13:48:46,721 - INFO - joeynmt.training - Example #6
2021-11-23 13:48:46,721 - DEBUG - joeynmt.training - 	Raw source:     ['<2en>', '<4us>', '<dict>', 'M', 'S20340', 'S21800']
2021-11-23 13:48:46,721 - DEBUG - joeynmt.training - 	Raw factor:     ['<2en>', '<4us>', '<dict>', 'M', 'S203', 'S218']
2021-11-23 13:48:46,721 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁s', 'ol']
2021-11-23 13:48:46,721 - INFO - joeynmt.training - 	Source:     <2en> <4us> <dict> M S20340 S21800
2021-11-23 13:48:46,721 - INFO - joeynmt.training - 	Factor:     <2en> <4us> <dict> M S203 S218
2021-11-23 13:48:46,721 - INFO - joeynmt.training - 	Reference:  ▁m il k
2021-11-23 13:48:46,721 - INFO - joeynmt.training - 	Hypothesis: ▁s ol
2021-11-23 13:48:46,722 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step     5000: bleu:   0.71, loss: 125875.0312, ppl:  41.1303, duration: 136.7512s
2021-11-23 13:48:59,369 - INFO - joeynmt.training - Epoch   2, Step:     5100, Batch Loss:     3.753112, Tokens per Sec:     2198, Lr: 0.000100
2021-11-23 13:49:11,955 - INFO - joeynmt.training - Epoch   2, Step:     5200, Batch Loss:     3.575639, Tokens per Sec:     2287, Lr: 0.000100
2021-11-23 13:49:24,539 - INFO - joeynmt.training - Epoch   2, Step:     5300, Batch Loss:     3.760797, Tokens per Sec:     2241, Lr: 0.000100
2021-11-23 13:49:37,006 - INFO - joeynmt.training - Epoch   2, Step:     5400, Batch Loss:     3.656118, Tokens per Sec:     2292, Lr: 0.000100
2021-11-23 13:49:49,902 - INFO - joeynmt.training - Epoch   2, Step:     5500, Batch Loss:     3.752426, Tokens per Sec:     2229, Lr: 0.000100
2021-11-23 13:50:02,502 - INFO - joeynmt.training - Epoch   2, Step:     5600, Batch Loss:     3.896475, Tokens per Sec:     2269, Lr: 0.000100
2021-11-23 13:50:14,876 - INFO - joeynmt.training - Epoch   2, Step:     5700, Batch Loss:     3.730161, Tokens per Sec:     2175, Lr: 0.000100
2021-11-23 13:50:27,452 - INFO - joeynmt.training - Epoch   2, Step:     5800, Batch Loss:     3.598383, Tokens per Sec:     2218, Lr: 0.000100
2021-11-23 13:50:40,358 - INFO - joeynmt.training - Epoch   2, Step:     5900, Batch Loss:     3.691757, Tokens per Sec:     2321, Lr: 0.000100
2021-11-23 13:50:52,885 - INFO - joeynmt.training - Epoch   2, Step:     6000, Batch Loss:     3.623128, Tokens per Sec:     2225, Lr: 0.000100
2021-11-23 13:55:28,452 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-11-23 13:55:28,452 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-11-23 13:55:28,452 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-11-23 13:55:28,467 - INFO - joeynmt.training - Example #0
2021-11-23 13:55:28,468 - DEBUG - joeynmt.training - 	Raw source:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a07', 'S1f010', 'S26507', 'M', 'S11e20', 'S1a520', 'P', 'S38800', 'M', 'S15a56', 'S15a41', 'S2b700', 'S37c06', 'S36d01', 'S30d00', 'M', 'S10040', 'P', 'S38700', 'M', 'S37806', 'S10050', 'S20356', 'S36d01', 'M', 'S15a37', 'S1ce51', 'S26a07', 'P', 'S38700', 'M', 'S19220', 'S2a20c', 'S11520', 'S10120', 'S1f720', 'S20320', 'S36d01', 'M', 'S10018', 'S26505', 'S10641', 'M', 'S1f000', 'S1f720', 'S1dc20', 'S19220', 'S14a20', 'S1dc20', 'S14a20', 'P', 'S38800', 'M', 'S10041', 'S30d00', 'S36d01', 'M', 'S15a31', 'S10018', 'S26503', 'M', 'S2e74c', 'S14220', 'S2e700', 'S14228', 'M', 'S26521', 'S1f548', 'S1f540', 'M', 'S10041', 'P', 'S38700', 'M', 'S10021', 'S10029', 'S22a07', 'S22a11', 'S36d01', 'M', 'S2ea3e', 'S10000', 'M', 'S15a18', 'S10051', 'S26601', 'M', 'S10058', 'S26526', 'S10050', 'S20500', 'S20500', 'P', 'S38900', 'M', 'S15a37', 'S15a01', 'S2e900', 'S36d01', 'M', 'S15a20', 'S26501', 'M', 'S26521', 'S23b0a', 'S1f548', 'S15a40', 'S15a48', 'S23b1a', 'S1f540', 'M', 'S14c57', 'S14c5f', 'S26511', 'S26507', 'S22527', 'S20314', 'S22521', 'S2031c', 'P', 'S38800']
2021-11-23 13:55:28,468 - DEBUG - joeynmt.training - 	Raw factor:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a', 'S1f0', 'S265', 'M', 'S11e', 'S1a5', 'P', 'S388', 'M', 'S15a', 'S15a', 'S2b7', 'S37c', 'S36d', 'S30d', 'M', 'S100', 'P', 'S387', 'M', 'S378', 'S100', 'S203', 'S36d', 'M', 'S15a', 'S1ce', 'S26a', 'P', 'S387', 'M', 'S192', 'S2a2', 'S115', 'S101', 'S1f7', 'S203', 'S36d', 'M', 'S100', 'S265', 'S106', 'M', 'S1f0', 'S1f7', 'S1dc', 'S192', 'S14a', 'S1dc', 'S14a', 'P', 'S388', 'M', 'S100', 'S30d', 'S36d', 'M', 'S15a', 'S100', 'S265', 'M', 'S2e7', 'S142', 'S2e7', 'S142', 'M', 'S265', 'S1f5', 'S1f5', 'M', 'S100', 'P', 'S387', 'M', 'S100', 'S100', 'S22a', 'S22a', 'S36d', 'M', 'S2ea', 'S100', 'M', 'S15a', 'S100', 'S266', 'M', 'S100', 'S265', 'S100', 'S205', 'S205', 'P', 'S389', 'M', 'S15a', 'S15a', 'S2e9', 'S36d', 'M', 'S15a', 'S265', 'M', 'S265', 'S23b', 'S1f5', 'S15a', 'S15a', 'S23b', 'S1f5', 'M', 'S14c', 'S14c', 'S265', 'S265', 'S225', 'S203', 'S225', 'S203', 'P', 'S388']
2021-11-23 13:55:28,468 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁Verse', '▁13.', '▁"', 'I', '▁am', '▁the', '▁LORD', ',', '▁"', 'I', '▁am', '▁the', '▁LORD', ',', '▁"', 'I', "'", 't', '▁was', '▁the', '▁LORD', ',', '▁and', '▁the', '▁LORD', ',', '▁and', '▁the', '▁LORD', '.']
2021-11-23 13:55:28,468 - INFO - joeynmt.training - 	Source:     <2en> <4us> <sent> M S15a07 S1f010 S26507 M S11e20 S1a520 P S38800 M S15a56 S15a41 S2b700 S37c06 S36d01 S30d00 M S10040 P S38700 M S37806 S10050 S20356 S36d01 M S15a37 S1ce51 S26a07 P S38700 M S19220 S2a20c S11520 S10120 S1f720 S20320 S36d01 M S10018 S26505 S10641 M S1f000 S1f720 S1dc20 S19220 S14a20 S1dc20 S14a20 P S38800 M S10041 S30d00 S36d01 M S15a31 S10018 S26503 M S2e74c S14220 S2e700 S14228 M S26521 S1f548 S1f540 M S10041 P S38700 M S10021 S10029 S22a07 S22a11 S36d01 M S2ea3e S10000 M S15a18 S10051 S26601 M S10058 S26526 S10050 S20500 S20500 P S38900 M S15a37 S15a01 S2e900 S36d01 M S15a20 S26501 M S26521 S23b0a S1f548 S15a40 S15a48 S23b1a S1f540 M S14c57 S14c5f S26511 S26507 S22527 S20314 S22521 S2031c P S38800
2021-11-23 13:55:28,468 - INFO - joeynmt.training - 	Factor:     <2en> <4us> <sent> M S15a S1f0 S265 M S11e S1a5 P S388 M S15a S15a S2b7 S37c S36d S30d M S100 P S387 M S378 S100 S203 S36d M S15a S1ce S26a P S387 M S192 S2a2 S115 S101 S1f7 S203 S36d M S100 S265 S106 M S1f0 S1f7 S1dc S192 S14a S1dc S14a P S388 M S100 S30d S36d M S15a S100 S265 M S2e7 S142 S2e7 S142 M S265 S1f5 S1f5 M S100 P S387 M S100 S100 S22a S22a S36d M S2ea S100 M S15a S100 S266 M S100 S265 S100 S205 S205 P S389 M S15a S15a S2e9 S36d M S15a S265 M S265 S23b S1f5 S15a S15a S23b S1f5 M S14c S14c S265 S265 S225 S203 S225 S203 P S388
2021-11-23 13:55:28,468 - INFO - joeynmt.training - 	Reference:  ▁Verse ▁3 7. ▁After ▁him , ▁at ▁the ▁time ▁of ▁the ▁c ens us , ▁there ▁was ▁Jud as ▁of ▁Gal ile e . ▁He ▁g ot ▁people ▁to ▁follow ▁him , ▁but ▁he ▁was ▁k illed , ▁too , ▁and ▁all ▁his ▁follow ers ▁were ▁sc atter ed .
2021-11-23 13:55:28,468 - INFO - joeynmt.training - 	Hypothesis: ▁Verse ▁13. ▁" I ▁am ▁the ▁LORD , ▁" I ▁am ▁the ▁LORD , ▁" I ' t ▁was ▁the ▁LORD , ▁and ▁the ▁LORD , ▁and ▁the ▁LORD .
2021-11-23 13:55:28,468 - INFO - joeynmt.training - Example #1
2021-11-23 13:55:28,468 - DEBUG - joeynmt.training - 	Raw source:     ['<2pt>', '<4br>', '<dict>', 'M', 'S34700', 'S21100', 'S1f410']
2021-11-23 13:55:28,468 - DEBUG - joeynmt.training - 	Raw factor:     ['<2pt>', '<4br>', '<dict>', 'M', 'S347', 'S211', 'S1f4']
2021-11-23 13:55:28,468 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁C', 'o']
2021-11-23 13:55:28,468 - INFO - joeynmt.training - 	Source:     <2pt> <4br> <dict> M S34700 S21100 S1f410
2021-11-23 13:55:28,468 - INFO - joeynmt.training - 	Factor:     <2pt> <4br> <dict> M S347 S211 S1f4
2021-11-23 13:55:28,468 - INFO - joeynmt.training - 	Reference:  ▁L á p is
2021-11-23 13:55:28,468 - INFO - joeynmt.training - 	Hypothesis: ▁C o
2021-11-23 13:55:28,468 - INFO - joeynmt.training - Example #2
2021-11-23 13:55:28,468 - DEBUG - joeynmt.training - 	Raw source:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a07', 'S1f010', 'S26507', 'M', 'S10e00', 'P', 'S38800', 'M', 'S1dc0a', 'S10003', 'S20500', 'S20500', 'S30a00', 'M', 'S20302', 'S2030a', 'S20340', 'S20348', 'S30e00', 'S30124', 'M', 'S20500', 'S10013', 'M', 'S30124', 'S10010', 'S26500', 'M', 'S22f10', 'S20500', 'S15a0a', 'S22f00', 'S15a02', 'S20500', 'P', 'S38900', 'M', 'S17911', 'S17919', 'S2c300', 'S2c311', 'S30a00', 'P', 'S38700', 'M', 'S15a06', 'S15a41', 'S23d04', 'M', 'S2ff00', 'S10050', 'S10058', 'S10011', 'S22a04', 'M', 'S1f522', 'S2ea46', 'S2ea02', 'S1f50a', 'P', 'S38700', 'R', 'S3770b', 'S20305', 'S20500', 'S20303', 'S20500', 'S37713', 'S30e00', 'S30124', 'R', 'S1f522', 'S2ea46', 'S2ea02', 'S1f50a', 'P', 'S38700', 'M', 'S20356', 'S20350', 'S37706', 'S22f04', 'S30e00', 'S30124', 'M', 'S1f740', 'S1f748', 'S26a20', 'M', 'S1814b', 'S20500', 'S20500', 'S18143', 'R', 'S10000', 'S26500', 'R', 'S10011', 'S2ff00', 'P', 'S38700', 'M', 'S10000', 'S26500', 'M', 'S15a18', 'S15a18', 'S10e22', 'S10e02', 'P', 'S38800']
2021-11-23 13:55:28,468 - DEBUG - joeynmt.training - 	Raw factor:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a', 'S1f0', 'S265', 'M', 'S10e', 'P', 'S388', 'M', 'S1dc', 'S100', 'S205', 'S205', 'S30a', 'M', 'S203', 'S203', 'S203', 'S203', 'S30e', 'S301', 'M', 'S205', 'S100', 'M', 'S301', 'S100', 'S265', 'M', 'S22f', 'S205', 'S15a', 'S22f', 'S15a', 'S205', 'P', 'S389', 'M', 'S179', 'S179', 'S2c3', 'S2c3', 'S30a', 'P', 'S387', 'M', 'S15a', 'S15a', 'S23d', 'M', 'S2ff', 'S100', 'S100', 'S100', 'S22a', 'M', 'S1f5', 'S2ea', 'S2ea', 'S1f5', 'P', 'S387', 'R', 'S377', 'S203', 'S205', 'S203', 'S205', 'S377', 'S30e', 'S301', 'R', 'S1f5', 'S2ea', 'S2ea', 'S1f5', 'P', 'S387', 'M', 'S203', 'S203', 'S377', 'S22f', 'S30e', 'S301', 'M', 'S1f7', 'S1f7', 'S26a', 'M', 'S181', 'S205', 'S205', 'S181', 'R', 'S100', 'S265', 'R', 'S100', 'S2ff', 'P', 'S387', 'M', 'S100', 'S265', 'M', 'S15a', 'S15a', 'S10e', 'S10e', 'P', 'S388']
2021-11-23 13:55:28,468 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁Verse', '▁13.', '▁"', 'I', "'", 't', '▁have', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '.']
2021-11-23 13:55:28,468 - INFO - joeynmt.training - 	Source:     <2en> <4us> <sent> M S15a07 S1f010 S26507 M S10e00 P S38800 M S1dc0a S10003 S20500 S20500 S30a00 M S20302 S2030a S20340 S20348 S30e00 S30124 M S20500 S10013 M S30124 S10010 S26500 M S22f10 S20500 S15a0a S22f00 S15a02 S20500 P S38900 M S17911 S17919 S2c300 S2c311 S30a00 P S38700 M S15a06 S15a41 S23d04 M S2ff00 S10050 S10058 S10011 S22a04 M S1f522 S2ea46 S2ea02 S1f50a P S38700 R S3770b S20305 S20500 S20303 S20500 S37713 S30e00 S30124 R S1f522 S2ea46 S2ea02 S1f50a P S38700 M S20356 S20350 S37706 S22f04 S30e00 S30124 M S1f740 S1f748 S26a20 M S1814b S20500 S20500 S18143 R S10000 S26500 R S10011 S2ff00 P S38700 M S10000 S26500 M S15a18 S15a18 S10e22 S10e02 P S38800
2021-11-23 13:55:28,468 - INFO - joeynmt.training - 	Factor:     <2en> <4us> <sent> M S15a S1f0 S265 M S10e P S388 M S1dc S100 S205 S205 S30a M S203 S203 S203 S203 S30e S301 M S205 S100 M S301 S100 S265 M S22f S205 S15a S22f S15a S205 P S389 M S179 S179 S2c3 S2c3 S30a P S387 M S15a S15a S23d M S2ff S100 S100 S100 S22a M S1f5 S2ea S2ea S1f5 P S387 R S377 S203 S205 S203 S205 S377 S30e S301 R S1f5 S2ea S2ea S1f5 P S387 M S203 S203 S377 S22f S30e S301 M S1f7 S1f7 S26a M S181 S205 S205 S181 R S100 S265 R S100 S2ff P S387 M S100 S265 M S15a S15a S10e S10e P S388
2021-11-23 13:55:28,469 - INFO - joeynmt.training - 	Reference:  ▁Verse ▁2. ▁Then ▁make ▁me ▁tr ul y ▁hap p y ▁by ▁ag ree ing ▁whole he art ed ly ▁with ▁each ▁other , ▁lo v ing ▁one ▁another , ▁and ▁work ing ▁together ▁with ▁one ▁m ind ▁and ▁p ur p ose .
2021-11-23 13:55:28,469 - INFO - joeynmt.training - 	Hypothesis: ▁Verse ▁13. ▁" I ' t ▁have ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be .
2021-11-23 13:55:28,469 - INFO - joeynmt.training - Example #3
2021-11-23 13:55:28,469 - DEBUG - joeynmt.training - 	Raw source:     ['<2pt>', '<4br>', '<dict>', 'M', 'S10e40', 'S11040', 'S26504', 'S2fb04']
2021-11-23 13:55:28,469 - DEBUG - joeynmt.training - 	Raw factor:     ['<2pt>', '<4br>', '<dict>', 'M', 'S10e', 'S110', 'S265', 'S2fb']
2021-11-23 13:55:28,469 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁C', 'o']
2021-11-23 13:55:28,469 - INFO - joeynmt.training - 	Source:     <2pt> <4br> <dict> M S10e40 S11040 S26504 S2fb04
2021-11-23 13:55:28,469 - INFO - joeynmt.training - 	Factor:     <2pt> <4br> <dict> M S10e S110 S265 S2fb
2021-11-23 13:55:28,469 - INFO - joeynmt.training - 	Reference:  ▁expl or ar
2021-11-23 13:55:28,469 - INFO - joeynmt.training - 	Hypothesis: ▁C o
2021-11-23 13:55:28,469 - INFO - joeynmt.training - Example #6
2021-11-23 13:55:28,469 - DEBUG - joeynmt.training - 	Raw source:     ['<2en>', '<4us>', '<dict>', 'M', 'S20340', 'S21800']
2021-11-23 13:55:28,469 - DEBUG - joeynmt.training - 	Raw factor:     ['<2en>', '<4us>', '<dict>', 'M', 'S203', 'S218']
2021-11-23 13:55:28,469 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁b', 'in']
2021-11-23 13:55:28,469 - INFO - joeynmt.training - 	Source:     <2en> <4us> <dict> M S20340 S21800
2021-11-23 13:55:28,469 - INFO - joeynmt.training - 	Factor:     <2en> <4us> <dict> M S203 S218
2021-11-23 13:55:28,469 - INFO - joeynmt.training - 	Reference:  ▁m il k
2021-11-23 13:55:28,469 - INFO - joeynmt.training - 	Hypothesis: ▁b in
2021-11-23 13:55:28,469 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step     6000: bleu:   0.46, loss: 123907.4141, ppl:  38.8088, duration: 275.5840s
2021-11-23 13:55:41,090 - INFO - joeynmt.training - Epoch   2, Step:     6100, Batch Loss:     3.293128, Tokens per Sec:     2264, Lr: 0.000100
2021-11-23 13:55:53,667 - INFO - joeynmt.training - Epoch   2, Step:     6200, Batch Loss:     3.494355, Tokens per Sec:     2276, Lr: 0.000100
2021-11-23 13:56:06,449 - INFO - joeynmt.training - Epoch   2, Step:     6300, Batch Loss:     3.499315, Tokens per Sec:     2248, Lr: 0.000100
2021-11-23 13:56:19,066 - INFO - joeynmt.training - Epoch   2, Step:     6400, Batch Loss:     3.660851, Tokens per Sec:     2270, Lr: 0.000100
2021-11-23 13:56:31,643 - INFO - joeynmt.training - Epoch   2, Step:     6500, Batch Loss:     3.621941, Tokens per Sec:     2180, Lr: 0.000100
2021-11-23 13:56:44,301 - INFO - joeynmt.training - Epoch   2, Step:     6600, Batch Loss:     3.678778, Tokens per Sec:     2374, Lr: 0.000100
2021-11-23 13:56:56,699 - INFO - joeynmt.training - Epoch   2, Step:     6700, Batch Loss:     3.684643, Tokens per Sec:     2292, Lr: 0.000100
2021-11-23 13:57:09,488 - INFO - joeynmt.training - Epoch   2, Step:     6800, Batch Loss:     3.609520, Tokens per Sec:     2306, Lr: 0.000100
2021-11-23 13:57:21,894 - INFO - joeynmt.training - Epoch   2, Step:     6900, Batch Loss:     3.651479, Tokens per Sec:     2298, Lr: 0.000100
2021-11-23 13:57:34,670 - INFO - joeynmt.training - Epoch   2, Step:     7000, Batch Loss:     3.681638, Tokens per Sec:     2337, Lr: 0.000100
2021-11-23 13:59:13,359 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-11-23 13:59:13,359 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-11-23 13:59:13,359 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-11-23 13:59:13,369 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-11-23 13:59:14,909 - INFO - joeynmt.helpers - delete models/baseline_multilingual_mem/5000.ckpt
2021-11-23 13:59:14,910 - INFO - joeynmt.helpers - delete /net/cephfs/home/zifjia/signwriting-translation/models/baseline_multilingual_mem/5000.ckpt
2021-11-23 13:59:14,910 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /net/cephfs/home/zifjia/signwriting-translation/models/baseline_multilingual_mem/5000.ckpt but file does not exist. ([Errno 2] No such file or directory: '/net/cephfs/home/zifjia/signwriting-translation/models/baseline_multilingual_mem/5000.ckpt')
2021-11-23 13:59:14,916 - INFO - joeynmt.training - Example #0
2021-11-23 13:59:14,916 - DEBUG - joeynmt.training - 	Raw source:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a07', 'S1f010', 'S26507', 'M', 'S11e20', 'S1a520', 'P', 'S38800', 'M', 'S15a56', 'S15a41', 'S2b700', 'S37c06', 'S36d01', 'S30d00', 'M', 'S10040', 'P', 'S38700', 'M', 'S37806', 'S10050', 'S20356', 'S36d01', 'M', 'S15a37', 'S1ce51', 'S26a07', 'P', 'S38700', 'M', 'S19220', 'S2a20c', 'S11520', 'S10120', 'S1f720', 'S20320', 'S36d01', 'M', 'S10018', 'S26505', 'S10641', 'M', 'S1f000', 'S1f720', 'S1dc20', 'S19220', 'S14a20', 'S1dc20', 'S14a20', 'P', 'S38800', 'M', 'S10041', 'S30d00', 'S36d01', 'M', 'S15a31', 'S10018', 'S26503', 'M', 'S2e74c', 'S14220', 'S2e700', 'S14228', 'M', 'S26521', 'S1f548', 'S1f540', 'M', 'S10041', 'P', 'S38700', 'M', 'S10021', 'S10029', 'S22a07', 'S22a11', 'S36d01', 'M', 'S2ea3e', 'S10000', 'M', 'S15a18', 'S10051', 'S26601', 'M', 'S10058', 'S26526', 'S10050', 'S20500', 'S20500', 'P', 'S38900', 'M', 'S15a37', 'S15a01', 'S2e900', 'S36d01', 'M', 'S15a20', 'S26501', 'M', 'S26521', 'S23b0a', 'S1f548', 'S15a40', 'S15a48', 'S23b1a', 'S1f540', 'M', 'S14c57', 'S14c5f', 'S26511', 'S26507', 'S22527', 'S20314', 'S22521', 'S2031c', 'P', 'S38800']
2021-11-23 13:59:14,916 - DEBUG - joeynmt.training - 	Raw factor:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a', 'S1f0', 'S265', 'M', 'S11e', 'S1a5', 'P', 'S388', 'M', 'S15a', 'S15a', 'S2b7', 'S37c', 'S36d', 'S30d', 'M', 'S100', 'P', 'S387', 'M', 'S378', 'S100', 'S203', 'S36d', 'M', 'S15a', 'S1ce', 'S26a', 'P', 'S387', 'M', 'S192', 'S2a2', 'S115', 'S101', 'S1f7', 'S203', 'S36d', 'M', 'S100', 'S265', 'S106', 'M', 'S1f0', 'S1f7', 'S1dc', 'S192', 'S14a', 'S1dc', 'S14a', 'P', 'S388', 'M', 'S100', 'S30d', 'S36d', 'M', 'S15a', 'S100', 'S265', 'M', 'S2e7', 'S142', 'S2e7', 'S142', 'M', 'S265', 'S1f5', 'S1f5', 'M', 'S100', 'P', 'S387', 'M', 'S100', 'S100', 'S22a', 'S22a', 'S36d', 'M', 'S2ea', 'S100', 'M', 'S15a', 'S100', 'S266', 'M', 'S100', 'S265', 'S100', 'S205', 'S205', 'P', 'S389', 'M', 'S15a', 'S15a', 'S2e9', 'S36d', 'M', 'S15a', 'S265', 'M', 'S265', 'S23b', 'S1f5', 'S15a', 'S15a', 'S23b', 'S1f5', 'M', 'S14c', 'S14c', 'S265', 'S265', 'S225', 'S203', 'S225', 'S203', 'P', 'S388']
2021-11-23 13:59:14,916 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁Verse', '▁3', '1.', '▁"', 'I', '▁am', '▁the', '▁LORD', ',', '▁and', '▁the', '▁LORD', ',', '▁and', '▁the', '▁LORD', ',', '▁and', '▁the', '▁LORD', ',', '▁and', '▁the', '▁LORD', ',', '▁and', '▁the', '▁LORD', '.']
2021-11-23 13:59:14,916 - INFO - joeynmt.training - 	Source:     <2en> <4us> <sent> M S15a07 S1f010 S26507 M S11e20 S1a520 P S38800 M S15a56 S15a41 S2b700 S37c06 S36d01 S30d00 M S10040 P S38700 M S37806 S10050 S20356 S36d01 M S15a37 S1ce51 S26a07 P S38700 M S19220 S2a20c S11520 S10120 S1f720 S20320 S36d01 M S10018 S26505 S10641 M S1f000 S1f720 S1dc20 S19220 S14a20 S1dc20 S14a20 P S38800 M S10041 S30d00 S36d01 M S15a31 S10018 S26503 M S2e74c S14220 S2e700 S14228 M S26521 S1f548 S1f540 M S10041 P S38700 M S10021 S10029 S22a07 S22a11 S36d01 M S2ea3e S10000 M S15a18 S10051 S26601 M S10058 S26526 S10050 S20500 S20500 P S38900 M S15a37 S15a01 S2e900 S36d01 M S15a20 S26501 M S26521 S23b0a S1f548 S15a40 S15a48 S23b1a S1f540 M S14c57 S14c5f S26511 S26507 S22527 S20314 S22521 S2031c P S38800
2021-11-23 13:59:14,916 - INFO - joeynmt.training - 	Factor:     <2en> <4us> <sent> M S15a S1f0 S265 M S11e S1a5 P S388 M S15a S15a S2b7 S37c S36d S30d M S100 P S387 M S378 S100 S203 S36d M S15a S1ce S26a P S387 M S192 S2a2 S115 S101 S1f7 S203 S36d M S100 S265 S106 M S1f0 S1f7 S1dc S192 S14a S1dc S14a P S388 M S100 S30d S36d M S15a S100 S265 M S2e7 S142 S2e7 S142 M S265 S1f5 S1f5 M S100 P S387 M S100 S100 S22a S22a S36d M S2ea S100 M S15a S100 S266 M S100 S265 S100 S205 S205 P S389 M S15a S15a S2e9 S36d M S15a S265 M S265 S23b S1f5 S15a S15a S23b S1f5 M S14c S14c S265 S265 S225 S203 S225 S203 P S388
2021-11-23 13:59:14,916 - INFO - joeynmt.training - 	Reference:  ▁Verse ▁3 7. ▁After ▁him , ▁at ▁the ▁time ▁of ▁the ▁c ens us , ▁there ▁was ▁Jud as ▁of ▁Gal ile e . ▁He ▁g ot ▁people ▁to ▁follow ▁him , ▁but ▁he ▁was ▁k illed , ▁too , ▁and ▁all ▁his ▁follow ers ▁were ▁sc atter ed .
2021-11-23 13:59:14,916 - INFO - joeynmt.training - 	Hypothesis: ▁Verse ▁3 1. ▁" I ▁am ▁the ▁LORD , ▁and ▁the ▁LORD , ▁and ▁the ▁LORD , ▁and ▁the ▁LORD , ▁and ▁the ▁LORD , ▁and ▁the ▁LORD .
2021-11-23 13:59:14,916 - INFO - joeynmt.training - Example #1
2021-11-23 13:59:14,916 - DEBUG - joeynmt.training - 	Raw source:     ['<2pt>', '<4br>', '<dict>', 'M', 'S34700', 'S21100', 'S1f410']
2021-11-23 13:59:14,916 - DEBUG - joeynmt.training - 	Raw factor:     ['<2pt>', '<4br>', '<dict>', 'M', 'S347', 'S211', 'S1f4']
2021-11-23 13:59:14,916 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁C', 'O']
2021-11-23 13:59:14,916 - INFO - joeynmt.training - 	Source:     <2pt> <4br> <dict> M S34700 S21100 S1f410
2021-11-23 13:59:14,916 - INFO - joeynmt.training - 	Factor:     <2pt> <4br> <dict> M S347 S211 S1f4
2021-11-23 13:59:14,916 - INFO - joeynmt.training - 	Reference:  ▁L á p is
2021-11-23 13:59:14,916 - INFO - joeynmt.training - 	Hypothesis: ▁C O
2021-11-23 13:59:14,916 - INFO - joeynmt.training - Example #2
2021-11-23 13:59:14,916 - DEBUG - joeynmt.training - 	Raw source:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a07', 'S1f010', 'S26507', 'M', 'S10e00', 'P', 'S38800', 'M', 'S1dc0a', 'S10003', 'S20500', 'S20500', 'S30a00', 'M', 'S20302', 'S2030a', 'S20340', 'S20348', 'S30e00', 'S30124', 'M', 'S20500', 'S10013', 'M', 'S30124', 'S10010', 'S26500', 'M', 'S22f10', 'S20500', 'S15a0a', 'S22f00', 'S15a02', 'S20500', 'P', 'S38900', 'M', 'S17911', 'S17919', 'S2c300', 'S2c311', 'S30a00', 'P', 'S38700', 'M', 'S15a06', 'S15a41', 'S23d04', 'M', 'S2ff00', 'S10050', 'S10058', 'S10011', 'S22a04', 'M', 'S1f522', 'S2ea46', 'S2ea02', 'S1f50a', 'P', 'S38700', 'R', 'S3770b', 'S20305', 'S20500', 'S20303', 'S20500', 'S37713', 'S30e00', 'S30124', 'R', 'S1f522', 'S2ea46', 'S2ea02', 'S1f50a', 'P', 'S38700', 'M', 'S20356', 'S20350', 'S37706', 'S22f04', 'S30e00', 'S30124', 'M', 'S1f740', 'S1f748', 'S26a20', 'M', 'S1814b', 'S20500', 'S20500', 'S18143', 'R', 'S10000', 'S26500', 'R', 'S10011', 'S2ff00', 'P', 'S38700', 'M', 'S10000', 'S26500', 'M', 'S15a18', 'S15a18', 'S10e22', 'S10e02', 'P', 'S38800']
2021-11-23 13:59:14,916 - DEBUG - joeynmt.training - 	Raw factor:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a', 'S1f0', 'S265', 'M', 'S10e', 'P', 'S388', 'M', 'S1dc', 'S100', 'S205', 'S205', 'S30a', 'M', 'S203', 'S203', 'S203', 'S203', 'S30e', 'S301', 'M', 'S205', 'S100', 'M', 'S301', 'S100', 'S265', 'M', 'S22f', 'S205', 'S15a', 'S22f', 'S15a', 'S205', 'P', 'S389', 'M', 'S179', 'S179', 'S2c3', 'S2c3', 'S30a', 'P', 'S387', 'M', 'S15a', 'S15a', 'S23d', 'M', 'S2ff', 'S100', 'S100', 'S100', 'S22a', 'M', 'S1f5', 'S2ea', 'S2ea', 'S1f5', 'P', 'S387', 'R', 'S377', 'S203', 'S205', 'S203', 'S205', 'S377', 'S30e', 'S301', 'R', 'S1f5', 'S2ea', 'S2ea', 'S1f5', 'P', 'S387', 'M', 'S203', 'S203', 'S377', 'S22f', 'S30e', 'S301', 'M', 'S1f7', 'S1f7', 'S26a', 'M', 'S181', 'S205', 'S205', 'S181', 'R', 'S100', 'S265', 'R', 'S100', 'S2ff', 'P', 'S387', 'M', 'S100', 'S265', 'M', 'S15a', 'S15a', 'S10e', 'S10e', 'P', 'S388']
2021-11-23 13:59:14,916 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁Verse', '▁14.', '▁But', '▁you', '▁will', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '.']
2021-11-23 13:59:14,917 - INFO - joeynmt.training - 	Source:     <2en> <4us> <sent> M S15a07 S1f010 S26507 M S10e00 P S38800 M S1dc0a S10003 S20500 S20500 S30a00 M S20302 S2030a S20340 S20348 S30e00 S30124 M S20500 S10013 M S30124 S10010 S26500 M S22f10 S20500 S15a0a S22f00 S15a02 S20500 P S38900 M S17911 S17919 S2c300 S2c311 S30a00 P S38700 M S15a06 S15a41 S23d04 M S2ff00 S10050 S10058 S10011 S22a04 M S1f522 S2ea46 S2ea02 S1f50a P S38700 R S3770b S20305 S20500 S20303 S20500 S37713 S30e00 S30124 R S1f522 S2ea46 S2ea02 S1f50a P S38700 M S20356 S20350 S37706 S22f04 S30e00 S30124 M S1f740 S1f748 S26a20 M S1814b S20500 S20500 S18143 R S10000 S26500 R S10011 S2ff00 P S38700 M S10000 S26500 M S15a18 S15a18 S10e22 S10e02 P S38800
2021-11-23 13:59:14,917 - INFO - joeynmt.training - 	Factor:     <2en> <4us> <sent> M S15a S1f0 S265 M S10e P S388 M S1dc S100 S205 S205 S30a M S203 S203 S203 S203 S30e S301 M S205 S100 M S301 S100 S265 M S22f S205 S15a S22f S15a S205 P S389 M S179 S179 S2c3 S2c3 S30a P S387 M S15a S15a S23d M S2ff S100 S100 S100 S22a M S1f5 S2ea S2ea S1f5 P S387 R S377 S203 S205 S203 S205 S377 S30e S301 R S1f5 S2ea S2ea S1f5 P S387 M S203 S203 S377 S22f S30e S301 M S1f7 S1f7 S26a M S181 S205 S205 S181 R S100 S265 R S100 S2ff P S387 M S100 S265 M S15a S15a S10e S10e P S388
2021-11-23 13:59:14,917 - INFO - joeynmt.training - 	Reference:  ▁Verse ▁2. ▁Then ▁make ▁me ▁tr ul y ▁hap p y ▁by ▁ag ree ing ▁whole he art ed ly ▁with ▁each ▁other , ▁lo v ing ▁one ▁another , ▁and ▁work ing ▁together ▁with ▁one ▁m ind ▁and ▁p ur p ose .
2021-11-23 13:59:14,917 - INFO - joeynmt.training - 	Hypothesis: ▁Verse ▁14. ▁But ▁you ▁will ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be .
2021-11-23 13:59:14,917 - INFO - joeynmt.training - Example #3
2021-11-23 13:59:14,917 - DEBUG - joeynmt.training - 	Raw source:     ['<2pt>', '<4br>', '<dict>', 'M', 'S10e40', 'S11040', 'S26504', 'S2fb04']
2021-11-23 13:59:14,917 - DEBUG - joeynmt.training - 	Raw factor:     ['<2pt>', '<4br>', '<dict>', 'M', 'S10e', 'S110', 'S265', 'S2fb']
2021-11-23 13:59:14,917 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁p', 'é']
2021-11-23 13:59:14,917 - INFO - joeynmt.training - 	Source:     <2pt> <4br> <dict> M S10e40 S11040 S26504 S2fb04
2021-11-23 13:59:14,917 - INFO - joeynmt.training - 	Factor:     <2pt> <4br> <dict> M S10e S110 S265 S2fb
2021-11-23 13:59:14,917 - INFO - joeynmt.training - 	Reference:  ▁expl or ar
2021-11-23 13:59:14,917 - INFO - joeynmt.training - 	Hypothesis: ▁p é
2021-11-23 13:59:14,917 - INFO - joeynmt.training - Example #6
2021-11-23 13:59:14,917 - DEBUG - joeynmt.training - 	Raw source:     ['<2en>', '<4us>', '<dict>', 'M', 'S20340', 'S21800']
2021-11-23 13:59:14,917 - DEBUG - joeynmt.training - 	Raw factor:     ['<2en>', '<4us>', '<dict>', 'M', 'S203', 'S218']
2021-11-23 13:59:14,917 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁b', 'at']
2021-11-23 13:59:14,917 - INFO - joeynmt.training - 	Source:     <2en> <4us> <dict> M S20340 S21800
2021-11-23 13:59:14,917 - INFO - joeynmt.training - 	Factor:     <2en> <4us> <dict> M S203 S218
2021-11-23 13:59:14,917 - INFO - joeynmt.training - 	Reference:  ▁m il k
2021-11-23 13:59:14,917 - INFO - joeynmt.training - 	Hypothesis: ▁b at
2021-11-23 13:59:14,917 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step     7000: bleu:   0.85, loss: 121939.2891, ppl:  36.6178, duration: 100.2464s
2021-11-23 13:59:27,588 - INFO - joeynmt.training - Epoch   2, Step:     7100, Batch Loss:     3.493600, Tokens per Sec:     2267, Lr: 0.000100
2021-11-23 13:59:40,531 - INFO - joeynmt.training - Epoch   2, Step:     7200, Batch Loss:     3.606278, Tokens per Sec:     2265, Lr: 0.000100
2021-11-23 13:59:42,471 - INFO - joeynmt.training - Epoch   2: total training loss 13411.04
2021-11-23 13:59:42,471 - INFO - joeynmt.training - EPOCH 3
2021-11-23 13:59:53,498 - INFO - joeynmt.training - Epoch   3, Step:     7300, Batch Loss:     3.676719, Tokens per Sec:     2233, Lr: 0.000100
2021-11-23 14:00:06,205 - INFO - joeynmt.training - Epoch   3, Step:     7400, Batch Loss:     3.531485, Tokens per Sec:     2263, Lr: 0.000100
2021-11-23 14:00:18,943 - INFO - joeynmt.training - Epoch   3, Step:     7500, Batch Loss:     3.699059, Tokens per Sec:     2246, Lr: 0.000100
2021-11-23 14:00:31,610 - INFO - joeynmt.training - Epoch   3, Step:     7600, Batch Loss:     3.698535, Tokens per Sec:     2303, Lr: 0.000100
2021-11-23 14:00:44,339 - INFO - joeynmt.training - Epoch   3, Step:     7700, Batch Loss:     3.641433, Tokens per Sec:     2287, Lr: 0.000100
2021-11-23 14:00:56,801 - INFO - joeynmt.training - Epoch   3, Step:     7800, Batch Loss:     3.793271, Tokens per Sec:     2212, Lr: 0.000100
2021-11-23 14:01:09,642 - INFO - joeynmt.training - Epoch   3, Step:     7900, Batch Loss:     3.818432, Tokens per Sec:     2240, Lr: 0.000100
2021-11-23 14:01:22,332 - INFO - joeynmt.training - Epoch   3, Step:     8000, Batch Loss:     3.525104, Tokens per Sec:     2325, Lr: 0.000100
2021-11-23 14:06:14,568 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-11-23 14:06:14,568 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-11-23 14:06:14,568 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-11-23 14:06:14,583 - INFO - joeynmt.training - Example #0
2021-11-23 14:06:14,583 - DEBUG - joeynmt.training - 	Raw source:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a07', 'S1f010', 'S26507', 'M', 'S11e20', 'S1a520', 'P', 'S38800', 'M', 'S15a56', 'S15a41', 'S2b700', 'S37c06', 'S36d01', 'S30d00', 'M', 'S10040', 'P', 'S38700', 'M', 'S37806', 'S10050', 'S20356', 'S36d01', 'M', 'S15a37', 'S1ce51', 'S26a07', 'P', 'S38700', 'M', 'S19220', 'S2a20c', 'S11520', 'S10120', 'S1f720', 'S20320', 'S36d01', 'M', 'S10018', 'S26505', 'S10641', 'M', 'S1f000', 'S1f720', 'S1dc20', 'S19220', 'S14a20', 'S1dc20', 'S14a20', 'P', 'S38800', 'M', 'S10041', 'S30d00', 'S36d01', 'M', 'S15a31', 'S10018', 'S26503', 'M', 'S2e74c', 'S14220', 'S2e700', 'S14228', 'M', 'S26521', 'S1f548', 'S1f540', 'M', 'S10041', 'P', 'S38700', 'M', 'S10021', 'S10029', 'S22a07', 'S22a11', 'S36d01', 'M', 'S2ea3e', 'S10000', 'M', 'S15a18', 'S10051', 'S26601', 'M', 'S10058', 'S26526', 'S10050', 'S20500', 'S20500', 'P', 'S38900', 'M', 'S15a37', 'S15a01', 'S2e900', 'S36d01', 'M', 'S15a20', 'S26501', 'M', 'S26521', 'S23b0a', 'S1f548', 'S15a40', 'S15a48', 'S23b1a', 'S1f540', 'M', 'S14c57', 'S14c5f', 'S26511', 'S26507', 'S22527', 'S20314', 'S22521', 'S2031c', 'P', 'S38800']
2021-11-23 14:06:14,583 - DEBUG - joeynmt.training - 	Raw factor:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a', 'S1f0', 'S265', 'M', 'S11e', 'S1a5', 'P', 'S388', 'M', 'S15a', 'S15a', 'S2b7', 'S37c', 'S36d', 'S30d', 'M', 'S100', 'P', 'S387', 'M', 'S378', 'S100', 'S203', 'S36d', 'M', 'S15a', 'S1ce', 'S26a', 'P', 'S387', 'M', 'S192', 'S2a2', 'S115', 'S101', 'S1f7', 'S203', 'S36d', 'M', 'S100', 'S265', 'S106', 'M', 'S1f0', 'S1f7', 'S1dc', 'S192', 'S14a', 'S1dc', 'S14a', 'P', 'S388', 'M', 'S100', 'S30d', 'S36d', 'M', 'S15a', 'S100', 'S265', 'M', 'S2e7', 'S142', 'S2e7', 'S142', 'M', 'S265', 'S1f5', 'S1f5', 'M', 'S100', 'P', 'S387', 'M', 'S100', 'S100', 'S22a', 'S22a', 'S36d', 'M', 'S2ea', 'S100', 'M', 'S15a', 'S100', 'S266', 'M', 'S100', 'S265', 'S100', 'S205', 'S205', 'P', 'S389', 'M', 'S15a', 'S15a', 'S2e9', 'S36d', 'M', 'S15a', 'S265', 'M', 'S265', 'S23b', 'S1f5', 'S15a', 'S15a', 'S23b', 'S1f5', 'M', 'S14c', 'S14c', 'S265', 'S265', 'S225', 'S203', 'S225', 'S203', 'P', 'S388']
2021-11-23 14:06:14,583 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁Verse', '▁3', '1.', '▁Then', '▁the', '▁LORD', ',', '▁"', 'W', 'W', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', ',', '▁and', '▁he', '▁was', '▁the', '▁LORD', ',', '▁and', '▁the', '▁LORD', ',', '▁and', '▁he', '▁was', '▁the', '▁LORD', ',', '▁and', '▁the', '▁LORD', '.']
2021-11-23 14:06:14,583 - INFO - joeynmt.training - 	Source:     <2en> <4us> <sent> M S15a07 S1f010 S26507 M S11e20 S1a520 P S38800 M S15a56 S15a41 S2b700 S37c06 S36d01 S30d00 M S10040 P S38700 M S37806 S10050 S20356 S36d01 M S15a37 S1ce51 S26a07 P S38700 M S19220 S2a20c S11520 S10120 S1f720 S20320 S36d01 M S10018 S26505 S10641 M S1f000 S1f720 S1dc20 S19220 S14a20 S1dc20 S14a20 P S38800 M S10041 S30d00 S36d01 M S15a31 S10018 S26503 M S2e74c S14220 S2e700 S14228 M S26521 S1f548 S1f540 M S10041 P S38700 M S10021 S10029 S22a07 S22a11 S36d01 M S2ea3e S10000 M S15a18 S10051 S26601 M S10058 S26526 S10050 S20500 S20500 P S38900 M S15a37 S15a01 S2e900 S36d01 M S15a20 S26501 M S26521 S23b0a S1f548 S15a40 S15a48 S23b1a S1f540 M S14c57 S14c5f S26511 S26507 S22527 S20314 S22521 S2031c P S38800
2021-11-23 14:06:14,583 - INFO - joeynmt.training - 	Factor:     <2en> <4us> <sent> M S15a S1f0 S265 M S11e S1a5 P S388 M S15a S15a S2b7 S37c S36d S30d M S100 P S387 M S378 S100 S203 S36d M S15a S1ce S26a P S387 M S192 S2a2 S115 S101 S1f7 S203 S36d M S100 S265 S106 M S1f0 S1f7 S1dc S192 S14a S1dc S14a P S388 M S100 S30d S36d M S15a S100 S265 M S2e7 S142 S2e7 S142 M S265 S1f5 S1f5 M S100 P S387 M S100 S100 S22a S22a S36d M S2ea S100 M S15a S100 S266 M S100 S265 S100 S205 S205 P S389 M S15a S15a S2e9 S36d M S15a S265 M S265 S23b S1f5 S15a S15a S23b S1f5 M S14c S14c S265 S265 S225 S203 S225 S203 P S388
2021-11-23 14:06:14,584 - INFO - joeynmt.training - 	Reference:  ▁Verse ▁3 7. ▁After ▁him , ▁at ▁the ▁time ▁of ▁the ▁c ens us , ▁there ▁was ▁Jud as ▁of ▁Gal ile e . ▁He ▁g ot ▁people ▁to ▁follow ▁him , ▁but ▁he ▁was ▁k illed , ▁too , ▁and ▁all ▁his ▁follow ers ▁were ▁sc atter ed .
2021-11-23 14:06:14,584 - INFO - joeynmt.training - 	Hypothesis: ▁Verse ▁3 1. ▁Then ▁the ▁LORD , ▁" W W e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e , ▁and ▁he ▁was ▁the ▁LORD , ▁and ▁the ▁LORD , ▁and ▁he ▁was ▁the ▁LORD , ▁and ▁the ▁LORD .
2021-11-23 14:06:14,584 - INFO - joeynmt.training - Example #1
2021-11-23 14:06:14,584 - DEBUG - joeynmt.training - 	Raw source:     ['<2pt>', '<4br>', '<dict>', 'M', 'S34700', 'S21100', 'S1f410']
2021-11-23 14:06:14,584 - DEBUG - joeynmt.training - 	Raw factor:     ['<2pt>', '<4br>', '<dict>', 'M', 'S347', 'S211', 'S1f4']
2021-11-23 14:06:14,584 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁C', 'A']
2021-11-23 14:06:14,584 - INFO - joeynmt.training - 	Source:     <2pt> <4br> <dict> M S34700 S21100 S1f410
2021-11-23 14:06:14,584 - INFO - joeynmt.training - 	Factor:     <2pt> <4br> <dict> M S347 S211 S1f4
2021-11-23 14:06:14,584 - INFO - joeynmt.training - 	Reference:  ▁L á p is
2021-11-23 14:06:14,584 - INFO - joeynmt.training - 	Hypothesis: ▁C A
2021-11-23 14:06:14,584 - INFO - joeynmt.training - Example #2
2021-11-23 14:06:14,584 - DEBUG - joeynmt.training - 	Raw source:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a07', 'S1f010', 'S26507', 'M', 'S10e00', 'P', 'S38800', 'M', 'S1dc0a', 'S10003', 'S20500', 'S20500', 'S30a00', 'M', 'S20302', 'S2030a', 'S20340', 'S20348', 'S30e00', 'S30124', 'M', 'S20500', 'S10013', 'M', 'S30124', 'S10010', 'S26500', 'M', 'S22f10', 'S20500', 'S15a0a', 'S22f00', 'S15a02', 'S20500', 'P', 'S38900', 'M', 'S17911', 'S17919', 'S2c300', 'S2c311', 'S30a00', 'P', 'S38700', 'M', 'S15a06', 'S15a41', 'S23d04', 'M', 'S2ff00', 'S10050', 'S10058', 'S10011', 'S22a04', 'M', 'S1f522', 'S2ea46', 'S2ea02', 'S1f50a', 'P', 'S38700', 'R', 'S3770b', 'S20305', 'S20500', 'S20303', 'S20500', 'S37713', 'S30e00', 'S30124', 'R', 'S1f522', 'S2ea46', 'S2ea02', 'S1f50a', 'P', 'S38700', 'M', 'S20356', 'S20350', 'S37706', 'S22f04', 'S30e00', 'S30124', 'M', 'S1f740', 'S1f748', 'S26a20', 'M', 'S1814b', 'S20500', 'S20500', 'S18143', 'R', 'S10000', 'S26500', 'R', 'S10011', 'S2ff00', 'P', 'S38700', 'M', 'S10000', 'S26500', 'M', 'S15a18', 'S15a18', 'S10e22', 'S10e02', 'P', 'S38800']
2021-11-23 14:06:14,584 - DEBUG - joeynmt.training - 	Raw factor:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a', 'S1f0', 'S265', 'M', 'S10e', 'P', 'S388', 'M', 'S1dc', 'S100', 'S205', 'S205', 'S30a', 'M', 'S203', 'S203', 'S203', 'S203', 'S30e', 'S301', 'M', 'S205', 'S100', 'M', 'S301', 'S100', 'S265', 'M', 'S22f', 'S205', 'S15a', 'S22f', 'S15a', 'S205', 'P', 'S389', 'M', 'S179', 'S179', 'S2c3', 'S2c3', 'S30a', 'P', 'S387', 'M', 'S15a', 'S15a', 'S23d', 'M', 'S2ff', 'S100', 'S100', 'S100', 'S22a', 'M', 'S1f5', 'S2ea', 'S2ea', 'S1f5', 'P', 'S387', 'R', 'S377', 'S203', 'S205', 'S203', 'S205', 'S377', 'S30e', 'S301', 'R', 'S1f5', 'S2ea', 'S2ea', 'S1f5', 'P', 'S387', 'M', 'S203', 'S203', 'S377', 'S22f', 'S30e', 'S301', 'M', 'S1f7', 'S1f7', 'S26a', 'M', 'S181', 'S205', 'S205', 'S181', 'R', 'S100', 'S265', 'R', 'S100', 'S2ff', 'P', 'S387', 'M', 'S100', 'S265', 'M', 'S15a', 'S15a', 'S10e', 'S10e', 'P', 'S388']
2021-11-23 14:06:14,584 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁Verse', '▁3', '1.', '▁"', 'I', '▁am', '▁you', '▁you', '▁you', '▁are', '▁you', ',', '▁you', '▁are', '▁you', '▁are', '▁you', '▁are', '▁you', '▁are', '▁you', '.']
2021-11-23 14:06:14,584 - INFO - joeynmt.training - 	Source:     <2en> <4us> <sent> M S15a07 S1f010 S26507 M S10e00 P S38800 M S1dc0a S10003 S20500 S20500 S30a00 M S20302 S2030a S20340 S20348 S30e00 S30124 M S20500 S10013 M S30124 S10010 S26500 M S22f10 S20500 S15a0a S22f00 S15a02 S20500 P S38900 M S17911 S17919 S2c300 S2c311 S30a00 P S38700 M S15a06 S15a41 S23d04 M S2ff00 S10050 S10058 S10011 S22a04 M S1f522 S2ea46 S2ea02 S1f50a P S38700 R S3770b S20305 S20500 S20303 S20500 S37713 S30e00 S30124 R S1f522 S2ea46 S2ea02 S1f50a P S38700 M S20356 S20350 S37706 S22f04 S30e00 S30124 M S1f740 S1f748 S26a20 M S1814b S20500 S20500 S18143 R S10000 S26500 R S10011 S2ff00 P S38700 M S10000 S26500 M S15a18 S15a18 S10e22 S10e02 P S38800
2021-11-23 14:06:14,584 - INFO - joeynmt.training - 	Factor:     <2en> <4us> <sent> M S15a S1f0 S265 M S10e P S388 M S1dc S100 S205 S205 S30a M S203 S203 S203 S203 S30e S301 M S205 S100 M S301 S100 S265 M S22f S205 S15a S22f S15a S205 P S389 M S179 S179 S2c3 S2c3 S30a P S387 M S15a S15a S23d M S2ff S100 S100 S100 S22a M S1f5 S2ea S2ea S1f5 P S387 R S377 S203 S205 S203 S205 S377 S30e S301 R S1f5 S2ea S2ea S1f5 P S387 M S203 S203 S377 S22f S30e S301 M S1f7 S1f7 S26a M S181 S205 S205 S181 R S100 S265 R S100 S2ff P S387 M S100 S265 M S15a S15a S10e S10e P S388
2021-11-23 14:06:14,584 - INFO - joeynmt.training - 	Reference:  ▁Verse ▁2. ▁Then ▁make ▁me ▁tr ul y ▁hap p y ▁by ▁ag ree ing ▁whole he art ed ly ▁with ▁each ▁other , ▁lo v ing ▁one ▁another , ▁and ▁work ing ▁together ▁with ▁one ▁m ind ▁and ▁p ur p ose .
2021-11-23 14:06:14,584 - INFO - joeynmt.training - 	Hypothesis: ▁Verse ▁3 1. ▁" I ▁am ▁you ▁you ▁you ▁are ▁you , ▁you ▁are ▁you ▁are ▁you ▁are ▁you ▁are ▁you .
2021-11-23 14:06:14,584 - INFO - joeynmt.training - Example #3
2021-11-23 14:06:14,584 - DEBUG - joeynmt.training - 	Raw source:     ['<2pt>', '<4br>', '<dict>', 'M', 'S10e40', 'S11040', 'S26504', 'S2fb04']
2021-11-23 14:06:14,584 - DEBUG - joeynmt.training - 	Raw factor:     ['<2pt>', '<4br>', '<dict>', 'M', 'S10e', 'S110', 'S265', 'S2fb']
2021-11-23 14:06:14,584 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁c', 'é']
2021-11-23 14:06:14,584 - INFO - joeynmt.training - 	Source:     <2pt> <4br> <dict> M S10e40 S11040 S26504 S2fb04
2021-11-23 14:06:14,584 - INFO - joeynmt.training - 	Factor:     <2pt> <4br> <dict> M S10e S110 S265 S2fb
2021-11-23 14:06:14,584 - INFO - joeynmt.training - 	Reference:  ▁expl or ar
2021-11-23 14:06:14,584 - INFO - joeynmt.training - 	Hypothesis: ▁c é
2021-11-23 14:06:14,584 - INFO - joeynmt.training - Example #6
2021-11-23 14:06:14,584 - DEBUG - joeynmt.training - 	Raw source:     ['<2en>', '<4us>', '<dict>', 'M', 'S20340', 'S21800']
2021-11-23 14:06:14,584 - DEBUG - joeynmt.training - 	Raw factor:     ['<2en>', '<4us>', '<dict>', 'M', 'S203', 'S218']
2021-11-23 14:06:14,584 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁s', 'in']
2021-11-23 14:06:14,585 - INFO - joeynmt.training - 	Source:     <2en> <4us> <dict> M S20340 S21800
2021-11-23 14:06:14,585 - INFO - joeynmt.training - 	Factor:     <2en> <4us> <dict> M S203 S218
2021-11-23 14:06:14,585 - INFO - joeynmt.training - 	Reference:  ▁m il k
2021-11-23 14:06:14,585 - INFO - joeynmt.training - 	Hypothesis: ▁s in
2021-11-23 14:06:14,585 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step     8000: bleu:   0.45, loss: 120069.5000, ppl:  34.6509, duration: 292.2527s
2021-11-23 14:06:27,472 - INFO - joeynmt.training - Epoch   3, Step:     8100, Batch Loss:     3.571730, Tokens per Sec:     2275, Lr: 0.000100
2021-11-23 14:06:40,213 - INFO - joeynmt.training - Epoch   3, Step:     8200, Batch Loss:     3.536785, Tokens per Sec:     2233, Lr: 0.000100
2021-11-23 14:06:52,936 - INFO - joeynmt.training - Epoch   3, Step:     8300, Batch Loss:     3.526322, Tokens per Sec:     2313, Lr: 0.000100
2021-11-23 14:07:05,533 - INFO - joeynmt.training - Epoch   3, Step:     8400, Batch Loss:     3.517023, Tokens per Sec:     2208, Lr: 0.000100
2021-11-23 14:07:18,272 - INFO - joeynmt.training - Epoch   3, Step:     8500, Batch Loss:     3.611138, Tokens per Sec:     2221, Lr: 0.000100
2021-11-23 14:07:30,958 - INFO - joeynmt.training - Epoch   3, Step:     8600, Batch Loss:     3.474821, Tokens per Sec:     2176, Lr: 0.000100
2021-11-23 14:07:43,882 - INFO - joeynmt.training - Epoch   3, Step:     8700, Batch Loss:     3.457154, Tokens per Sec:     2360, Lr: 0.000100
2021-11-23 14:07:56,395 - INFO - joeynmt.training - Epoch   3, Step:     8800, Batch Loss:     3.418967, Tokens per Sec:     2267, Lr: 0.000100
2021-11-23 14:08:09,133 - INFO - joeynmt.training - Epoch   3, Step:     8900, Batch Loss:     3.605594, Tokens per Sec:     2297, Lr: 0.000100
2021-11-23 14:08:21,849 - INFO - joeynmt.training - Epoch   3, Step:     9000, Batch Loss:     3.558795, Tokens per Sec:     2229, Lr: 0.000100
2021-11-23 14:10:52,097 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-11-23 14:10:52,098 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-11-23 14:10:52,098 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-11-23 14:10:52,108 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-11-23 14:10:52,878 - INFO - joeynmt.helpers - delete models/baseline_multilingual_mem/7000.ckpt
2021-11-23 14:10:52,878 - INFO - joeynmt.helpers - delete /net/cephfs/home/zifjia/signwriting-translation/models/baseline_multilingual_mem/7000.ckpt
2021-11-23 14:10:52,879 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /net/cephfs/home/zifjia/signwriting-translation/models/baseline_multilingual_mem/7000.ckpt but file does not exist. ([Errno 2] No such file or directory: '/net/cephfs/home/zifjia/signwriting-translation/models/baseline_multilingual_mem/7000.ckpt')
2021-11-23 14:10:52,905 - INFO - joeynmt.training - Example #0
2021-11-23 14:10:52,905 - DEBUG - joeynmt.training - 	Raw source:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a07', 'S1f010', 'S26507', 'M', 'S11e20', 'S1a520', 'P', 'S38800', 'M', 'S15a56', 'S15a41', 'S2b700', 'S37c06', 'S36d01', 'S30d00', 'M', 'S10040', 'P', 'S38700', 'M', 'S37806', 'S10050', 'S20356', 'S36d01', 'M', 'S15a37', 'S1ce51', 'S26a07', 'P', 'S38700', 'M', 'S19220', 'S2a20c', 'S11520', 'S10120', 'S1f720', 'S20320', 'S36d01', 'M', 'S10018', 'S26505', 'S10641', 'M', 'S1f000', 'S1f720', 'S1dc20', 'S19220', 'S14a20', 'S1dc20', 'S14a20', 'P', 'S38800', 'M', 'S10041', 'S30d00', 'S36d01', 'M', 'S15a31', 'S10018', 'S26503', 'M', 'S2e74c', 'S14220', 'S2e700', 'S14228', 'M', 'S26521', 'S1f548', 'S1f540', 'M', 'S10041', 'P', 'S38700', 'M', 'S10021', 'S10029', 'S22a07', 'S22a11', 'S36d01', 'M', 'S2ea3e', 'S10000', 'M', 'S15a18', 'S10051', 'S26601', 'M', 'S10058', 'S26526', 'S10050', 'S20500', 'S20500', 'P', 'S38900', 'M', 'S15a37', 'S15a01', 'S2e900', 'S36d01', 'M', 'S15a20', 'S26501', 'M', 'S26521', 'S23b0a', 'S1f548', 'S15a40', 'S15a48', 'S23b1a', 'S1f540', 'M', 'S14c57', 'S14c5f', 'S26511', 'S26507', 'S22527', 'S20314', 'S22521', 'S2031c', 'P', 'S38800']
2021-11-23 14:10:52,906 - DEBUG - joeynmt.training - 	Raw factor:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a', 'S1f0', 'S265', 'M', 'S11e', 'S1a5', 'P', 'S388', 'M', 'S15a', 'S15a', 'S2b7', 'S37c', 'S36d', 'S30d', 'M', 'S100', 'P', 'S387', 'M', 'S378', 'S100', 'S203', 'S36d', 'M', 'S15a', 'S1ce', 'S26a', 'P', 'S387', 'M', 'S192', 'S2a2', 'S115', 'S101', 'S1f7', 'S203', 'S36d', 'M', 'S100', 'S265', 'S106', 'M', 'S1f0', 'S1f7', 'S1dc', 'S192', 'S14a', 'S1dc', 'S14a', 'P', 'S388', 'M', 'S100', 'S30d', 'S36d', 'M', 'S15a', 'S100', 'S265', 'M', 'S2e7', 'S142', 'S2e7', 'S142', 'M', 'S265', 'S1f5', 'S1f5', 'M', 'S100', 'P', 'S387', 'M', 'S100', 'S100', 'S22a', 'S22a', 'S36d', 'M', 'S2ea', 'S100', 'M', 'S15a', 'S100', 'S266', 'M', 'S100', 'S265', 'S100', 'S205', 'S205', 'P', 'S389', 'M', 'S15a', 'S15a', 'S2e9', 'S36d', 'M', 'S15a', 'S265', 'M', 'S265', 'S23b', 'S1f5', 'S15a', 'S15a', 'S23b', 'S1f5', 'M', 'S14c', 'S14c', 'S265', 'S265', 'S225', 'S203', 'S225', 'S203', 'P', 'S388']
2021-11-23 14:10:52,906 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁Verse', '▁11.', '▁Then', '▁the', '▁LORD', '▁was', '▁the', '▁LORD', ',', '▁"', 'I', '▁am', '▁the', '▁king', "'", 't', '▁have', '▁be', '▁be', '▁be', '▁the', '▁king', "'", 's', '▁of', '▁the', '▁king', "'", 's', '▁of', '▁the', '▁king', "'", 's', '▁of', '▁the', '▁king', "'", 's', '▁of', '▁the', '▁king', "'", 's', '▁of', '▁the', '▁king', "'", 's', '▁of', '▁the', '▁king', "'", 's', '.']
2021-11-23 14:10:52,906 - INFO - joeynmt.training - 	Source:     <2en> <4us> <sent> M S15a07 S1f010 S26507 M S11e20 S1a520 P S38800 M S15a56 S15a41 S2b700 S37c06 S36d01 S30d00 M S10040 P S38700 M S37806 S10050 S20356 S36d01 M S15a37 S1ce51 S26a07 P S38700 M S19220 S2a20c S11520 S10120 S1f720 S20320 S36d01 M S10018 S26505 S10641 M S1f000 S1f720 S1dc20 S19220 S14a20 S1dc20 S14a20 P S38800 M S10041 S30d00 S36d01 M S15a31 S10018 S26503 M S2e74c S14220 S2e700 S14228 M S26521 S1f548 S1f540 M S10041 P S38700 M S10021 S10029 S22a07 S22a11 S36d01 M S2ea3e S10000 M S15a18 S10051 S26601 M S10058 S26526 S10050 S20500 S20500 P S38900 M S15a37 S15a01 S2e900 S36d01 M S15a20 S26501 M S26521 S23b0a S1f548 S15a40 S15a48 S23b1a S1f540 M S14c57 S14c5f S26511 S26507 S22527 S20314 S22521 S2031c P S38800
2021-11-23 14:10:52,906 - INFO - joeynmt.training - 	Factor:     <2en> <4us> <sent> M S15a S1f0 S265 M S11e S1a5 P S388 M S15a S15a S2b7 S37c S36d S30d M S100 P S387 M S378 S100 S203 S36d M S15a S1ce S26a P S387 M S192 S2a2 S115 S101 S1f7 S203 S36d M S100 S265 S106 M S1f0 S1f7 S1dc S192 S14a S1dc S14a P S388 M S100 S30d S36d M S15a S100 S265 M S2e7 S142 S2e7 S142 M S265 S1f5 S1f5 M S100 P S387 M S100 S100 S22a S22a S36d M S2ea S100 M S15a S100 S266 M S100 S265 S100 S205 S205 P S389 M S15a S15a S2e9 S36d M S15a S265 M S265 S23b S1f5 S15a S15a S23b S1f5 M S14c S14c S265 S265 S225 S203 S225 S203 P S388
2021-11-23 14:10:52,906 - INFO - joeynmt.training - 	Reference:  ▁Verse ▁3 7. ▁After ▁him , ▁at ▁the ▁time ▁of ▁the ▁c ens us , ▁there ▁was ▁Jud as ▁of ▁Gal ile e . ▁He ▁g ot ▁people ▁to ▁follow ▁him , ▁but ▁he ▁was ▁k illed , ▁too , ▁and ▁all ▁his ▁follow ers ▁were ▁sc atter ed .
2021-11-23 14:10:52,906 - INFO - joeynmt.training - 	Hypothesis: ▁Verse ▁11. ▁Then ▁the ▁LORD ▁was ▁the ▁LORD , ▁" I ▁am ▁the ▁king ' t ▁have ▁be ▁be ▁be ▁the ▁king ' s ▁of ▁the ▁king ' s ▁of ▁the ▁king ' s ▁of ▁the ▁king ' s ▁of ▁the ▁king ' s ▁of ▁the ▁king ' s ▁of ▁the ▁king ' s .
2021-11-23 14:10:52,906 - INFO - joeynmt.training - Example #1
2021-11-23 14:10:52,906 - DEBUG - joeynmt.training - 	Raw source:     ['<2pt>', '<4br>', '<dict>', 'M', 'S34700', 'S21100', 'S1f410']
2021-11-23 14:10:52,906 - DEBUG - joeynmt.training - 	Raw factor:     ['<2pt>', '<4br>', '<dict>', 'M', 'S347', 'S211', 'S1f4']
2021-11-23 14:10:52,907 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁S', 'A']
2021-11-23 14:10:52,907 - INFO - joeynmt.training - 	Source:     <2pt> <4br> <dict> M S34700 S21100 S1f410
2021-11-23 14:10:52,907 - INFO - joeynmt.training - 	Factor:     <2pt> <4br> <dict> M S347 S211 S1f4
2021-11-23 14:10:52,907 - INFO - joeynmt.training - 	Reference:  ▁L á p is
2021-11-23 14:10:52,907 - INFO - joeynmt.training - 	Hypothesis: ▁S A
2021-11-23 14:10:52,907 - INFO - joeynmt.training - Example #2
2021-11-23 14:10:52,907 - DEBUG - joeynmt.training - 	Raw source:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a07', 'S1f010', 'S26507', 'M', 'S10e00', 'P', 'S38800', 'M', 'S1dc0a', 'S10003', 'S20500', 'S20500', 'S30a00', 'M', 'S20302', 'S2030a', 'S20340', 'S20348', 'S30e00', 'S30124', 'M', 'S20500', 'S10013', 'M', 'S30124', 'S10010', 'S26500', 'M', 'S22f10', 'S20500', 'S15a0a', 'S22f00', 'S15a02', 'S20500', 'P', 'S38900', 'M', 'S17911', 'S17919', 'S2c300', 'S2c311', 'S30a00', 'P', 'S38700', 'M', 'S15a06', 'S15a41', 'S23d04', 'M', 'S2ff00', 'S10050', 'S10058', 'S10011', 'S22a04', 'M', 'S1f522', 'S2ea46', 'S2ea02', 'S1f50a', 'P', 'S38700', 'R', 'S3770b', 'S20305', 'S20500', 'S20303', 'S20500', 'S37713', 'S30e00', 'S30124', 'R', 'S1f522', 'S2ea46', 'S2ea02', 'S1f50a', 'P', 'S38700', 'M', 'S20356', 'S20350', 'S37706', 'S22f04', 'S30e00', 'S30124', 'M', 'S1f740', 'S1f748', 'S26a20', 'M', 'S1814b', 'S20500', 'S20500', 'S18143', 'R', 'S10000', 'S26500', 'R', 'S10011', 'S2ff00', 'P', 'S38700', 'M', 'S10000', 'S26500', 'M', 'S15a18', 'S15a18', 'S10e22', 'S10e02', 'P', 'S38800']
2021-11-23 14:10:52,907 - DEBUG - joeynmt.training - 	Raw factor:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a', 'S1f0', 'S265', 'M', 'S10e', 'P', 'S388', 'M', 'S1dc', 'S100', 'S205', 'S205', 'S30a', 'M', 'S203', 'S203', 'S203', 'S203', 'S30e', 'S301', 'M', 'S205', 'S100', 'M', 'S301', 'S100', 'S265', 'M', 'S22f', 'S205', 'S15a', 'S22f', 'S15a', 'S205', 'P', 'S389', 'M', 'S179', 'S179', 'S2c3', 'S2c3', 'S30a', 'P', 'S387', 'M', 'S15a', 'S15a', 'S23d', 'M', 'S2ff', 'S100', 'S100', 'S100', 'S22a', 'M', 'S1f5', 'S2ea', 'S2ea', 'S1f5', 'P', 'S387', 'R', 'S377', 'S203', 'S205', 'S203', 'S205', 'S377', 'S30e', 'S301', 'R', 'S1f5', 'S2ea', 'S2ea', 'S1f5', 'P', 'S387', 'M', 'S203', 'S203', 'S377', 'S22f', 'S30e', 'S301', 'M', 'S1f7', 'S1f7', 'S26a', 'M', 'S181', 'S205', 'S205', 'S181', 'R', 'S100', 'S265', 'R', 'S100', 'S2ff', 'P', 'S387', 'M', 'S100', 'S265', 'M', 'S15a', 'S15a', 'S10e', 'S10e', 'P', 'S388']
2021-11-23 14:10:52,907 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁Verse', '▁11.', '▁"', 'I', "'", 't', '▁have', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '.']
2021-11-23 14:10:52,907 - INFO - joeynmt.training - 	Source:     <2en> <4us> <sent> M S15a07 S1f010 S26507 M S10e00 P S38800 M S1dc0a S10003 S20500 S20500 S30a00 M S20302 S2030a S20340 S20348 S30e00 S30124 M S20500 S10013 M S30124 S10010 S26500 M S22f10 S20500 S15a0a S22f00 S15a02 S20500 P S38900 M S17911 S17919 S2c300 S2c311 S30a00 P S38700 M S15a06 S15a41 S23d04 M S2ff00 S10050 S10058 S10011 S22a04 M S1f522 S2ea46 S2ea02 S1f50a P S38700 R S3770b S20305 S20500 S20303 S20500 S37713 S30e00 S30124 R S1f522 S2ea46 S2ea02 S1f50a P S38700 M S20356 S20350 S37706 S22f04 S30e00 S30124 M S1f740 S1f748 S26a20 M S1814b S20500 S20500 S18143 R S10000 S26500 R S10011 S2ff00 P S38700 M S10000 S26500 M S15a18 S15a18 S10e22 S10e02 P S38800
2021-11-23 14:10:52,908 - INFO - joeynmt.training - 	Factor:     <2en> <4us> <sent> M S15a S1f0 S265 M S10e P S388 M S1dc S100 S205 S205 S30a M S203 S203 S203 S203 S30e S301 M S205 S100 M S301 S100 S265 M S22f S205 S15a S22f S15a S205 P S389 M S179 S179 S2c3 S2c3 S30a P S387 M S15a S15a S23d M S2ff S100 S100 S100 S22a M S1f5 S2ea S2ea S1f5 P S387 R S377 S203 S205 S203 S205 S377 S30e S301 R S1f5 S2ea S2ea S1f5 P S387 M S203 S203 S377 S22f S30e S301 M S1f7 S1f7 S26a M S181 S205 S205 S181 R S100 S265 R S100 S2ff P S387 M S100 S265 M S15a S15a S10e S10e P S388
2021-11-23 14:10:52,908 - INFO - joeynmt.training - 	Reference:  ▁Verse ▁2. ▁Then ▁make ▁me ▁tr ul y ▁hap p y ▁by ▁ag ree ing ▁whole he art ed ly ▁with ▁each ▁other , ▁lo v ing ▁one ▁another , ▁and ▁work ing ▁together ▁with ▁one ▁m ind ▁and ▁p ur p ose .
2021-11-23 14:10:52,908 - INFO - joeynmt.training - 	Hypothesis: ▁Verse ▁11. ▁" I ' t ▁have ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be .
2021-11-23 14:10:52,908 - INFO - joeynmt.training - Example #3
2021-11-23 14:10:52,908 - DEBUG - joeynmt.training - 	Raw source:     ['<2pt>', '<4br>', '<dict>', 'M', 'S10e40', 'S11040', 'S26504', 'S2fb04']
2021-11-23 14:10:52,908 - DEBUG - joeynmt.training - 	Raw factor:     ['<2pt>', '<4br>', '<dict>', 'M', 'S10e', 'S110', 'S265', 'S2fb']
2021-11-23 14:10:52,908 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁c', 'ol', 'ar']
2021-11-23 14:10:52,908 - INFO - joeynmt.training - 	Source:     <2pt> <4br> <dict> M S10e40 S11040 S26504 S2fb04
2021-11-23 14:10:52,908 - INFO - joeynmt.training - 	Factor:     <2pt> <4br> <dict> M S10e S110 S265 S2fb
2021-11-23 14:10:52,908 - INFO - joeynmt.training - 	Reference:  ▁expl or ar
2021-11-23 14:10:52,908 - INFO - joeynmt.training - 	Hypothesis: ▁c ol ar
2021-11-23 14:10:52,909 - INFO - joeynmt.training - Example #6
2021-11-23 14:10:52,909 - DEBUG - joeynmt.training - 	Raw source:     ['<2en>', '<4us>', '<dict>', 'M', 'S20340', 'S21800']
2021-11-23 14:10:52,909 - DEBUG - joeynmt.training - 	Raw factor:     ['<2en>', '<4us>', '<dict>', 'M', 'S203', 'S218']
2021-11-23 14:10:52,909 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁b', 'an']
2021-11-23 14:10:52,909 - INFO - joeynmt.training - 	Source:     <2en> <4us> <dict> M S20340 S21800
2021-11-23 14:10:52,909 - INFO - joeynmt.training - 	Factor:     <2en> <4us> <dict> M S203 S218
2021-11-23 14:10:52,909 - INFO - joeynmt.training - 	Reference:  ▁m il k
2021-11-23 14:10:52,909 - INFO - joeynmt.training - 	Hypothesis: ▁b an
2021-11-23 14:10:52,910 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step     9000: bleu:   0.87, loss: 118997.1094, ppl:  33.5709, duration: 151.0604s
2021-11-23 14:11:05,669 - INFO - joeynmt.training - Epoch   3, Step:     9100, Batch Loss:     3.715649, Tokens per Sec:     2348, Lr: 0.000100
2021-11-23 14:11:18,202 - INFO - joeynmt.training - Epoch   3, Step:     9200, Batch Loss:     3.443977, Tokens per Sec:     2260, Lr: 0.000100
2021-11-23 14:11:30,930 - INFO - joeynmt.training - Epoch   3, Step:     9300, Batch Loss:     3.577338, Tokens per Sec:     2203, Lr: 0.000100
2021-11-23 14:11:43,597 - INFO - joeynmt.training - Epoch   3, Step:     9400, Batch Loss:     3.391625, Tokens per Sec:     2259, Lr: 0.000100
2021-11-23 14:11:56,107 - INFO - joeynmt.training - Epoch   3, Step:     9500, Batch Loss:     3.482686, Tokens per Sec:     2307, Lr: 0.000100
2021-11-23 14:12:08,857 - INFO - joeynmt.training - Epoch   3, Step:     9600, Batch Loss:     3.591293, Tokens per Sec:     2258, Lr: 0.000100
2021-11-23 14:12:21,428 - INFO - joeynmt.training - Epoch   3, Step:     9700, Batch Loss:     3.601469, Tokens per Sec:     2268, Lr: 0.000100
2021-11-23 14:12:34,237 - INFO - joeynmt.training - Epoch   3, Step:     9800, Batch Loss:     3.606640, Tokens per Sec:     2272, Lr: 0.000100
2021-11-23 14:12:46,721 - INFO - joeynmt.training - Epoch   3, Step:     9900, Batch Loss:     3.462374, Tokens per Sec:     2328, Lr: 0.000100
2021-11-23 14:12:59,450 - INFO - joeynmt.training - Epoch   3, Step:    10000, Batch Loss:     3.437863, Tokens per Sec:     2244, Lr: 0.000100
2021-11-23 14:18:19,754 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-11-23 14:18:19,754 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-11-23 14:18:19,754 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-11-23 14:18:19,770 - INFO - joeynmt.training - Example #0
2021-11-23 14:18:19,770 - DEBUG - joeynmt.training - 	Raw source:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a07', 'S1f010', 'S26507', 'M', 'S11e20', 'S1a520', 'P', 'S38800', 'M', 'S15a56', 'S15a41', 'S2b700', 'S37c06', 'S36d01', 'S30d00', 'M', 'S10040', 'P', 'S38700', 'M', 'S37806', 'S10050', 'S20356', 'S36d01', 'M', 'S15a37', 'S1ce51', 'S26a07', 'P', 'S38700', 'M', 'S19220', 'S2a20c', 'S11520', 'S10120', 'S1f720', 'S20320', 'S36d01', 'M', 'S10018', 'S26505', 'S10641', 'M', 'S1f000', 'S1f720', 'S1dc20', 'S19220', 'S14a20', 'S1dc20', 'S14a20', 'P', 'S38800', 'M', 'S10041', 'S30d00', 'S36d01', 'M', 'S15a31', 'S10018', 'S26503', 'M', 'S2e74c', 'S14220', 'S2e700', 'S14228', 'M', 'S26521', 'S1f548', 'S1f540', 'M', 'S10041', 'P', 'S38700', 'M', 'S10021', 'S10029', 'S22a07', 'S22a11', 'S36d01', 'M', 'S2ea3e', 'S10000', 'M', 'S15a18', 'S10051', 'S26601', 'M', 'S10058', 'S26526', 'S10050', 'S20500', 'S20500', 'P', 'S38900', 'M', 'S15a37', 'S15a01', 'S2e900', 'S36d01', 'M', 'S15a20', 'S26501', 'M', 'S26521', 'S23b0a', 'S1f548', 'S15a40', 'S15a48', 'S23b1a', 'S1f540', 'M', 'S14c57', 'S14c5f', 'S26511', 'S26507', 'S22527', 'S20314', 'S22521', 'S2031c', 'P', 'S38800']
2021-11-23 14:18:19,770 - DEBUG - joeynmt.training - 	Raw factor:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a', 'S1f0', 'S265', 'M', 'S11e', 'S1a5', 'P', 'S388', 'M', 'S15a', 'S15a', 'S2b7', 'S37c', 'S36d', 'S30d', 'M', 'S100', 'P', 'S387', 'M', 'S378', 'S100', 'S203', 'S36d', 'M', 'S15a', 'S1ce', 'S26a', 'P', 'S387', 'M', 'S192', 'S2a2', 'S115', 'S101', 'S1f7', 'S203', 'S36d', 'M', 'S100', 'S265', 'S106', 'M', 'S1f0', 'S1f7', 'S1dc', 'S192', 'S14a', 'S1dc', 'S14a', 'P', 'S388', 'M', 'S100', 'S30d', 'S36d', 'M', 'S15a', 'S100', 'S265', 'M', 'S2e7', 'S142', 'S2e7', 'S142', 'M', 'S265', 'S1f5', 'S1f5', 'M', 'S100', 'P', 'S387', 'M', 'S100', 'S100', 'S22a', 'S22a', 'S36d', 'M', 'S2ea', 'S100', 'M', 'S15a', 'S100', 'S266', 'M', 'S100', 'S265', 'S100', 'S205', 'S205', 'P', 'S389', 'M', 'S15a', 'S15a', 'S2e9', 'S36d', 'M', 'S15a', 'S265', 'M', 'S265', 'S23b', 'S1f5', 'S15a', 'S15a', 'S23b', 'S1f5', 'M', 'S14c', 'S14c', 'S265', 'S265', 'S225', 'S203', 'S225', 'S203', 'P', 'S388']
2021-11-23 14:18:19,770 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁Verse', '▁6.', '▁The', '▁LORD', '▁was', '▁the', '▁king', '▁of', '▁the', '▁king', "'", 's', '▁the', '▁king', "'", 's', '▁of', '▁the', '▁king', "'", 's', '▁of', '▁the', '▁king', "'", 's', '▁of', '▁the', '▁king', "'", 's', '▁of', '▁the', '▁king', "'", 's', '▁of', '▁the', '▁king', "'", 's', '▁of', '▁the', '▁king', "'", 's', '▁of', '▁the', '▁king', "'", 's', '▁of', '▁the', '▁king', "'", 's', '.']
2021-11-23 14:18:19,770 - INFO - joeynmt.training - 	Source:     <2en> <4us> <sent> M S15a07 S1f010 S26507 M S11e20 S1a520 P S38800 M S15a56 S15a41 S2b700 S37c06 S36d01 S30d00 M S10040 P S38700 M S37806 S10050 S20356 S36d01 M S15a37 S1ce51 S26a07 P S38700 M S19220 S2a20c S11520 S10120 S1f720 S20320 S36d01 M S10018 S26505 S10641 M S1f000 S1f720 S1dc20 S19220 S14a20 S1dc20 S14a20 P S38800 M S10041 S30d00 S36d01 M S15a31 S10018 S26503 M S2e74c S14220 S2e700 S14228 M S26521 S1f548 S1f540 M S10041 P S38700 M S10021 S10029 S22a07 S22a11 S36d01 M S2ea3e S10000 M S15a18 S10051 S26601 M S10058 S26526 S10050 S20500 S20500 P S38900 M S15a37 S15a01 S2e900 S36d01 M S15a20 S26501 M S26521 S23b0a S1f548 S15a40 S15a48 S23b1a S1f540 M S14c57 S14c5f S26511 S26507 S22527 S20314 S22521 S2031c P S38800
2021-11-23 14:18:19,771 - INFO - joeynmt.training - 	Factor:     <2en> <4us> <sent> M S15a S1f0 S265 M S11e S1a5 P S388 M S15a S15a S2b7 S37c S36d S30d M S100 P S387 M S378 S100 S203 S36d M S15a S1ce S26a P S387 M S192 S2a2 S115 S101 S1f7 S203 S36d M S100 S265 S106 M S1f0 S1f7 S1dc S192 S14a S1dc S14a P S388 M S100 S30d S36d M S15a S100 S265 M S2e7 S142 S2e7 S142 M S265 S1f5 S1f5 M S100 P S387 M S100 S100 S22a S22a S36d M S2ea S100 M S15a S100 S266 M S100 S265 S100 S205 S205 P S389 M S15a S15a S2e9 S36d M S15a S265 M S265 S23b S1f5 S15a S15a S23b S1f5 M S14c S14c S265 S265 S225 S203 S225 S203 P S388
2021-11-23 14:18:19,771 - INFO - joeynmt.training - 	Reference:  ▁Verse ▁3 7. ▁After ▁him , ▁at ▁the ▁time ▁of ▁the ▁c ens us , ▁there ▁was ▁Jud as ▁of ▁Gal ile e . ▁He ▁g ot ▁people ▁to ▁follow ▁him , ▁but ▁he ▁was ▁k illed , ▁too , ▁and ▁all ▁his ▁follow ers ▁were ▁sc atter ed .
2021-11-23 14:18:19,771 - INFO - joeynmt.training - 	Hypothesis: ▁Verse ▁6. ▁The ▁LORD ▁was ▁the ▁king ▁of ▁the ▁king ' s ▁the ▁king ' s ▁of ▁the ▁king ' s ▁of ▁the ▁king ' s ▁of ▁the ▁king ' s ▁of ▁the ▁king ' s ▁of ▁the ▁king ' s ▁of ▁the ▁king ' s ▁of ▁the ▁king ' s ▁of ▁the ▁king ' s .
2021-11-23 14:18:19,771 - INFO - joeynmt.training - Example #1
2021-11-23 14:18:19,771 - DEBUG - joeynmt.training - 	Raw source:     ['<2pt>', '<4br>', '<dict>', 'M', 'S34700', 'S21100', 'S1f410']
2021-11-23 14:18:19,771 - DEBUG - joeynmt.training - 	Raw factor:     ['<2pt>', '<4br>', '<dict>', 'M', 'S347', 'S211', 'S1f4']
2021-11-23 14:18:19,771 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁P', 'O']
2021-11-23 14:18:19,771 - INFO - joeynmt.training - 	Source:     <2pt> <4br> <dict> M S34700 S21100 S1f410
2021-11-23 14:18:19,771 - INFO - joeynmt.training - 	Factor:     <2pt> <4br> <dict> M S347 S211 S1f4
2021-11-23 14:18:19,771 - INFO - joeynmt.training - 	Reference:  ▁L á p is
2021-11-23 14:18:19,771 - INFO - joeynmt.training - 	Hypothesis: ▁P O
2021-11-23 14:18:19,771 - INFO - joeynmt.training - Example #2
2021-11-23 14:18:19,771 - DEBUG - joeynmt.training - 	Raw source:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a07', 'S1f010', 'S26507', 'M', 'S10e00', 'P', 'S38800', 'M', 'S1dc0a', 'S10003', 'S20500', 'S20500', 'S30a00', 'M', 'S20302', 'S2030a', 'S20340', 'S20348', 'S30e00', 'S30124', 'M', 'S20500', 'S10013', 'M', 'S30124', 'S10010', 'S26500', 'M', 'S22f10', 'S20500', 'S15a0a', 'S22f00', 'S15a02', 'S20500', 'P', 'S38900', 'M', 'S17911', 'S17919', 'S2c300', 'S2c311', 'S30a00', 'P', 'S38700', 'M', 'S15a06', 'S15a41', 'S23d04', 'M', 'S2ff00', 'S10050', 'S10058', 'S10011', 'S22a04', 'M', 'S1f522', 'S2ea46', 'S2ea02', 'S1f50a', 'P', 'S38700', 'R', 'S3770b', 'S20305', 'S20500', 'S20303', 'S20500', 'S37713', 'S30e00', 'S30124', 'R', 'S1f522', 'S2ea46', 'S2ea02', 'S1f50a', 'P', 'S38700', 'M', 'S20356', 'S20350', 'S37706', 'S22f04', 'S30e00', 'S30124', 'M', 'S1f740', 'S1f748', 'S26a20', 'M', 'S1814b', 'S20500', 'S20500', 'S18143', 'R', 'S10000', 'S26500', 'R', 'S10011', 'S2ff00', 'P', 'S38700', 'M', 'S10000', 'S26500', 'M', 'S15a18', 'S15a18', 'S10e22', 'S10e02', 'P', 'S38800']
2021-11-23 14:18:19,771 - DEBUG - joeynmt.training - 	Raw factor:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a', 'S1f0', 'S265', 'M', 'S10e', 'P', 'S388', 'M', 'S1dc', 'S100', 'S205', 'S205', 'S30a', 'M', 'S203', 'S203', 'S203', 'S203', 'S30e', 'S301', 'M', 'S205', 'S100', 'M', 'S301', 'S100', 'S265', 'M', 'S22f', 'S205', 'S15a', 'S22f', 'S15a', 'S205', 'P', 'S389', 'M', 'S179', 'S179', 'S2c3', 'S2c3', 'S30a', 'P', 'S387', 'M', 'S15a', 'S15a', 'S23d', 'M', 'S2ff', 'S100', 'S100', 'S100', 'S22a', 'M', 'S1f5', 'S2ea', 'S2ea', 'S1f5', 'P', 'S387', 'R', 'S377', 'S203', 'S205', 'S203', 'S205', 'S377', 'S30e', 'S301', 'R', 'S1f5', 'S2ea', 'S2ea', 'S1f5', 'P', 'S387', 'M', 'S203', 'S203', 'S377', 'S22f', 'S30e', 'S301', 'M', 'S1f7', 'S1f7', 'S26a', 'M', 'S181', 'S205', 'S205', 'S181', 'R', 'S100', 'S265', 'R', 'S100', 'S2ff', 'P', 'S387', 'M', 'S100', 'S265', 'M', 'S15a', 'S15a', 'S10e', 'S10e', 'P', 'S388']
2021-11-23 14:18:19,771 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁Verse', '▁6.', '▁"', 'I', '▁am', '▁you', '▁have', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '.']
2021-11-23 14:18:19,771 - INFO - joeynmt.training - 	Source:     <2en> <4us> <sent> M S15a07 S1f010 S26507 M S10e00 P S38800 M S1dc0a S10003 S20500 S20500 S30a00 M S20302 S2030a S20340 S20348 S30e00 S30124 M S20500 S10013 M S30124 S10010 S26500 M S22f10 S20500 S15a0a S22f00 S15a02 S20500 P S38900 M S17911 S17919 S2c300 S2c311 S30a00 P S38700 M S15a06 S15a41 S23d04 M S2ff00 S10050 S10058 S10011 S22a04 M S1f522 S2ea46 S2ea02 S1f50a P S38700 R S3770b S20305 S20500 S20303 S20500 S37713 S30e00 S30124 R S1f522 S2ea46 S2ea02 S1f50a P S38700 M S20356 S20350 S37706 S22f04 S30e00 S30124 M S1f740 S1f748 S26a20 M S1814b S20500 S20500 S18143 R S10000 S26500 R S10011 S2ff00 P S38700 M S10000 S26500 M S15a18 S15a18 S10e22 S10e02 P S38800
2021-11-23 14:18:19,771 - INFO - joeynmt.training - 	Factor:     <2en> <4us> <sent> M S15a S1f0 S265 M S10e P S388 M S1dc S100 S205 S205 S30a M S203 S203 S203 S203 S30e S301 M S205 S100 M S301 S100 S265 M S22f S205 S15a S22f S15a S205 P S389 M S179 S179 S2c3 S2c3 S30a P S387 M S15a S15a S23d M S2ff S100 S100 S100 S22a M S1f5 S2ea S2ea S1f5 P S387 R S377 S203 S205 S203 S205 S377 S30e S301 R S1f5 S2ea S2ea S1f5 P S387 M S203 S203 S377 S22f S30e S301 M S1f7 S1f7 S26a M S181 S205 S205 S181 R S100 S265 R S100 S2ff P S387 M S100 S265 M S15a S15a S10e S10e P S388
2021-11-23 14:18:19,771 - INFO - joeynmt.training - 	Reference:  ▁Verse ▁2. ▁Then ▁make ▁me ▁tr ul y ▁hap p y ▁by ▁ag ree ing ▁whole he art ed ly ▁with ▁each ▁other , ▁lo v ing ▁one ▁another , ▁and ▁work ing ▁together ▁with ▁one ▁m ind ▁and ▁p ur p ose .
2021-11-23 14:18:19,771 - INFO - joeynmt.training - 	Hypothesis: ▁Verse ▁6. ▁" I ▁am ▁you ▁have ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be .
2021-11-23 14:18:19,771 - INFO - joeynmt.training - Example #3
2021-11-23 14:18:19,771 - DEBUG - joeynmt.training - 	Raw source:     ['<2pt>', '<4br>', '<dict>', 'M', 'S10e40', 'S11040', 'S26504', 'S2fb04']
2021-11-23 14:18:19,771 - DEBUG - joeynmt.training - 	Raw factor:     ['<2pt>', '<4br>', '<dict>', 'M', 'S10e', 'S110', 'S265', 'S2fb']
2021-11-23 14:18:19,771 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁m', 'ol', 'ar']
2021-11-23 14:18:19,771 - INFO - joeynmt.training - 	Source:     <2pt> <4br> <dict> M S10e40 S11040 S26504 S2fb04
2021-11-23 14:18:19,772 - INFO - joeynmt.training - 	Factor:     <2pt> <4br> <dict> M S10e S110 S265 S2fb
2021-11-23 14:18:19,772 - INFO - joeynmt.training - 	Reference:  ▁expl or ar
2021-11-23 14:18:19,772 - INFO - joeynmt.training - 	Hypothesis: ▁m ol ar
2021-11-23 14:18:19,772 - INFO - joeynmt.training - Example #6
2021-11-23 14:18:19,772 - DEBUG - joeynmt.training - 	Raw source:     ['<2en>', '<4us>', '<dict>', 'M', 'S20340', 'S21800']
2021-11-23 14:18:19,772 - DEBUG - joeynmt.training - 	Raw factor:     ['<2en>', '<4us>', '<dict>', 'M', 'S203', 'S218']
2021-11-23 14:18:19,772 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁b', 'ol']
2021-11-23 14:18:19,772 - INFO - joeynmt.training - 	Source:     <2en> <4us> <dict> M S20340 S21800
2021-11-23 14:18:19,772 - INFO - joeynmt.training - 	Factor:     <2en> <4us> <dict> M S203 S218
2021-11-23 14:18:19,772 - INFO - joeynmt.training - 	Reference:  ▁m il k
2021-11-23 14:18:19,772 - INFO - joeynmt.training - 	Hypothesis: ▁b ol
2021-11-23 14:18:19,772 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step    10000: bleu:   0.41, loss: 117410.9297, ppl:  32.0348, duration: 320.3211s
2021-11-23 14:18:32,676 - INFO - joeynmt.training - Epoch   3, Step:    10100, Batch Loss:     3.565575, Tokens per Sec:     2312, Lr: 0.000100
2021-11-23 14:18:45,487 - INFO - joeynmt.training - Epoch   3, Step:    10200, Batch Loss:     3.520003, Tokens per Sec:     2240, Lr: 0.000100
2021-11-23 14:18:58,008 - INFO - joeynmt.training - Epoch   3, Step:    10300, Batch Loss:     3.412712, Tokens per Sec:     2308, Lr: 0.000100
2021-11-23 14:19:10,642 - INFO - joeynmt.training - Epoch   3, Step:    10400, Batch Loss:     3.561478, Tokens per Sec:     2220, Lr: 0.000100
2021-11-23 14:19:23,323 - INFO - joeynmt.training - Epoch   3, Step:    10500, Batch Loss:     3.527809, Tokens per Sec:     2295, Lr: 0.000100
2021-11-23 14:19:36,011 - INFO - joeynmt.training - Epoch   3, Step:    10600, Batch Loss:     3.588526, Tokens per Sec:     2216, Lr: 0.000100
2021-11-23 14:19:48,572 - INFO - joeynmt.training - Epoch   3, Step:    10700, Batch Loss:     3.495532, Tokens per Sec:     2207, Lr: 0.000100
2021-11-23 14:20:01,249 - INFO - joeynmt.training - Epoch   3, Step:    10800, Batch Loss:     3.413318, Tokens per Sec:     2299, Lr: 0.000100
2021-11-23 14:20:04,227 - INFO - joeynmt.training - Epoch   3: total training loss 12751.70
2021-11-23 14:20:04,228 - INFO - joeynmt.training - EPOCH 4
2021-11-23 14:20:14,060 - INFO - joeynmt.training - Epoch   4, Step:    10900, Batch Loss:     3.516568, Tokens per Sec:     2315, Lr: 0.000100
2021-11-23 14:20:26,765 - INFO - joeynmt.training - Epoch   4, Step:    11000, Batch Loss:     3.408736, Tokens per Sec:     2246, Lr: 0.000100
2021-11-23 14:23:34,455 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-11-23 14:23:34,455 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-11-23 14:23:34,455 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-11-23 14:23:34,535 - INFO - joeynmt.training - Example #0
2021-11-23 14:23:34,535 - DEBUG - joeynmt.training - 	Raw source:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a07', 'S1f010', 'S26507', 'M', 'S11e20', 'S1a520', 'P', 'S38800', 'M', 'S15a56', 'S15a41', 'S2b700', 'S37c06', 'S36d01', 'S30d00', 'M', 'S10040', 'P', 'S38700', 'M', 'S37806', 'S10050', 'S20356', 'S36d01', 'M', 'S15a37', 'S1ce51', 'S26a07', 'P', 'S38700', 'M', 'S19220', 'S2a20c', 'S11520', 'S10120', 'S1f720', 'S20320', 'S36d01', 'M', 'S10018', 'S26505', 'S10641', 'M', 'S1f000', 'S1f720', 'S1dc20', 'S19220', 'S14a20', 'S1dc20', 'S14a20', 'P', 'S38800', 'M', 'S10041', 'S30d00', 'S36d01', 'M', 'S15a31', 'S10018', 'S26503', 'M', 'S2e74c', 'S14220', 'S2e700', 'S14228', 'M', 'S26521', 'S1f548', 'S1f540', 'M', 'S10041', 'P', 'S38700', 'M', 'S10021', 'S10029', 'S22a07', 'S22a11', 'S36d01', 'M', 'S2ea3e', 'S10000', 'M', 'S15a18', 'S10051', 'S26601', 'M', 'S10058', 'S26526', 'S10050', 'S20500', 'S20500', 'P', 'S38900', 'M', 'S15a37', 'S15a01', 'S2e900', 'S36d01', 'M', 'S15a20', 'S26501', 'M', 'S26521', 'S23b0a', 'S1f548', 'S15a40', 'S15a48', 'S23b1a', 'S1f540', 'M', 'S14c57', 'S14c5f', 'S26511', 'S26507', 'S22527', 'S20314', 'S22521', 'S2031c', 'P', 'S38800']
2021-11-23 14:23:34,535 - DEBUG - joeynmt.training - 	Raw factor:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a', 'S1f0', 'S265', 'M', 'S11e', 'S1a5', 'P', 'S388', 'M', 'S15a', 'S15a', 'S2b7', 'S37c', 'S36d', 'S30d', 'M', 'S100', 'P', 'S387', 'M', 'S378', 'S100', 'S203', 'S36d', 'M', 'S15a', 'S1ce', 'S26a', 'P', 'S387', 'M', 'S192', 'S2a2', 'S115', 'S101', 'S1f7', 'S203', 'S36d', 'M', 'S100', 'S265', 'S106', 'M', 'S1f0', 'S1f7', 'S1dc', 'S192', 'S14a', 'S1dc', 'S14a', 'P', 'S388', 'M', 'S100', 'S30d', 'S36d', 'M', 'S15a', 'S100', 'S265', 'M', 'S2e7', 'S142', 'S2e7', 'S142', 'M', 'S265', 'S1f5', 'S1f5', 'M', 'S100', 'P', 'S387', 'M', 'S100', 'S100', 'S22a', 'S22a', 'S36d', 'M', 'S2ea', 'S100', 'M', 'S15a', 'S100', 'S266', 'M', 'S100', 'S265', 'S100', 'S205', 'S205', 'P', 'S389', 'M', 'S15a', 'S15a', 'S2e9', 'S36d', 'M', 'S15a', 'S265', 'M', 'S265', 'S23b', 'S1f5', 'S15a', 'S15a', 'S23b', 'S1f5', 'M', 'S14c', 'S14c', 'S265', 'S265', 'S225', 'S203', 'S225', 'S203', 'P', 'S388']
2021-11-23 14:23:34,536 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁Verse', '▁3', '0.', '▁Then', '▁the', '▁king', "'", 's', '▁disciples', '▁said', ',', '▁"', 'I', '▁am', '▁the', '▁king', "'", 't', '▁come', '▁to', '▁the', '▁king', "'", 's', '▁of', '▁the', '▁king', "'", 's', '▁of', '▁the', '▁king', "'", 's', '▁of', '▁the', '▁king', "'", 's', '▁of', '▁the', '▁king', "'", 's', '▁of', '▁the', '▁king', "'", 's', '▁of', '▁the', '▁king', "'", 's', '."']
2021-11-23 14:23:34,536 - INFO - joeynmt.training - 	Source:     <2en> <4us> <sent> M S15a07 S1f010 S26507 M S11e20 S1a520 P S38800 M S15a56 S15a41 S2b700 S37c06 S36d01 S30d00 M S10040 P S38700 M S37806 S10050 S20356 S36d01 M S15a37 S1ce51 S26a07 P S38700 M S19220 S2a20c S11520 S10120 S1f720 S20320 S36d01 M S10018 S26505 S10641 M S1f000 S1f720 S1dc20 S19220 S14a20 S1dc20 S14a20 P S38800 M S10041 S30d00 S36d01 M S15a31 S10018 S26503 M S2e74c S14220 S2e700 S14228 M S26521 S1f548 S1f540 M S10041 P S38700 M S10021 S10029 S22a07 S22a11 S36d01 M S2ea3e S10000 M S15a18 S10051 S26601 M S10058 S26526 S10050 S20500 S20500 P S38900 M S15a37 S15a01 S2e900 S36d01 M S15a20 S26501 M S26521 S23b0a S1f548 S15a40 S15a48 S23b1a S1f540 M S14c57 S14c5f S26511 S26507 S22527 S20314 S22521 S2031c P S38800
2021-11-23 14:23:34,536 - INFO - joeynmt.training - 	Factor:     <2en> <4us> <sent> M S15a S1f0 S265 M S11e S1a5 P S388 M S15a S15a S2b7 S37c S36d S30d M S100 P S387 M S378 S100 S203 S36d M S15a S1ce S26a P S387 M S192 S2a2 S115 S101 S1f7 S203 S36d M S100 S265 S106 M S1f0 S1f7 S1dc S192 S14a S1dc S14a P S388 M S100 S30d S36d M S15a S100 S265 M S2e7 S142 S2e7 S142 M S265 S1f5 S1f5 M S100 P S387 M S100 S100 S22a S22a S36d M S2ea S100 M S15a S100 S266 M S100 S265 S100 S205 S205 P S389 M S15a S15a S2e9 S36d M S15a S265 M S265 S23b S1f5 S15a S15a S23b S1f5 M S14c S14c S265 S265 S225 S203 S225 S203 P S388
2021-11-23 14:23:34,536 - INFO - joeynmt.training - 	Reference:  ▁Verse ▁3 7. ▁After ▁him , ▁at ▁the ▁time ▁of ▁the ▁c ens us , ▁there ▁was ▁Jud as ▁of ▁Gal ile e . ▁He ▁g ot ▁people ▁to ▁follow ▁him , ▁but ▁he ▁was ▁k illed , ▁too , ▁and ▁all ▁his ▁follow ers ▁were ▁sc atter ed .
2021-11-23 14:23:34,536 - INFO - joeynmt.training - 	Hypothesis: ▁Verse ▁3 0. ▁Then ▁the ▁king ' s ▁disciples ▁said , ▁" I ▁am ▁the ▁king ' t ▁come ▁to ▁the ▁king ' s ▁of ▁the ▁king ' s ▁of ▁the ▁king ' s ▁of ▁the ▁king ' s ▁of ▁the ▁king ' s ▁of ▁the ▁king ' s ▁of ▁the ▁king ' s ."
2021-11-23 14:23:34,536 - INFO - joeynmt.training - Example #1
2021-11-23 14:23:34,536 - DEBUG - joeynmt.training - 	Raw source:     ['<2pt>', '<4br>', '<dict>', 'M', 'S34700', 'S21100', 'S1f410']
2021-11-23 14:23:34,537 - DEBUG - joeynmt.training - 	Raw factor:     ['<2pt>', '<4br>', '<dict>', 'M', 'S347', 'S211', 'S1f4']
2021-11-23 14:23:34,537 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁F', 'o']
2021-11-23 14:23:34,537 - INFO - joeynmt.training - 	Source:     <2pt> <4br> <dict> M S34700 S21100 S1f410
2021-11-23 14:23:34,537 - INFO - joeynmt.training - 	Factor:     <2pt> <4br> <dict> M S347 S211 S1f4
2021-11-23 14:23:34,537 - INFO - joeynmt.training - 	Reference:  ▁L á p is
2021-11-23 14:23:34,537 - INFO - joeynmt.training - 	Hypothesis: ▁F o
2021-11-23 14:23:34,537 - INFO - joeynmt.training - Example #2
2021-11-23 14:23:34,537 - DEBUG - joeynmt.training - 	Raw source:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a07', 'S1f010', 'S26507', 'M', 'S10e00', 'P', 'S38800', 'M', 'S1dc0a', 'S10003', 'S20500', 'S20500', 'S30a00', 'M', 'S20302', 'S2030a', 'S20340', 'S20348', 'S30e00', 'S30124', 'M', 'S20500', 'S10013', 'M', 'S30124', 'S10010', 'S26500', 'M', 'S22f10', 'S20500', 'S15a0a', 'S22f00', 'S15a02', 'S20500', 'P', 'S38900', 'M', 'S17911', 'S17919', 'S2c300', 'S2c311', 'S30a00', 'P', 'S38700', 'M', 'S15a06', 'S15a41', 'S23d04', 'M', 'S2ff00', 'S10050', 'S10058', 'S10011', 'S22a04', 'M', 'S1f522', 'S2ea46', 'S2ea02', 'S1f50a', 'P', 'S38700', 'R', 'S3770b', 'S20305', 'S20500', 'S20303', 'S20500', 'S37713', 'S30e00', 'S30124', 'R', 'S1f522', 'S2ea46', 'S2ea02', 'S1f50a', 'P', 'S38700', 'M', 'S20356', 'S20350', 'S37706', 'S22f04', 'S30e00', 'S30124', 'M', 'S1f740', 'S1f748', 'S26a20', 'M', 'S1814b', 'S20500', 'S20500', 'S18143', 'R', 'S10000', 'S26500', 'R', 'S10011', 'S2ff00', 'P', 'S38700', 'M', 'S10000', 'S26500', 'M', 'S15a18', 'S15a18', 'S10e22', 'S10e02', 'P', 'S38800']
2021-11-23 14:23:34,537 - DEBUG - joeynmt.training - 	Raw factor:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a', 'S1f0', 'S265', 'M', 'S10e', 'P', 'S388', 'M', 'S1dc', 'S100', 'S205', 'S205', 'S30a', 'M', 'S203', 'S203', 'S203', 'S203', 'S30e', 'S301', 'M', 'S205', 'S100', 'M', 'S301', 'S100', 'S265', 'M', 'S22f', 'S205', 'S15a', 'S22f', 'S15a', 'S205', 'P', 'S389', 'M', 'S179', 'S179', 'S2c3', 'S2c3', 'S30a', 'P', 'S387', 'M', 'S15a', 'S15a', 'S23d', 'M', 'S2ff', 'S100', 'S100', 'S100', 'S22a', 'M', 'S1f5', 'S2ea', 'S2ea', 'S1f5', 'P', 'S387', 'R', 'S377', 'S203', 'S205', 'S203', 'S205', 'S377', 'S30e', 'S301', 'R', 'S1f5', 'S2ea', 'S2ea', 'S1f5', 'P', 'S387', 'M', 'S203', 'S203', 'S377', 'S22f', 'S30e', 'S301', 'M', 'S1f7', 'S1f7', 'S26a', 'M', 'S181', 'S205', 'S205', 'S181', 'R', 'S100', 'S265', 'R', 'S100', 'S2ff', 'P', 'S387', 'M', 'S100', 'S265', 'M', 'S15a', 'S15a', 'S10e', 'S10e', 'P', 'S388']
2021-11-23 14:23:34,537 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁Verse', '▁8.', '▁"', 'I', '▁am', '▁you', '▁are', '▁you', ',', '▁"', 'I', '▁am', '▁you', '▁are', '▁you', '▁are', '▁you', '▁are', '▁you', '▁are', '▁you', '▁are', '▁you', '▁are', '▁you', '▁are', '▁not', '▁not', '▁not', '▁not', '▁not', '▁not', '▁not', '▁not', '▁not', '▁not', '▁not', '.']
2021-11-23 14:23:34,538 - INFO - joeynmt.training - 	Source:     <2en> <4us> <sent> M S15a07 S1f010 S26507 M S10e00 P S38800 M S1dc0a S10003 S20500 S20500 S30a00 M S20302 S2030a S20340 S20348 S30e00 S30124 M S20500 S10013 M S30124 S10010 S26500 M S22f10 S20500 S15a0a S22f00 S15a02 S20500 P S38900 M S17911 S17919 S2c300 S2c311 S30a00 P S38700 M S15a06 S15a41 S23d04 M S2ff00 S10050 S10058 S10011 S22a04 M S1f522 S2ea46 S2ea02 S1f50a P S38700 R S3770b S20305 S20500 S20303 S20500 S37713 S30e00 S30124 R S1f522 S2ea46 S2ea02 S1f50a P S38700 M S20356 S20350 S37706 S22f04 S30e00 S30124 M S1f740 S1f748 S26a20 M S1814b S20500 S20500 S18143 R S10000 S26500 R S10011 S2ff00 P S38700 M S10000 S26500 M S15a18 S15a18 S10e22 S10e02 P S38800
2021-11-23 14:23:34,538 - INFO - joeynmt.training - 	Factor:     <2en> <4us> <sent> M S15a S1f0 S265 M S10e P S388 M S1dc S100 S205 S205 S30a M S203 S203 S203 S203 S30e S301 M S205 S100 M S301 S100 S265 M S22f S205 S15a S22f S15a S205 P S389 M S179 S179 S2c3 S2c3 S30a P S387 M S15a S15a S23d M S2ff S100 S100 S100 S22a M S1f5 S2ea S2ea S1f5 P S387 R S377 S203 S205 S203 S205 S377 S30e S301 R S1f5 S2ea S2ea S1f5 P S387 M S203 S203 S377 S22f S30e S301 M S1f7 S1f7 S26a M S181 S205 S205 S181 R S100 S265 R S100 S2ff P S387 M S100 S265 M S15a S15a S10e S10e P S388
2021-11-23 14:23:34,538 - INFO - joeynmt.training - 	Reference:  ▁Verse ▁2. ▁Then ▁make ▁me ▁tr ul y ▁hap p y ▁by ▁ag ree ing ▁whole he art ed ly ▁with ▁each ▁other , ▁lo v ing ▁one ▁another , ▁and ▁work ing ▁together ▁with ▁one ▁m ind ▁and ▁p ur p ose .
2021-11-23 14:23:34,538 - INFO - joeynmt.training - 	Hypothesis: ▁Verse ▁8. ▁" I ▁am ▁you ▁are ▁you , ▁" I ▁am ▁you ▁are ▁you ▁are ▁you ▁are ▁you ▁are ▁you ▁are ▁you ▁are ▁you ▁are ▁not ▁not ▁not ▁not ▁not ▁not ▁not ▁not ▁not ▁not ▁not .
2021-11-23 14:23:34,538 - INFO - joeynmt.training - Example #3
2021-11-23 14:23:34,538 - DEBUG - joeynmt.training - 	Raw source:     ['<2pt>', '<4br>', '<dict>', 'M', 'S10e40', 'S11040', 'S26504', 'S2fb04']
2021-11-23 14:23:34,538 - DEBUG - joeynmt.training - 	Raw factor:     ['<2pt>', '<4br>', '<dict>', 'M', 'S10e', 'S110', 'S265', 'S2fb']
2021-11-23 14:23:34,538 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁c', 'un', 'ar']
2021-11-23 14:23:34,538 - INFO - joeynmt.training - 	Source:     <2pt> <4br> <dict> M S10e40 S11040 S26504 S2fb04
2021-11-23 14:23:34,538 - INFO - joeynmt.training - 	Factor:     <2pt> <4br> <dict> M S10e S110 S265 S2fb
2021-11-23 14:23:34,538 - INFO - joeynmt.training - 	Reference:  ▁expl or ar
2021-11-23 14:23:34,539 - INFO - joeynmt.training - 	Hypothesis: ▁c un ar
2021-11-23 14:23:34,539 - INFO - joeynmt.training - Example #6
2021-11-23 14:23:34,539 - DEBUG - joeynmt.training - 	Raw source:     ['<2en>', '<4us>', '<dict>', 'M', 'S20340', 'S21800']
2021-11-23 14:23:34,539 - DEBUG - joeynmt.training - 	Raw factor:     ['<2en>', '<4us>', '<dict>', 'M', 'S203', 'S218']
2021-11-23 14:23:34,539 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁b', 'at']
2021-11-23 14:23:34,539 - INFO - joeynmt.training - 	Source:     <2en> <4us> <dict> M S20340 S21800
2021-11-23 14:23:34,539 - INFO - joeynmt.training - 	Factor:     <2en> <4us> <dict> M S203 S218
2021-11-23 14:23:34,539 - INFO - joeynmt.training - 	Reference:  ▁m il k
2021-11-23 14:23:34,539 - INFO - joeynmt.training - 	Hypothesis: ▁b at
2021-11-23 14:23:34,540 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step    11000: bleu:   0.79, loss: 116384.7969, ppl:  31.0788, duration: 187.7743s
2021-11-23 14:23:47,205 - INFO - joeynmt.training - Epoch   4, Step:    11100, Batch Loss:     3.382377, Tokens per Sec:     2298, Lr: 0.000100
2021-11-23 14:23:59,641 - INFO - joeynmt.training - Epoch   4, Step:    11200, Batch Loss:     3.419158, Tokens per Sec:     2269, Lr: 0.000100
2021-11-23 14:24:12,251 - INFO - joeynmt.training - Epoch   4, Step:    11300, Batch Loss:     3.378256, Tokens per Sec:     2292, Lr: 0.000100
2021-11-23 14:24:25,140 - INFO - joeynmt.training - Epoch   4, Step:    11400, Batch Loss:     3.396338, Tokens per Sec:     2247, Lr: 0.000100
2021-11-23 14:24:37,805 - INFO - joeynmt.training - Epoch   4, Step:    11500, Batch Loss:     3.513776, Tokens per Sec:     2269, Lr: 0.000100
2021-11-23 14:24:50,666 - INFO - joeynmt.training - Epoch   4, Step:    11600, Batch Loss:     3.617555, Tokens per Sec:     2270, Lr: 0.000100
2021-11-23 14:25:03,319 - INFO - joeynmt.training - Epoch   4, Step:    11700, Batch Loss:     3.379177, Tokens per Sec:     2235, Lr: 0.000100
2021-11-23 14:25:15,978 - INFO - joeynmt.training - Epoch   4, Step:    11800, Batch Loss:     3.344358, Tokens per Sec:     2249, Lr: 0.000100
2021-11-23 14:25:28,697 - INFO - joeynmt.training - Epoch   4, Step:    11900, Batch Loss:     3.442121, Tokens per Sec:     2283, Lr: 0.000100
2021-11-23 14:25:41,348 - INFO - joeynmt.training - Epoch   4, Step:    12000, Batch Loss:     3.321985, Tokens per Sec:     2264, Lr: 0.000100
2021-11-23 14:27:42,386 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-11-23 14:27:42,386 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-11-23 14:27:42,386 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-11-23 14:27:42,397 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-11-23 14:27:43,381 - INFO - joeynmt.helpers - delete models/baseline_multilingual_mem/9000.ckpt
2021-11-23 14:27:43,410 - INFO - joeynmt.helpers - delete /net/cephfs/home/zifjia/signwriting-translation/models/baseline_multilingual_mem/9000.ckpt
2021-11-23 14:27:43,422 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /net/cephfs/home/zifjia/signwriting-translation/models/baseline_multilingual_mem/9000.ckpt but file does not exist. ([Errno 2] No such file or directory: '/net/cephfs/home/zifjia/signwriting-translation/models/baseline_multilingual_mem/9000.ckpt')
2021-11-23 14:27:43,509 - INFO - joeynmt.training - Example #0
2021-11-23 14:27:43,509 - DEBUG - joeynmt.training - 	Raw source:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a07', 'S1f010', 'S26507', 'M', 'S11e20', 'S1a520', 'P', 'S38800', 'M', 'S15a56', 'S15a41', 'S2b700', 'S37c06', 'S36d01', 'S30d00', 'M', 'S10040', 'P', 'S38700', 'M', 'S37806', 'S10050', 'S20356', 'S36d01', 'M', 'S15a37', 'S1ce51', 'S26a07', 'P', 'S38700', 'M', 'S19220', 'S2a20c', 'S11520', 'S10120', 'S1f720', 'S20320', 'S36d01', 'M', 'S10018', 'S26505', 'S10641', 'M', 'S1f000', 'S1f720', 'S1dc20', 'S19220', 'S14a20', 'S1dc20', 'S14a20', 'P', 'S38800', 'M', 'S10041', 'S30d00', 'S36d01', 'M', 'S15a31', 'S10018', 'S26503', 'M', 'S2e74c', 'S14220', 'S2e700', 'S14228', 'M', 'S26521', 'S1f548', 'S1f540', 'M', 'S10041', 'P', 'S38700', 'M', 'S10021', 'S10029', 'S22a07', 'S22a11', 'S36d01', 'M', 'S2ea3e', 'S10000', 'M', 'S15a18', 'S10051', 'S26601', 'M', 'S10058', 'S26526', 'S10050', 'S20500', 'S20500', 'P', 'S38900', 'M', 'S15a37', 'S15a01', 'S2e900', 'S36d01', 'M', 'S15a20', 'S26501', 'M', 'S26521', 'S23b0a', 'S1f548', 'S15a40', 'S15a48', 'S23b1a', 'S1f540', 'M', 'S14c57', 'S14c5f', 'S26511', 'S26507', 'S22527', 'S20314', 'S22521', 'S2031c', 'P', 'S38800']
2021-11-23 14:27:43,510 - DEBUG - joeynmt.training - 	Raw factor:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a', 'S1f0', 'S265', 'M', 'S11e', 'S1a5', 'P', 'S388', 'M', 'S15a', 'S15a', 'S2b7', 'S37c', 'S36d', 'S30d', 'M', 'S100', 'P', 'S387', 'M', 'S378', 'S100', 'S203', 'S36d', 'M', 'S15a', 'S1ce', 'S26a', 'P', 'S387', 'M', 'S192', 'S2a2', 'S115', 'S101', 'S1f7', 'S203', 'S36d', 'M', 'S100', 'S265', 'S106', 'M', 'S1f0', 'S1f7', 'S1dc', 'S192', 'S14a', 'S1dc', 'S14a', 'P', 'S388', 'M', 'S100', 'S30d', 'S36d', 'M', 'S15a', 'S100', 'S265', 'M', 'S2e7', 'S142', 'S2e7', 'S142', 'M', 'S265', 'S1f5', 'S1f5', 'M', 'S100', 'P', 'S387', 'M', 'S100', 'S100', 'S22a', 'S22a', 'S36d', 'M', 'S2ea', 'S100', 'M', 'S15a', 'S100', 'S266', 'M', 'S100', 'S265', 'S100', 'S205', 'S205', 'P', 'S389', 'M', 'S15a', 'S15a', 'S2e9', 'S36d', 'M', 'S15a', 'S265', 'M', 'S265', 'S23b', 'S1f5', 'S15a', 'S15a', 'S23b', 'S1f5', 'M', 'S14c', 'S14c', 'S265', 'S265', 'S225', 'S203', 'S225', 'S203', 'P', 'S388']
2021-11-23 14:27:43,510 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁Verse', '▁3', '1.', '▁"', 'I', '▁am', '▁the', '▁king', '▁of', '▁the', '▁king', "'", 's', '▁people', '▁of', '▁the', '▁king', "'", 's', '▁people', '▁of', '▁the', '▁king', "'", 's', '▁people', '▁of', '▁the', '▁king', "'", 's', '▁people', '▁of', '▁the', '▁king', "'", 's', '▁people', '▁of', '▁the', '▁king', "'", 's', '▁people', '.']
2021-11-23 14:27:43,510 - INFO - joeynmt.training - 	Source:     <2en> <4us> <sent> M S15a07 S1f010 S26507 M S11e20 S1a520 P S38800 M S15a56 S15a41 S2b700 S37c06 S36d01 S30d00 M S10040 P S38700 M S37806 S10050 S20356 S36d01 M S15a37 S1ce51 S26a07 P S38700 M S19220 S2a20c S11520 S10120 S1f720 S20320 S36d01 M S10018 S26505 S10641 M S1f000 S1f720 S1dc20 S19220 S14a20 S1dc20 S14a20 P S38800 M S10041 S30d00 S36d01 M S15a31 S10018 S26503 M S2e74c S14220 S2e700 S14228 M S26521 S1f548 S1f540 M S10041 P S38700 M S10021 S10029 S22a07 S22a11 S36d01 M S2ea3e S10000 M S15a18 S10051 S26601 M S10058 S26526 S10050 S20500 S20500 P S38900 M S15a37 S15a01 S2e900 S36d01 M S15a20 S26501 M S26521 S23b0a S1f548 S15a40 S15a48 S23b1a S1f540 M S14c57 S14c5f S26511 S26507 S22527 S20314 S22521 S2031c P S38800
2021-11-23 14:27:43,510 - INFO - joeynmt.training - 	Factor:     <2en> <4us> <sent> M S15a S1f0 S265 M S11e S1a5 P S388 M S15a S15a S2b7 S37c S36d S30d M S100 P S387 M S378 S100 S203 S36d M S15a S1ce S26a P S387 M S192 S2a2 S115 S101 S1f7 S203 S36d M S100 S265 S106 M S1f0 S1f7 S1dc S192 S14a S1dc S14a P S388 M S100 S30d S36d M S15a S100 S265 M S2e7 S142 S2e7 S142 M S265 S1f5 S1f5 M S100 P S387 M S100 S100 S22a S22a S36d M S2ea S100 M S15a S100 S266 M S100 S265 S100 S205 S205 P S389 M S15a S15a S2e9 S36d M S15a S265 M S265 S23b S1f5 S15a S15a S23b S1f5 M S14c S14c S265 S265 S225 S203 S225 S203 P S388
2021-11-23 14:27:43,510 - INFO - joeynmt.training - 	Reference:  ▁Verse ▁3 7. ▁After ▁him , ▁at ▁the ▁time ▁of ▁the ▁c ens us , ▁there ▁was ▁Jud as ▁of ▁Gal ile e . ▁He ▁g ot ▁people ▁to ▁follow ▁him , ▁but ▁he ▁was ▁k illed , ▁too , ▁and ▁all ▁his ▁follow ers ▁were ▁sc atter ed .
2021-11-23 14:27:43,511 - INFO - joeynmt.training - 	Hypothesis: ▁Verse ▁3 1. ▁" I ▁am ▁the ▁king ▁of ▁the ▁king ' s ▁people ▁of ▁the ▁king ' s ▁people ▁of ▁the ▁king ' s ▁people ▁of ▁the ▁king ' s ▁people ▁of ▁the ▁king ' s ▁people ▁of ▁the ▁king ' s ▁people .
2021-11-23 14:27:43,511 - INFO - joeynmt.training - Example #1
2021-11-23 14:27:43,511 - DEBUG - joeynmt.training - 	Raw source:     ['<2pt>', '<4br>', '<dict>', 'M', 'S34700', 'S21100', 'S1f410']
2021-11-23 14:27:43,511 - DEBUG - joeynmt.training - 	Raw factor:     ['<2pt>', '<4br>', '<dict>', 'M', 'S347', 'S211', 'S1f4']
2021-11-23 14:27:43,511 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁C', 'A']
2021-11-23 14:27:43,511 - INFO - joeynmt.training - 	Source:     <2pt> <4br> <dict> M S34700 S21100 S1f410
2021-11-23 14:27:43,511 - INFO - joeynmt.training - 	Factor:     <2pt> <4br> <dict> M S347 S211 S1f4
2021-11-23 14:27:43,511 - INFO - joeynmt.training - 	Reference:  ▁L á p is
2021-11-23 14:27:43,511 - INFO - joeynmt.training - 	Hypothesis: ▁C A
2021-11-23 14:27:43,511 - INFO - joeynmt.training - Example #2
2021-11-23 14:27:43,511 - DEBUG - joeynmt.training - 	Raw source:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a07', 'S1f010', 'S26507', 'M', 'S10e00', 'P', 'S38800', 'M', 'S1dc0a', 'S10003', 'S20500', 'S20500', 'S30a00', 'M', 'S20302', 'S2030a', 'S20340', 'S20348', 'S30e00', 'S30124', 'M', 'S20500', 'S10013', 'M', 'S30124', 'S10010', 'S26500', 'M', 'S22f10', 'S20500', 'S15a0a', 'S22f00', 'S15a02', 'S20500', 'P', 'S38900', 'M', 'S17911', 'S17919', 'S2c300', 'S2c311', 'S30a00', 'P', 'S38700', 'M', 'S15a06', 'S15a41', 'S23d04', 'M', 'S2ff00', 'S10050', 'S10058', 'S10011', 'S22a04', 'M', 'S1f522', 'S2ea46', 'S2ea02', 'S1f50a', 'P', 'S38700', 'R', 'S3770b', 'S20305', 'S20500', 'S20303', 'S20500', 'S37713', 'S30e00', 'S30124', 'R', 'S1f522', 'S2ea46', 'S2ea02', 'S1f50a', 'P', 'S38700', 'M', 'S20356', 'S20350', 'S37706', 'S22f04', 'S30e00', 'S30124', 'M', 'S1f740', 'S1f748', 'S26a20', 'M', 'S1814b', 'S20500', 'S20500', 'S18143', 'R', 'S10000', 'S26500', 'R', 'S10011', 'S2ff00', 'P', 'S38700', 'M', 'S10000', 'S26500', 'M', 'S15a18', 'S15a18', 'S10e22', 'S10e02', 'P', 'S38800']
2021-11-23 14:27:43,512 - DEBUG - joeynmt.training - 	Raw factor:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a', 'S1f0', 'S265', 'M', 'S10e', 'P', 'S388', 'M', 'S1dc', 'S100', 'S205', 'S205', 'S30a', 'M', 'S203', 'S203', 'S203', 'S203', 'S30e', 'S301', 'M', 'S205', 'S100', 'M', 'S301', 'S100', 'S265', 'M', 'S22f', 'S205', 'S15a', 'S22f', 'S15a', 'S205', 'P', 'S389', 'M', 'S179', 'S179', 'S2c3', 'S2c3', 'S30a', 'P', 'S387', 'M', 'S15a', 'S15a', 'S23d', 'M', 'S2ff', 'S100', 'S100', 'S100', 'S22a', 'M', 'S1f5', 'S2ea', 'S2ea', 'S1f5', 'P', 'S387', 'R', 'S377', 'S203', 'S205', 'S203', 'S205', 'S377', 'S30e', 'S301', 'R', 'S1f5', 'S2ea', 'S2ea', 'S1f5', 'P', 'S387', 'M', 'S203', 'S203', 'S377', 'S22f', 'S30e', 'S301', 'M', 'S1f7', 'S1f7', 'S26a', 'M', 'S181', 'S205', 'S205', 'S181', 'R', 'S100', 'S265', 'R', 'S100', 'S2ff', 'P', 'S387', 'M', 'S100', 'S265', 'M', 'S15a', 'S15a', 'S10e', 'S10e', 'P', 'S388']
2021-11-23 14:27:43,512 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁Verse', '▁3', '1.', '▁"', 'I', '▁am', '▁you', '▁you', ',', '▁you', '▁are', '▁you', '▁are', '▁you', '▁are', '▁you', ',', '▁you', '▁are', '▁you', '▁are', '▁you', '▁are', '▁you', '.']
2021-11-23 14:27:43,512 - INFO - joeynmt.training - 	Source:     <2en> <4us> <sent> M S15a07 S1f010 S26507 M S10e00 P S38800 M S1dc0a S10003 S20500 S20500 S30a00 M S20302 S2030a S20340 S20348 S30e00 S30124 M S20500 S10013 M S30124 S10010 S26500 M S22f10 S20500 S15a0a S22f00 S15a02 S20500 P S38900 M S17911 S17919 S2c300 S2c311 S30a00 P S38700 M S15a06 S15a41 S23d04 M S2ff00 S10050 S10058 S10011 S22a04 M S1f522 S2ea46 S2ea02 S1f50a P S38700 R S3770b S20305 S20500 S20303 S20500 S37713 S30e00 S30124 R S1f522 S2ea46 S2ea02 S1f50a P S38700 M S20356 S20350 S37706 S22f04 S30e00 S30124 M S1f740 S1f748 S26a20 M S1814b S20500 S20500 S18143 R S10000 S26500 R S10011 S2ff00 P S38700 M S10000 S26500 M S15a18 S15a18 S10e22 S10e02 P S38800
2021-11-23 14:27:43,512 - INFO - joeynmt.training - 	Factor:     <2en> <4us> <sent> M S15a S1f0 S265 M S10e P S388 M S1dc S100 S205 S205 S30a M S203 S203 S203 S203 S30e S301 M S205 S100 M S301 S100 S265 M S22f S205 S15a S22f S15a S205 P S389 M S179 S179 S2c3 S2c3 S30a P S387 M S15a S15a S23d M S2ff S100 S100 S100 S22a M S1f5 S2ea S2ea S1f5 P S387 R S377 S203 S205 S203 S205 S377 S30e S301 R S1f5 S2ea S2ea S1f5 P S387 M S203 S203 S377 S22f S30e S301 M S1f7 S1f7 S26a M S181 S205 S205 S181 R S100 S265 R S100 S2ff P S387 M S100 S265 M S15a S15a S10e S10e P S388
2021-11-23 14:27:43,512 - INFO - joeynmt.training - 	Reference:  ▁Verse ▁2. ▁Then ▁make ▁me ▁tr ul y ▁hap p y ▁by ▁ag ree ing ▁whole he art ed ly ▁with ▁each ▁other , ▁lo v ing ▁one ▁another , ▁and ▁work ing ▁together ▁with ▁one ▁m ind ▁and ▁p ur p ose .
2021-11-23 14:27:43,512 - INFO - joeynmt.training - 	Hypothesis: ▁Verse ▁3 1. ▁" I ▁am ▁you ▁you , ▁you ▁are ▁you ▁are ▁you ▁are ▁you , ▁you ▁are ▁you ▁are ▁you ▁are ▁you .
2021-11-23 14:27:43,512 - INFO - joeynmt.training - Example #3
2021-11-23 14:27:43,512 - DEBUG - joeynmt.training - 	Raw source:     ['<2pt>', '<4br>', '<dict>', 'M', 'S10e40', 'S11040', 'S26504', 'S2fb04']
2021-11-23 14:27:43,512 - DEBUG - joeynmt.training - 	Raw factor:     ['<2pt>', '<4br>', '<dict>', 'M', 'S10e', 'S110', 'S265', 'S2fb']
2021-11-23 14:27:43,512 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁c', 'é', 's']
2021-11-23 14:27:43,512 - INFO - joeynmt.training - 	Source:     <2pt> <4br> <dict> M S10e40 S11040 S26504 S2fb04
2021-11-23 14:27:43,512 - INFO - joeynmt.training - 	Factor:     <2pt> <4br> <dict> M S10e S110 S265 S2fb
2021-11-23 14:27:43,513 - INFO - joeynmt.training - 	Reference:  ▁expl or ar
2021-11-23 14:27:43,513 - INFO - joeynmt.training - 	Hypothesis: ▁c é s
2021-11-23 14:27:43,513 - INFO - joeynmt.training - Example #6
2021-11-23 14:27:43,513 - DEBUG - joeynmt.training - 	Raw source:     ['<2en>', '<4us>', '<dict>', 'M', 'S20340', 'S21800']
2021-11-23 14:27:43,513 - DEBUG - joeynmt.training - 	Raw factor:     ['<2en>', '<4us>', '<dict>', 'M', 'S203', 'S218']
2021-11-23 14:27:43,513 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁b', 'at']
2021-11-23 14:27:43,513 - INFO - joeynmt.training - 	Source:     <2en> <4us> <dict> M S20340 S21800
2021-11-23 14:27:43,513 - INFO - joeynmt.training - 	Factor:     <2en> <4us> <dict> M S203 S218
2021-11-23 14:27:43,513 - INFO - joeynmt.training - 	Reference:  ▁m il k
2021-11-23 14:27:43,513 - INFO - joeynmt.training - 	Hypothesis: ▁b at
2021-11-23 14:27:43,514 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step    12000: bleu:   1.13, loss: 115107.1016, ppl:  29.9281, duration: 122.1651s
2021-11-23 14:27:56,398 - INFO - joeynmt.training - Epoch   4, Step:    12100, Batch Loss:     3.318503, Tokens per Sec:     2200, Lr: 0.000100
2021-11-23 14:28:09,146 - INFO - joeynmt.training - Epoch   4, Step:    12200, Batch Loss:     3.416282, Tokens per Sec:     2379, Lr: 0.000100
2021-11-23 14:28:21,959 - INFO - joeynmt.training - Epoch   4, Step:    12300, Batch Loss:     3.383681, Tokens per Sec:     2272, Lr: 0.000100
2021-11-23 14:28:34,742 - INFO - joeynmt.training - Epoch   4, Step:    12400, Batch Loss:     3.345171, Tokens per Sec:     2298, Lr: 0.000100
2021-11-23 14:28:47,443 - INFO - joeynmt.training - Epoch   4, Step:    12500, Batch Loss:     3.310804, Tokens per Sec:     2268, Lr: 0.000100
2021-11-23 14:29:00,057 - INFO - joeynmt.training - Epoch   4, Step:    12600, Batch Loss:     3.320179, Tokens per Sec:     2198, Lr: 0.000100
2021-11-23 14:29:12,806 - INFO - joeynmt.training - Epoch   4, Step:    12700, Batch Loss:     3.646829, Tokens per Sec:     2302, Lr: 0.000100
2021-11-23 14:29:25,571 - INFO - joeynmt.training - Epoch   4, Step:    12800, Batch Loss:     3.307987, Tokens per Sec:     2264, Lr: 0.000100
2021-11-23 14:29:38,282 - INFO - joeynmt.training - Epoch   4, Step:    12900, Batch Loss:     3.322476, Tokens per Sec:     2278, Lr: 0.000100
2021-11-23 14:29:50,820 - INFO - joeynmt.training - Epoch   4, Step:    13000, Batch Loss:     3.472217, Tokens per Sec:     2216, Lr: 0.000100
2021-11-23 14:34:33,326 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-11-23 14:34:33,327 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-11-23 14:34:33,327 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-11-23 14:34:33,429 - INFO - joeynmt.training - Example #0
2021-11-23 14:34:33,429 - DEBUG - joeynmt.training - 	Raw source:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a07', 'S1f010', 'S26507', 'M', 'S11e20', 'S1a520', 'P', 'S38800', 'M', 'S15a56', 'S15a41', 'S2b700', 'S37c06', 'S36d01', 'S30d00', 'M', 'S10040', 'P', 'S38700', 'M', 'S37806', 'S10050', 'S20356', 'S36d01', 'M', 'S15a37', 'S1ce51', 'S26a07', 'P', 'S38700', 'M', 'S19220', 'S2a20c', 'S11520', 'S10120', 'S1f720', 'S20320', 'S36d01', 'M', 'S10018', 'S26505', 'S10641', 'M', 'S1f000', 'S1f720', 'S1dc20', 'S19220', 'S14a20', 'S1dc20', 'S14a20', 'P', 'S38800', 'M', 'S10041', 'S30d00', 'S36d01', 'M', 'S15a31', 'S10018', 'S26503', 'M', 'S2e74c', 'S14220', 'S2e700', 'S14228', 'M', 'S26521', 'S1f548', 'S1f540', 'M', 'S10041', 'P', 'S38700', 'M', 'S10021', 'S10029', 'S22a07', 'S22a11', 'S36d01', 'M', 'S2ea3e', 'S10000', 'M', 'S15a18', 'S10051', 'S26601', 'M', 'S10058', 'S26526', 'S10050', 'S20500', 'S20500', 'P', 'S38900', 'M', 'S15a37', 'S15a01', 'S2e900', 'S36d01', 'M', 'S15a20', 'S26501', 'M', 'S26521', 'S23b0a', 'S1f548', 'S15a40', 'S15a48', 'S23b1a', 'S1f540', 'M', 'S14c57', 'S14c5f', 'S26511', 'S26507', 'S22527', 'S20314', 'S22521', 'S2031c', 'P', 'S38800']
2021-11-23 14:34:33,429 - DEBUG - joeynmt.training - 	Raw factor:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a', 'S1f0', 'S265', 'M', 'S11e', 'S1a5', 'P', 'S388', 'M', 'S15a', 'S15a', 'S2b7', 'S37c', 'S36d', 'S30d', 'M', 'S100', 'P', 'S387', 'M', 'S378', 'S100', 'S203', 'S36d', 'M', 'S15a', 'S1ce', 'S26a', 'P', 'S387', 'M', 'S192', 'S2a2', 'S115', 'S101', 'S1f7', 'S203', 'S36d', 'M', 'S100', 'S265', 'S106', 'M', 'S1f0', 'S1f7', 'S1dc', 'S192', 'S14a', 'S1dc', 'S14a', 'P', 'S388', 'M', 'S100', 'S30d', 'S36d', 'M', 'S15a', 'S100', 'S265', 'M', 'S2e7', 'S142', 'S2e7', 'S142', 'M', 'S265', 'S1f5', 'S1f5', 'M', 'S100', 'P', 'S387', 'M', 'S100', 'S100', 'S22a', 'S22a', 'S36d', 'M', 'S2ea', 'S100', 'M', 'S15a', 'S100', 'S266', 'M', 'S100', 'S265', 'S100', 'S205', 'S205', 'P', 'S389', 'M', 'S15a', 'S15a', 'S2e9', 'S36d', 'M', 'S15a', 'S265', 'M', 'S265', 'S23b', 'S1f5', 'S15a', 'S15a', 'S23b', 'S1f5', 'M', 'S14c', 'S14c', 'S265', 'S265', 'S225', 'S203', 'S225', 'S203', 'P', 'S388']
2021-11-23 14:34:33,430 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁Verse', '▁3', '2.', '▁"', 'W', 'hen', '▁the', '▁LORD', ',', '▁the', '▁LORD', ',', '▁"', 'I', '▁will', '▁be', '▁be', '▁be', '▁be', '▁a', '▁man', '.']
2021-11-23 14:34:33,430 - INFO - joeynmt.training - 	Source:     <2en> <4us> <sent> M S15a07 S1f010 S26507 M S11e20 S1a520 P S38800 M S15a56 S15a41 S2b700 S37c06 S36d01 S30d00 M S10040 P S38700 M S37806 S10050 S20356 S36d01 M S15a37 S1ce51 S26a07 P S38700 M S19220 S2a20c S11520 S10120 S1f720 S20320 S36d01 M S10018 S26505 S10641 M S1f000 S1f720 S1dc20 S19220 S14a20 S1dc20 S14a20 P S38800 M S10041 S30d00 S36d01 M S15a31 S10018 S26503 M S2e74c S14220 S2e700 S14228 M S26521 S1f548 S1f540 M S10041 P S38700 M S10021 S10029 S22a07 S22a11 S36d01 M S2ea3e S10000 M S15a18 S10051 S26601 M S10058 S26526 S10050 S20500 S20500 P S38900 M S15a37 S15a01 S2e900 S36d01 M S15a20 S26501 M S26521 S23b0a S1f548 S15a40 S15a48 S23b1a S1f540 M S14c57 S14c5f S26511 S26507 S22527 S20314 S22521 S2031c P S38800
2021-11-23 14:34:33,430 - INFO - joeynmt.training - 	Factor:     <2en> <4us> <sent> M S15a S1f0 S265 M S11e S1a5 P S388 M S15a S15a S2b7 S37c S36d S30d M S100 P S387 M S378 S100 S203 S36d M S15a S1ce S26a P S387 M S192 S2a2 S115 S101 S1f7 S203 S36d M S100 S265 S106 M S1f0 S1f7 S1dc S192 S14a S1dc S14a P S388 M S100 S30d S36d M S15a S100 S265 M S2e7 S142 S2e7 S142 M S265 S1f5 S1f5 M S100 P S387 M S100 S100 S22a S22a S36d M S2ea S100 M S15a S100 S266 M S100 S265 S100 S205 S205 P S389 M S15a S15a S2e9 S36d M S15a S265 M S265 S23b S1f5 S15a S15a S23b S1f5 M S14c S14c S265 S265 S225 S203 S225 S203 P S388
2021-11-23 14:34:33,430 - INFO - joeynmt.training - 	Reference:  ▁Verse ▁3 7. ▁After ▁him , ▁at ▁the ▁time ▁of ▁the ▁c ens us , ▁there ▁was ▁Jud as ▁of ▁Gal ile e . ▁He ▁g ot ▁people ▁to ▁follow ▁him , ▁but ▁he ▁was ▁k illed , ▁too , ▁and ▁all ▁his ▁follow ers ▁were ▁sc atter ed .
2021-11-23 14:34:33,430 - INFO - joeynmt.training - 	Hypothesis: ▁Verse ▁3 2. ▁" W hen ▁the ▁LORD , ▁the ▁LORD , ▁" I ▁will ▁be ▁be ▁be ▁be ▁a ▁man .
2021-11-23 14:34:33,430 - INFO - joeynmt.training - Example #1
2021-11-23 14:34:33,430 - DEBUG - joeynmt.training - 	Raw source:     ['<2pt>', '<4br>', '<dict>', 'M', 'S34700', 'S21100', 'S1f410']
2021-11-23 14:34:33,430 - DEBUG - joeynmt.training - 	Raw factor:     ['<2pt>', '<4br>', '<dict>', 'M', 'S347', 'S211', 'S1f4']
2021-11-23 14:34:33,431 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁S', 'U', 'T', 'A']
2021-11-23 14:34:33,431 - INFO - joeynmt.training - 	Source:     <2pt> <4br> <dict> M S34700 S21100 S1f410
2021-11-23 14:34:33,431 - INFO - joeynmt.training - 	Factor:     <2pt> <4br> <dict> M S347 S211 S1f4
2021-11-23 14:34:33,431 - INFO - joeynmt.training - 	Reference:  ▁L á p is
2021-11-23 14:34:33,431 - INFO - joeynmt.training - 	Hypothesis: ▁S U T A
2021-11-23 14:34:33,431 - INFO - joeynmt.training - Example #2
2021-11-23 14:34:33,431 - DEBUG - joeynmt.training - 	Raw source:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a07', 'S1f010', 'S26507', 'M', 'S10e00', 'P', 'S38800', 'M', 'S1dc0a', 'S10003', 'S20500', 'S20500', 'S30a00', 'M', 'S20302', 'S2030a', 'S20340', 'S20348', 'S30e00', 'S30124', 'M', 'S20500', 'S10013', 'M', 'S30124', 'S10010', 'S26500', 'M', 'S22f10', 'S20500', 'S15a0a', 'S22f00', 'S15a02', 'S20500', 'P', 'S38900', 'M', 'S17911', 'S17919', 'S2c300', 'S2c311', 'S30a00', 'P', 'S38700', 'M', 'S15a06', 'S15a41', 'S23d04', 'M', 'S2ff00', 'S10050', 'S10058', 'S10011', 'S22a04', 'M', 'S1f522', 'S2ea46', 'S2ea02', 'S1f50a', 'P', 'S38700', 'R', 'S3770b', 'S20305', 'S20500', 'S20303', 'S20500', 'S37713', 'S30e00', 'S30124', 'R', 'S1f522', 'S2ea46', 'S2ea02', 'S1f50a', 'P', 'S38700', 'M', 'S20356', 'S20350', 'S37706', 'S22f04', 'S30e00', 'S30124', 'M', 'S1f740', 'S1f748', 'S26a20', 'M', 'S1814b', 'S20500', 'S20500', 'S18143', 'R', 'S10000', 'S26500', 'R', 'S10011', 'S2ff00', 'P', 'S38700', 'M', 'S10000', 'S26500', 'M', 'S15a18', 'S15a18', 'S10e22', 'S10e02', 'P', 'S38800']
2021-11-23 14:34:33,431 - DEBUG - joeynmt.training - 	Raw factor:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a', 'S1f0', 'S265', 'M', 'S10e', 'P', 'S388', 'M', 'S1dc', 'S100', 'S205', 'S205', 'S30a', 'M', 'S203', 'S203', 'S203', 'S203', 'S30e', 'S301', 'M', 'S205', 'S100', 'M', 'S301', 'S100', 'S265', 'M', 'S22f', 'S205', 'S15a', 'S22f', 'S15a', 'S205', 'P', 'S389', 'M', 'S179', 'S179', 'S2c3', 'S2c3', 'S30a', 'P', 'S387', 'M', 'S15a', 'S15a', 'S23d', 'M', 'S2ff', 'S100', 'S100', 'S100', 'S22a', 'M', 'S1f5', 'S2ea', 'S2ea', 'S1f5', 'P', 'S387', 'R', 'S377', 'S203', 'S205', 'S203', 'S205', 'S377', 'S30e', 'S301', 'R', 'S1f5', 'S2ea', 'S2ea', 'S1f5', 'P', 'S387', 'M', 'S203', 'S203', 'S377', 'S22f', 'S30e', 'S301', 'M', 'S1f7', 'S1f7', 'S26a', 'M', 'S181', 'S205', 'S205', 'S181', 'R', 'S100', 'S265', 'R', 'S100', 'S2ff', 'P', 'S387', 'M', 'S100', 'S265', 'M', 'S15a', 'S15a', 'S10e', 'S10e', 'P', 'S388']
2021-11-23 14:34:33,431 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁Verse', '▁2.', '▁"', 'I', '▁have', '▁not', '▁not', '▁not', '▁not', '▁not', '▁not', '▁not', '▁not', '▁not', '▁not', '▁not', '▁not', '▁not', '▁not', '▁not', '▁not', '▁not', '▁not', '▁not', '▁not', '▁not', '▁not', '▁not', '▁not', '▁not', '▁not', '▁not', '▁not', '.']
2021-11-23 14:34:33,431 - INFO - joeynmt.training - 	Source:     <2en> <4us> <sent> M S15a07 S1f010 S26507 M S10e00 P S38800 M S1dc0a S10003 S20500 S20500 S30a00 M S20302 S2030a S20340 S20348 S30e00 S30124 M S20500 S10013 M S30124 S10010 S26500 M S22f10 S20500 S15a0a S22f00 S15a02 S20500 P S38900 M S17911 S17919 S2c300 S2c311 S30a00 P S38700 M S15a06 S15a41 S23d04 M S2ff00 S10050 S10058 S10011 S22a04 M S1f522 S2ea46 S2ea02 S1f50a P S38700 R S3770b S20305 S20500 S20303 S20500 S37713 S30e00 S30124 R S1f522 S2ea46 S2ea02 S1f50a P S38700 M S20356 S20350 S37706 S22f04 S30e00 S30124 M S1f740 S1f748 S26a20 M S1814b S20500 S20500 S18143 R S10000 S26500 R S10011 S2ff00 P S38700 M S10000 S26500 M S15a18 S15a18 S10e22 S10e02 P S38800
2021-11-23 14:34:33,431 - INFO - joeynmt.training - 	Factor:     <2en> <4us> <sent> M S15a S1f0 S265 M S10e P S388 M S1dc S100 S205 S205 S30a M S203 S203 S203 S203 S30e S301 M S205 S100 M S301 S100 S265 M S22f S205 S15a S22f S15a S205 P S389 M S179 S179 S2c3 S2c3 S30a P S387 M S15a S15a S23d M S2ff S100 S100 S100 S22a M S1f5 S2ea S2ea S1f5 P S387 R S377 S203 S205 S203 S205 S377 S30e S301 R S1f5 S2ea S2ea S1f5 P S387 M S203 S203 S377 S22f S30e S301 M S1f7 S1f7 S26a M S181 S205 S205 S181 R S100 S265 R S100 S2ff P S387 M S100 S265 M S15a S15a S10e S10e P S388
2021-11-23 14:34:33,432 - INFO - joeynmt.training - 	Reference:  ▁Verse ▁2. ▁Then ▁make ▁me ▁tr ul y ▁hap p y ▁by ▁ag ree ing ▁whole he art ed ly ▁with ▁each ▁other , ▁lo v ing ▁one ▁another , ▁and ▁work ing ▁together ▁with ▁one ▁m ind ▁and ▁p ur p ose .
2021-11-23 14:34:33,432 - INFO - joeynmt.training - 	Hypothesis: ▁Verse ▁2. ▁" I ▁have ▁not ▁not ▁not ▁not ▁not ▁not ▁not ▁not ▁not ▁not ▁not ▁not ▁not ▁not ▁not ▁not ▁not ▁not ▁not ▁not ▁not ▁not ▁not ▁not ▁not ▁not ▁not ▁not .
2021-11-23 14:34:33,432 - INFO - joeynmt.training - Example #3
2021-11-23 14:34:33,432 - DEBUG - joeynmt.training - 	Raw source:     ['<2pt>', '<4br>', '<dict>', 'M', 'S10e40', 'S11040', 'S26504', 'S2fb04']
2021-11-23 14:34:33,432 - DEBUG - joeynmt.training - 	Raw factor:     ['<2pt>', '<4br>', '<dict>', 'M', 'S10e', 'S110', 'S265', 'S2fb']
2021-11-23 14:34:33,432 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁c', 'ro']
2021-11-23 14:34:33,432 - INFO - joeynmt.training - 	Source:     <2pt> <4br> <dict> M S10e40 S11040 S26504 S2fb04
2021-11-23 14:34:33,432 - INFO - joeynmt.training - 	Factor:     <2pt> <4br> <dict> M S10e S110 S265 S2fb
2021-11-23 14:34:33,432 - INFO - joeynmt.training - 	Reference:  ▁expl or ar
2021-11-23 14:34:33,432 - INFO - joeynmt.training - 	Hypothesis: ▁c ro
2021-11-23 14:34:33,433 - INFO - joeynmt.training - Example #6
2021-11-23 14:34:33,433 - DEBUG - joeynmt.training - 	Raw source:     ['<2en>', '<4us>', '<dict>', 'M', 'S20340', 'S21800']
2021-11-23 14:34:33,433 - DEBUG - joeynmt.training - 	Raw factor:     ['<2en>', '<4us>', '<dict>', 'M', 'S203', 'S218']
2021-11-23 14:34:33,433 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁c', 'ro', 've']
2021-11-23 14:34:33,433 - INFO - joeynmt.training - 	Source:     <2en> <4us> <dict> M S20340 S21800
2021-11-23 14:34:33,433 - INFO - joeynmt.training - 	Factor:     <2en> <4us> <dict> M S203 S218
2021-11-23 14:34:33,433 - INFO - joeynmt.training - 	Reference:  ▁m il k
2021-11-23 14:34:33,433 - INFO - joeynmt.training - 	Hypothesis: ▁c ro ve
2021-11-23 14:34:33,433 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step    13000: bleu:   0.57, loss: 114321.1875, ppl:  29.2416, duration: 282.6126s
2021-11-23 14:34:46,220 - INFO - joeynmt.training - Epoch   4, Step:    13100, Batch Loss:     3.539067, Tokens per Sec:     2239, Lr: 0.000100
2021-11-23 14:34:58,890 - INFO - joeynmt.training - Epoch   4, Step:    13200, Batch Loss:     3.496050, Tokens per Sec:     2230, Lr: 0.000100
2021-11-23 14:35:11,537 - INFO - joeynmt.training - Epoch   4, Step:    13300, Batch Loss:     3.538957, Tokens per Sec:     2268, Lr: 0.000100
2021-11-23 14:35:24,290 - INFO - joeynmt.training - Epoch   4, Step:    13400, Batch Loss:     3.175733, Tokens per Sec:     2223, Lr: 0.000100
2021-11-23 14:35:36,796 - INFO - joeynmt.training - Epoch   4, Step:    13500, Batch Loss:     3.290236, Tokens per Sec:     2245, Lr: 0.000100
2021-11-23 14:35:49,338 - INFO - joeynmt.training - Epoch   4, Step:    13600, Batch Loss:     3.170688, Tokens per Sec:     2253, Lr: 0.000100
2021-11-23 14:36:02,050 - INFO - joeynmt.training - Epoch   4, Step:    13700, Batch Loss:     3.309205, Tokens per Sec:     2232, Lr: 0.000100
2021-11-23 14:36:14,781 - INFO - joeynmt.training - Epoch   4, Step:    13800, Batch Loss:     3.438951, Tokens per Sec:     2326, Lr: 0.000100
2021-11-23 14:36:27,422 - INFO - joeynmt.training - Epoch   4, Step:    13900, Batch Loss:     3.289913, Tokens per Sec:     2219, Lr: 0.000100
2021-11-23 14:36:40,071 - INFO - joeynmt.training - Epoch   4, Step:    14000, Batch Loss:     3.476772, Tokens per Sec:     2218, Lr: 0.000100
2021-11-23 14:39:29,253 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-11-23 14:39:29,253 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-11-23 14:39:29,253 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-11-23 14:39:29,309 - INFO - joeynmt.training - Example #0
2021-11-23 14:39:29,310 - DEBUG - joeynmt.training - 	Raw source:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a07', 'S1f010', 'S26507', 'M', 'S11e20', 'S1a520', 'P', 'S38800', 'M', 'S15a56', 'S15a41', 'S2b700', 'S37c06', 'S36d01', 'S30d00', 'M', 'S10040', 'P', 'S38700', 'M', 'S37806', 'S10050', 'S20356', 'S36d01', 'M', 'S15a37', 'S1ce51', 'S26a07', 'P', 'S38700', 'M', 'S19220', 'S2a20c', 'S11520', 'S10120', 'S1f720', 'S20320', 'S36d01', 'M', 'S10018', 'S26505', 'S10641', 'M', 'S1f000', 'S1f720', 'S1dc20', 'S19220', 'S14a20', 'S1dc20', 'S14a20', 'P', 'S38800', 'M', 'S10041', 'S30d00', 'S36d01', 'M', 'S15a31', 'S10018', 'S26503', 'M', 'S2e74c', 'S14220', 'S2e700', 'S14228', 'M', 'S26521', 'S1f548', 'S1f540', 'M', 'S10041', 'P', 'S38700', 'M', 'S10021', 'S10029', 'S22a07', 'S22a11', 'S36d01', 'M', 'S2ea3e', 'S10000', 'M', 'S15a18', 'S10051', 'S26601', 'M', 'S10058', 'S26526', 'S10050', 'S20500', 'S20500', 'P', 'S38900', 'M', 'S15a37', 'S15a01', 'S2e900', 'S36d01', 'M', 'S15a20', 'S26501', 'M', 'S26521', 'S23b0a', 'S1f548', 'S15a40', 'S15a48', 'S23b1a', 'S1f540', 'M', 'S14c57', 'S14c5f', 'S26511', 'S26507', 'S22527', 'S20314', 'S22521', 'S2031c', 'P', 'S38800']
2021-11-23 14:39:29,310 - DEBUG - joeynmt.training - 	Raw factor:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a', 'S1f0', 'S265', 'M', 'S11e', 'S1a5', 'P', 'S388', 'M', 'S15a', 'S15a', 'S2b7', 'S37c', 'S36d', 'S30d', 'M', 'S100', 'P', 'S387', 'M', 'S378', 'S100', 'S203', 'S36d', 'M', 'S15a', 'S1ce', 'S26a', 'P', 'S387', 'M', 'S192', 'S2a2', 'S115', 'S101', 'S1f7', 'S203', 'S36d', 'M', 'S100', 'S265', 'S106', 'M', 'S1f0', 'S1f7', 'S1dc', 'S192', 'S14a', 'S1dc', 'S14a', 'P', 'S388', 'M', 'S100', 'S30d', 'S36d', 'M', 'S15a', 'S100', 'S265', 'M', 'S2e7', 'S142', 'S2e7', 'S142', 'M', 'S265', 'S1f5', 'S1f5', 'M', 'S100', 'P', 'S387', 'M', 'S100', 'S100', 'S22a', 'S22a', 'S36d', 'M', 'S2ea', 'S100', 'M', 'S15a', 'S100', 'S266', 'M', 'S100', 'S265', 'S100', 'S205', 'S205', 'P', 'S389', 'M', 'S15a', 'S15a', 'S2e9', 'S36d', 'M', 'S15a', 'S265', 'M', 'S265', 'S23b', 'S1f5', 'S15a', 'S15a', 'S23b', 'S1f5', 'M', 'S14c', 'S14c', 'S265', 'S265', 'S225', 'S203', 'S225', 'S203', 'P', 'S388']
2021-11-23 14:39:29,310 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁Verse', '▁3', '1.', '▁"', 'W', 'e', 'e', 'e', 'ver', 'ver', '▁the', '▁king', "'", 's', '▁son', '▁of', '▁the', '▁king', "'", 's', '▁son', '▁of', '▁the', '▁king', "'", 's', '▁son', '▁of', '▁the', '▁king', "'", 's', '▁son', '▁of', '▁the', '▁king', "'", 's', '▁people', '▁of', '▁the', '▁king', "'", 's', '▁people', '.']
2021-11-23 14:39:29,310 - INFO - joeynmt.training - 	Source:     <2en> <4us> <sent> M S15a07 S1f010 S26507 M S11e20 S1a520 P S38800 M S15a56 S15a41 S2b700 S37c06 S36d01 S30d00 M S10040 P S38700 M S37806 S10050 S20356 S36d01 M S15a37 S1ce51 S26a07 P S38700 M S19220 S2a20c S11520 S10120 S1f720 S20320 S36d01 M S10018 S26505 S10641 M S1f000 S1f720 S1dc20 S19220 S14a20 S1dc20 S14a20 P S38800 M S10041 S30d00 S36d01 M S15a31 S10018 S26503 M S2e74c S14220 S2e700 S14228 M S26521 S1f548 S1f540 M S10041 P S38700 M S10021 S10029 S22a07 S22a11 S36d01 M S2ea3e S10000 M S15a18 S10051 S26601 M S10058 S26526 S10050 S20500 S20500 P S38900 M S15a37 S15a01 S2e900 S36d01 M S15a20 S26501 M S26521 S23b0a S1f548 S15a40 S15a48 S23b1a S1f540 M S14c57 S14c5f S26511 S26507 S22527 S20314 S22521 S2031c P S38800
2021-11-23 14:39:29,310 - INFO - joeynmt.training - 	Factor:     <2en> <4us> <sent> M S15a S1f0 S265 M S11e S1a5 P S388 M S15a S15a S2b7 S37c S36d S30d M S100 P S387 M S378 S100 S203 S36d M S15a S1ce S26a P S387 M S192 S2a2 S115 S101 S1f7 S203 S36d M S100 S265 S106 M S1f0 S1f7 S1dc S192 S14a S1dc S14a P S388 M S100 S30d S36d M S15a S100 S265 M S2e7 S142 S2e7 S142 M S265 S1f5 S1f5 M S100 P S387 M S100 S100 S22a S22a S36d M S2ea S100 M S15a S100 S266 M S100 S265 S100 S205 S205 P S389 M S15a S15a S2e9 S36d M S15a S265 M S265 S23b S1f5 S15a S15a S23b S1f5 M S14c S14c S265 S265 S225 S203 S225 S203 P S388
2021-11-23 14:39:29,310 - INFO - joeynmt.training - 	Reference:  ▁Verse ▁3 7. ▁After ▁him , ▁at ▁the ▁time ▁of ▁the ▁c ens us , ▁there ▁was ▁Jud as ▁of ▁Gal ile e . ▁He ▁g ot ▁people ▁to ▁follow ▁him , ▁but ▁he ▁was ▁k illed , ▁too , ▁and ▁all ▁his ▁follow ers ▁were ▁sc atter ed .
2021-11-23 14:39:29,310 - INFO - joeynmt.training - 	Hypothesis: ▁Verse ▁3 1. ▁" W e e e ver ver ▁the ▁king ' s ▁son ▁of ▁the ▁king ' s ▁son ▁of ▁the ▁king ' s ▁son ▁of ▁the ▁king ' s ▁son ▁of ▁the ▁king ' s ▁people ▁of ▁the ▁king ' s ▁people .
2021-11-23 14:39:29,311 - INFO - joeynmt.training - Example #1
2021-11-23 14:39:29,311 - DEBUG - joeynmt.training - 	Raw source:     ['<2pt>', '<4br>', '<dict>', 'M', 'S34700', 'S21100', 'S1f410']
2021-11-23 14:39:29,311 - DEBUG - joeynmt.training - 	Raw factor:     ['<2pt>', '<4br>', '<dict>', 'M', 'S347', 'S211', 'S1f4']
2021-11-23 14:39:29,311 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁B', 'A']
2021-11-23 14:39:29,311 - INFO - joeynmt.training - 	Source:     <2pt> <4br> <dict> M S34700 S21100 S1f410
2021-11-23 14:39:29,311 - INFO - joeynmt.training - 	Factor:     <2pt> <4br> <dict> M S347 S211 S1f4
2021-11-23 14:39:29,311 - INFO - joeynmt.training - 	Reference:  ▁L á p is
2021-11-23 14:39:29,311 - INFO - joeynmt.training - 	Hypothesis: ▁B A
2021-11-23 14:39:29,311 - INFO - joeynmt.training - Example #2
2021-11-23 14:39:29,311 - DEBUG - joeynmt.training - 	Raw source:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a07', 'S1f010', 'S26507', 'M', 'S10e00', 'P', 'S38800', 'M', 'S1dc0a', 'S10003', 'S20500', 'S20500', 'S30a00', 'M', 'S20302', 'S2030a', 'S20340', 'S20348', 'S30e00', 'S30124', 'M', 'S20500', 'S10013', 'M', 'S30124', 'S10010', 'S26500', 'M', 'S22f10', 'S20500', 'S15a0a', 'S22f00', 'S15a02', 'S20500', 'P', 'S38900', 'M', 'S17911', 'S17919', 'S2c300', 'S2c311', 'S30a00', 'P', 'S38700', 'M', 'S15a06', 'S15a41', 'S23d04', 'M', 'S2ff00', 'S10050', 'S10058', 'S10011', 'S22a04', 'M', 'S1f522', 'S2ea46', 'S2ea02', 'S1f50a', 'P', 'S38700', 'R', 'S3770b', 'S20305', 'S20500', 'S20303', 'S20500', 'S37713', 'S30e00', 'S30124', 'R', 'S1f522', 'S2ea46', 'S2ea02', 'S1f50a', 'P', 'S38700', 'M', 'S20356', 'S20350', 'S37706', 'S22f04', 'S30e00', 'S30124', 'M', 'S1f740', 'S1f748', 'S26a20', 'M', 'S1814b', 'S20500', 'S20500', 'S18143', 'R', 'S10000', 'S26500', 'R', 'S10011', 'S2ff00', 'P', 'S38700', 'M', 'S10000', 'S26500', 'M', 'S15a18', 'S15a18', 'S10e22', 'S10e02', 'P', 'S38800']
2021-11-23 14:39:29,312 - DEBUG - joeynmt.training - 	Raw factor:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a', 'S1f0', 'S265', 'M', 'S10e', 'P', 'S388', 'M', 'S1dc', 'S100', 'S205', 'S205', 'S30a', 'M', 'S203', 'S203', 'S203', 'S203', 'S30e', 'S301', 'M', 'S205', 'S100', 'M', 'S301', 'S100', 'S265', 'M', 'S22f', 'S205', 'S15a', 'S22f', 'S15a', 'S205', 'P', 'S389', 'M', 'S179', 'S179', 'S2c3', 'S2c3', 'S30a', 'P', 'S387', 'M', 'S15a', 'S15a', 'S23d', 'M', 'S2ff', 'S100', 'S100', 'S100', 'S22a', 'M', 'S1f5', 'S2ea', 'S2ea', 'S1f5', 'P', 'S387', 'R', 'S377', 'S203', 'S205', 'S203', 'S205', 'S377', 'S30e', 'S301', 'R', 'S1f5', 'S2ea', 'S2ea', 'S1f5', 'P', 'S387', 'M', 'S203', 'S203', 'S377', 'S22f', 'S30e', 'S301', 'M', 'S1f7', 'S1f7', 'S26a', 'M', 'S181', 'S205', 'S205', 'S181', 'R', 'S100', 'S265', 'R', 'S100', 'S2ff', 'P', 'S387', 'M', 'S100', 'S265', 'M', 'S15a', 'S15a', 'S10e', 'S10e', 'P', 'S388']
2021-11-23 14:39:29,312 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁Verse', '▁3', '1.', '▁"', 'I', '▁am', '▁you', '▁to', '▁you', ',', '▁you', '▁are', '▁not', '▁not', '▁not', '▁not', '▁not', '▁not', '▁not', '▁not', '▁not', '▁not', '▁not', '▁not', '▁not', '▁not', '▁not', '▁not', '▁not', '▁not', '▁not', '▁not', '▁not', '▁not', '▁not', '▁not', '▁not', '.']
2021-11-23 14:39:29,312 - INFO - joeynmt.training - 	Source:     <2en> <4us> <sent> M S15a07 S1f010 S26507 M S10e00 P S38800 M S1dc0a S10003 S20500 S20500 S30a00 M S20302 S2030a S20340 S20348 S30e00 S30124 M S20500 S10013 M S30124 S10010 S26500 M S22f10 S20500 S15a0a S22f00 S15a02 S20500 P S38900 M S17911 S17919 S2c300 S2c311 S30a00 P S38700 M S15a06 S15a41 S23d04 M S2ff00 S10050 S10058 S10011 S22a04 M S1f522 S2ea46 S2ea02 S1f50a P S38700 R S3770b S20305 S20500 S20303 S20500 S37713 S30e00 S30124 R S1f522 S2ea46 S2ea02 S1f50a P S38700 M S20356 S20350 S37706 S22f04 S30e00 S30124 M S1f740 S1f748 S26a20 M S1814b S20500 S20500 S18143 R S10000 S26500 R S10011 S2ff00 P S38700 M S10000 S26500 M S15a18 S15a18 S10e22 S10e02 P S38800
2021-11-23 14:39:29,312 - INFO - joeynmt.training - 	Factor:     <2en> <4us> <sent> M S15a S1f0 S265 M S10e P S388 M S1dc S100 S205 S205 S30a M S203 S203 S203 S203 S30e S301 M S205 S100 M S301 S100 S265 M S22f S205 S15a S22f S15a S205 P S389 M S179 S179 S2c3 S2c3 S30a P S387 M S15a S15a S23d M S2ff S100 S100 S100 S22a M S1f5 S2ea S2ea S1f5 P S387 R S377 S203 S205 S203 S205 S377 S30e S301 R S1f5 S2ea S2ea S1f5 P S387 M S203 S203 S377 S22f S30e S301 M S1f7 S1f7 S26a M S181 S205 S205 S181 R S100 S265 R S100 S2ff P S387 M S100 S265 M S15a S15a S10e S10e P S388
2021-11-23 14:39:29,312 - INFO - joeynmt.training - 	Reference:  ▁Verse ▁2. ▁Then ▁make ▁me ▁tr ul y ▁hap p y ▁by ▁ag ree ing ▁whole he art ed ly ▁with ▁each ▁other , ▁lo v ing ▁one ▁another , ▁and ▁work ing ▁together ▁with ▁one ▁m ind ▁and ▁p ur p ose .
2021-11-23 14:39:29,312 - INFO - joeynmt.training - 	Hypothesis: ▁Verse ▁3 1. ▁" I ▁am ▁you ▁to ▁you , ▁you ▁are ▁not ▁not ▁not ▁not ▁not ▁not ▁not ▁not ▁not ▁not ▁not ▁not ▁not ▁not ▁not ▁not ▁not ▁not ▁not ▁not ▁not ▁not ▁not ▁not ▁not .
2021-11-23 14:39:29,312 - INFO - joeynmt.training - Example #3
2021-11-23 14:39:29,312 - DEBUG - joeynmt.training - 	Raw source:     ['<2pt>', '<4br>', '<dict>', 'M', 'S10e40', 'S11040', 'S26504', 'S2fb04']
2021-11-23 14:39:29,312 - DEBUG - joeynmt.training - 	Raw factor:     ['<2pt>', '<4br>', '<dict>', 'M', 'S10e', 'S110', 'S265', 'S2fb']
2021-11-23 14:39:29,312 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁m', 'ão']
2021-11-23 14:39:29,312 - INFO - joeynmt.training - 	Source:     <2pt> <4br> <dict> M S10e40 S11040 S26504 S2fb04
2021-11-23 14:39:29,312 - INFO - joeynmt.training - 	Factor:     <2pt> <4br> <dict> M S10e S110 S265 S2fb
2021-11-23 14:39:29,312 - INFO - joeynmt.training - 	Reference:  ▁expl or ar
2021-11-23 14:39:29,312 - INFO - joeynmt.training - 	Hypothesis: ▁m ão
2021-11-23 14:39:29,313 - INFO - joeynmt.training - Example #6
2021-11-23 14:39:29,313 - DEBUG - joeynmt.training - 	Raw source:     ['<2en>', '<4us>', '<dict>', 'M', 'S20340', 'S21800']
2021-11-23 14:39:29,313 - DEBUG - joeynmt.training - 	Raw factor:     ['<2en>', '<4us>', '<dict>', 'M', 'S203', 'S218']
2021-11-23 14:39:29,313 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁s', 'ide']
2021-11-23 14:39:29,313 - INFO - joeynmt.training - 	Source:     <2en> <4us> <dict> M S20340 S21800
2021-11-23 14:39:29,313 - INFO - joeynmt.training - 	Factor:     <2en> <4us> <dict> M S203 S218
2021-11-23 14:39:29,313 - INFO - joeynmt.training - 	Reference:  ▁m il k
2021-11-23 14:39:29,313 - INFO - joeynmt.training - 	Hypothesis: ▁s ide
2021-11-23 14:39:29,313 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step    14000: bleu:   1.09, loss: 112782.6719, ppl:  27.9429, duration: 169.2418s
2021-11-23 14:39:41,784 - INFO - joeynmt.training - Epoch   4, Step:    14100, Batch Loss:     3.472585, Tokens per Sec:     2358, Lr: 0.000100
2021-11-23 14:39:54,389 - INFO - joeynmt.training - Epoch   4, Step:    14200, Batch Loss:     3.464795, Tokens per Sec:     2270, Lr: 0.000100
2021-11-23 14:40:06,927 - INFO - joeynmt.training - Epoch   4, Step:    14300, Batch Loss:     3.294390, Tokens per Sec:     2296, Lr: 0.000100
2021-11-23 14:40:19,413 - INFO - joeynmt.training - Epoch   4, Step:    14400, Batch Loss:     3.307575, Tokens per Sec:     2260, Lr: 0.000100
2021-11-23 14:40:24,297 - INFO - joeynmt.training - Epoch   4: total training loss 12292.67
2021-11-23 14:40:24,298 - INFO - joeynmt.training - EPOCH 5
2021-11-23 14:40:32,299 - INFO - joeynmt.training - Epoch   5, Step:    14500, Batch Loss:     3.173238, Tokens per Sec:     2207, Lr: 0.000100
2021-11-23 14:40:45,003 - INFO - joeynmt.training - Epoch   5, Step:    14600, Batch Loss:     3.232064, Tokens per Sec:     2277, Lr: 0.000100
2021-11-23 14:40:57,793 - INFO - joeynmt.training - Epoch   5, Step:    14700, Batch Loss:     3.322574, Tokens per Sec:     2313, Lr: 0.000100
2021-11-23 14:41:10,588 - INFO - joeynmt.training - Epoch   5, Step:    14800, Batch Loss:     3.397048, Tokens per Sec:     2244, Lr: 0.000100
2021-11-23 14:41:23,424 - INFO - joeynmt.training - Epoch   5, Step:    14900, Batch Loss:     3.223557, Tokens per Sec:     2248, Lr: 0.000100
2021-11-23 14:41:36,370 - INFO - joeynmt.training - Epoch   5, Step:    15000, Batch Loss:     3.190242, Tokens per Sec:     2285, Lr: 0.000100
2021-11-23 14:45:22,013 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-11-23 14:45:22,013 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-11-23 14:45:22,013 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-11-23 14:45:22,050 - INFO - joeynmt.training - Example #0
2021-11-23 14:45:22,050 - DEBUG - joeynmt.training - 	Raw source:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a07', 'S1f010', 'S26507', 'M', 'S11e20', 'S1a520', 'P', 'S38800', 'M', 'S15a56', 'S15a41', 'S2b700', 'S37c06', 'S36d01', 'S30d00', 'M', 'S10040', 'P', 'S38700', 'M', 'S37806', 'S10050', 'S20356', 'S36d01', 'M', 'S15a37', 'S1ce51', 'S26a07', 'P', 'S38700', 'M', 'S19220', 'S2a20c', 'S11520', 'S10120', 'S1f720', 'S20320', 'S36d01', 'M', 'S10018', 'S26505', 'S10641', 'M', 'S1f000', 'S1f720', 'S1dc20', 'S19220', 'S14a20', 'S1dc20', 'S14a20', 'P', 'S38800', 'M', 'S10041', 'S30d00', 'S36d01', 'M', 'S15a31', 'S10018', 'S26503', 'M', 'S2e74c', 'S14220', 'S2e700', 'S14228', 'M', 'S26521', 'S1f548', 'S1f540', 'M', 'S10041', 'P', 'S38700', 'M', 'S10021', 'S10029', 'S22a07', 'S22a11', 'S36d01', 'M', 'S2ea3e', 'S10000', 'M', 'S15a18', 'S10051', 'S26601', 'M', 'S10058', 'S26526', 'S10050', 'S20500', 'S20500', 'P', 'S38900', 'M', 'S15a37', 'S15a01', 'S2e900', 'S36d01', 'M', 'S15a20', 'S26501', 'M', 'S26521', 'S23b0a', 'S1f548', 'S15a40', 'S15a48', 'S23b1a', 'S1f540', 'M', 'S14c57', 'S14c5f', 'S26511', 'S26507', 'S22527', 'S20314', 'S22521', 'S2031c', 'P', 'S38800']
2021-11-23 14:45:22,050 - DEBUG - joeynmt.training - 	Raw factor:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a', 'S1f0', 'S265', 'M', 'S11e', 'S1a5', 'P', 'S388', 'M', 'S15a', 'S15a', 'S2b7', 'S37c', 'S36d', 'S30d', 'M', 'S100', 'P', 'S387', 'M', 'S378', 'S100', 'S203', 'S36d', 'M', 'S15a', 'S1ce', 'S26a', 'P', 'S387', 'M', 'S192', 'S2a2', 'S115', 'S101', 'S1f7', 'S203', 'S36d', 'M', 'S100', 'S265', 'S106', 'M', 'S1f0', 'S1f7', 'S1dc', 'S192', 'S14a', 'S1dc', 'S14a', 'P', 'S388', 'M', 'S100', 'S30d', 'S36d', 'M', 'S15a', 'S100', 'S265', 'M', 'S2e7', 'S142', 'S2e7', 'S142', 'M', 'S265', 'S1f5', 'S1f5', 'M', 'S100', 'P', 'S387', 'M', 'S100', 'S100', 'S22a', 'S22a', 'S36d', 'M', 'S2ea', 'S100', 'M', 'S15a', 'S100', 'S266', 'M', 'S100', 'S265', 'S100', 'S205', 'S205', 'P', 'S389', 'M', 'S15a', 'S15a', 'S2e9', 'S36d', 'M', 'S15a', 'S265', 'M', 'S265', 'S23b', 'S1f5', 'S15a', 'S15a', 'S23b', 'S1f5', 'M', 'S14c', 'S14c', 'S265', 'S265', 'S225', 'S203', 'S225', 'S203', 'P', 'S388']
2021-11-23 14:45:22,050 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁Verse', '▁3', '0.', '▁Then', '▁Jesus', '▁said', ',', '▁"', 'W', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'ver', ',', '▁"', 'The', '▁LORD', '."']
2021-11-23 14:45:22,051 - INFO - joeynmt.training - 	Source:     <2en> <4us> <sent> M S15a07 S1f010 S26507 M S11e20 S1a520 P S38800 M S15a56 S15a41 S2b700 S37c06 S36d01 S30d00 M S10040 P S38700 M S37806 S10050 S20356 S36d01 M S15a37 S1ce51 S26a07 P S38700 M S19220 S2a20c S11520 S10120 S1f720 S20320 S36d01 M S10018 S26505 S10641 M S1f000 S1f720 S1dc20 S19220 S14a20 S1dc20 S14a20 P S38800 M S10041 S30d00 S36d01 M S15a31 S10018 S26503 M S2e74c S14220 S2e700 S14228 M S26521 S1f548 S1f540 M S10041 P S38700 M S10021 S10029 S22a07 S22a11 S36d01 M S2ea3e S10000 M S15a18 S10051 S26601 M S10058 S26526 S10050 S20500 S20500 P S38900 M S15a37 S15a01 S2e900 S36d01 M S15a20 S26501 M S26521 S23b0a S1f548 S15a40 S15a48 S23b1a S1f540 M S14c57 S14c5f S26511 S26507 S22527 S20314 S22521 S2031c P S38800
2021-11-23 14:45:22,051 - INFO - joeynmt.training - 	Factor:     <2en> <4us> <sent> M S15a S1f0 S265 M S11e S1a5 P S388 M S15a S15a S2b7 S37c S36d S30d M S100 P S387 M S378 S100 S203 S36d M S15a S1ce S26a P S387 M S192 S2a2 S115 S101 S1f7 S203 S36d M S100 S265 S106 M S1f0 S1f7 S1dc S192 S14a S1dc S14a P S388 M S100 S30d S36d M S15a S100 S265 M S2e7 S142 S2e7 S142 M S265 S1f5 S1f5 M S100 P S387 M S100 S100 S22a S22a S36d M S2ea S100 M S15a S100 S266 M S100 S265 S100 S205 S205 P S389 M S15a S15a S2e9 S36d M S15a S265 M S265 S23b S1f5 S15a S15a S23b S1f5 M S14c S14c S265 S265 S225 S203 S225 S203 P S388
2021-11-23 14:45:22,051 - INFO - joeynmt.training - 	Reference:  ▁Verse ▁3 7. ▁After ▁him , ▁at ▁the ▁time ▁of ▁the ▁c ens us , ▁there ▁was ▁Jud as ▁of ▁Gal ile e . ▁He ▁g ot ▁people ▁to ▁follow ▁him , ▁but ▁he ▁was ▁k illed , ▁too , ▁and ▁all ▁his ▁follow ers ▁were ▁sc atter ed .
2021-11-23 14:45:22,051 - INFO - joeynmt.training - 	Hypothesis: ▁Verse ▁3 0. ▁Then ▁Jesus ▁said , ▁" W e e e e e e e e e e e e e e e e e e e e e e ver , ▁" The ▁LORD ."
2021-11-23 14:45:22,051 - INFO - joeynmt.training - Example #1
2021-11-23 14:45:22,051 - DEBUG - joeynmt.training - 	Raw source:     ['<2pt>', '<4br>', '<dict>', 'M', 'S34700', 'S21100', 'S1f410']
2021-11-23 14:45:22,051 - DEBUG - joeynmt.training - 	Raw factor:     ['<2pt>', '<4br>', '<dict>', 'M', 'S347', 'S211', 'S1f4']
2021-11-23 14:45:22,051 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁S', 'A']
2021-11-23 14:45:22,051 - INFO - joeynmt.training - 	Source:     <2pt> <4br> <dict> M S34700 S21100 S1f410
2021-11-23 14:45:22,051 - INFO - joeynmt.training - 	Factor:     <2pt> <4br> <dict> M S347 S211 S1f4
2021-11-23 14:45:22,051 - INFO - joeynmt.training - 	Reference:  ▁L á p is
2021-11-23 14:45:22,051 - INFO - joeynmt.training - 	Hypothesis: ▁S A
2021-11-23 14:45:22,051 - INFO - joeynmt.training - Example #2
2021-11-23 14:45:22,052 - DEBUG - joeynmt.training - 	Raw source:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a07', 'S1f010', 'S26507', 'M', 'S10e00', 'P', 'S38800', 'M', 'S1dc0a', 'S10003', 'S20500', 'S20500', 'S30a00', 'M', 'S20302', 'S2030a', 'S20340', 'S20348', 'S30e00', 'S30124', 'M', 'S20500', 'S10013', 'M', 'S30124', 'S10010', 'S26500', 'M', 'S22f10', 'S20500', 'S15a0a', 'S22f00', 'S15a02', 'S20500', 'P', 'S38900', 'M', 'S17911', 'S17919', 'S2c300', 'S2c311', 'S30a00', 'P', 'S38700', 'M', 'S15a06', 'S15a41', 'S23d04', 'M', 'S2ff00', 'S10050', 'S10058', 'S10011', 'S22a04', 'M', 'S1f522', 'S2ea46', 'S2ea02', 'S1f50a', 'P', 'S38700', 'R', 'S3770b', 'S20305', 'S20500', 'S20303', 'S20500', 'S37713', 'S30e00', 'S30124', 'R', 'S1f522', 'S2ea46', 'S2ea02', 'S1f50a', 'P', 'S38700', 'M', 'S20356', 'S20350', 'S37706', 'S22f04', 'S30e00', 'S30124', 'M', 'S1f740', 'S1f748', 'S26a20', 'M', 'S1814b', 'S20500', 'S20500', 'S18143', 'R', 'S10000', 'S26500', 'R', 'S10011', 'S2ff00', 'P', 'S38700', 'M', 'S10000', 'S26500', 'M', 'S15a18', 'S15a18', 'S10e22', 'S10e02', 'P', 'S38800']
2021-11-23 14:45:22,052 - DEBUG - joeynmt.training - 	Raw factor:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a', 'S1f0', 'S265', 'M', 'S10e', 'P', 'S388', 'M', 'S1dc', 'S100', 'S205', 'S205', 'S30a', 'M', 'S203', 'S203', 'S203', 'S203', 'S30e', 'S301', 'M', 'S205', 'S100', 'M', 'S301', 'S100', 'S265', 'M', 'S22f', 'S205', 'S15a', 'S22f', 'S15a', 'S205', 'P', 'S389', 'M', 'S179', 'S179', 'S2c3', 'S2c3', 'S30a', 'P', 'S387', 'M', 'S15a', 'S15a', 'S23d', 'M', 'S2ff', 'S100', 'S100', 'S100', 'S22a', 'M', 'S1f5', 'S2ea', 'S2ea', 'S1f5', 'P', 'S387', 'R', 'S377', 'S203', 'S205', 'S203', 'S205', 'S377', 'S30e', 'S301', 'R', 'S1f5', 'S2ea', 'S2ea', 'S1f5', 'P', 'S387', 'M', 'S203', 'S203', 'S377', 'S22f', 'S30e', 'S301', 'M', 'S1f7', 'S1f7', 'S26a', 'M', 'S181', 'S205', 'S205', 'S181', 'R', 'S100', 'S265', 'R', 'S100', 'S2ff', 'P', 'S387', 'M', 'S100', 'S265', 'M', 'S15a', 'S15a', 'S10e', 'S10e', 'P', 'S388']
2021-11-23 14:45:22,052 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁Verse', '▁3', '0.', '▁"', 'I', '▁am', '▁you', '▁to', '▁you', '▁to', '▁you', ',', '▁you', '▁are', '▁you', '.', '▁But', '▁you', '▁are', '▁you', '▁are', '▁not', '▁not', '▁not', '▁to', '▁you', '.']
2021-11-23 14:45:22,052 - INFO - joeynmt.training - 	Source:     <2en> <4us> <sent> M S15a07 S1f010 S26507 M S10e00 P S38800 M S1dc0a S10003 S20500 S20500 S30a00 M S20302 S2030a S20340 S20348 S30e00 S30124 M S20500 S10013 M S30124 S10010 S26500 M S22f10 S20500 S15a0a S22f00 S15a02 S20500 P S38900 M S17911 S17919 S2c300 S2c311 S30a00 P S38700 M S15a06 S15a41 S23d04 M S2ff00 S10050 S10058 S10011 S22a04 M S1f522 S2ea46 S2ea02 S1f50a P S38700 R S3770b S20305 S20500 S20303 S20500 S37713 S30e00 S30124 R S1f522 S2ea46 S2ea02 S1f50a P S38700 M S20356 S20350 S37706 S22f04 S30e00 S30124 M S1f740 S1f748 S26a20 M S1814b S20500 S20500 S18143 R S10000 S26500 R S10011 S2ff00 P S38700 M S10000 S26500 M S15a18 S15a18 S10e22 S10e02 P S38800
2021-11-23 14:45:22,052 - INFO - joeynmt.training - 	Factor:     <2en> <4us> <sent> M S15a S1f0 S265 M S10e P S388 M S1dc S100 S205 S205 S30a M S203 S203 S203 S203 S30e S301 M S205 S100 M S301 S100 S265 M S22f S205 S15a S22f S15a S205 P S389 M S179 S179 S2c3 S2c3 S30a P S387 M S15a S15a S23d M S2ff S100 S100 S100 S22a M S1f5 S2ea S2ea S1f5 P S387 R S377 S203 S205 S203 S205 S377 S30e S301 R S1f5 S2ea S2ea S1f5 P S387 M S203 S203 S377 S22f S30e S301 M S1f7 S1f7 S26a M S181 S205 S205 S181 R S100 S265 R S100 S2ff P S387 M S100 S265 M S15a S15a S10e S10e P S388
2021-11-23 14:45:22,052 - INFO - joeynmt.training - 	Reference:  ▁Verse ▁2. ▁Then ▁make ▁me ▁tr ul y ▁hap p y ▁by ▁ag ree ing ▁whole he art ed ly ▁with ▁each ▁other , ▁lo v ing ▁one ▁another , ▁and ▁work ing ▁together ▁with ▁one ▁m ind ▁and ▁p ur p ose .
2021-11-23 14:45:22,052 - INFO - joeynmt.training - 	Hypothesis: ▁Verse ▁3 0. ▁" I ▁am ▁you ▁to ▁you ▁to ▁you , ▁you ▁are ▁you . ▁But ▁you ▁are ▁you ▁are ▁not ▁not ▁not ▁to ▁you .
2021-11-23 14:45:22,052 - INFO - joeynmt.training - Example #3
2021-11-23 14:45:22,052 - DEBUG - joeynmt.training - 	Raw source:     ['<2pt>', '<4br>', '<dict>', 'M', 'S10e40', 'S11040', 'S26504', 'S2fb04']
2021-11-23 14:45:22,052 - DEBUG - joeynmt.training - 	Raw factor:     ['<2pt>', '<4br>', '<dict>', 'M', 'S10e', 'S110', 'S265', 'S2fb']
2021-11-23 14:45:22,052 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁c', 'oc', 'ar']
2021-11-23 14:45:22,052 - INFO - joeynmt.training - 	Source:     <2pt> <4br> <dict> M S10e40 S11040 S26504 S2fb04
2021-11-23 14:45:22,052 - INFO - joeynmt.training - 	Factor:     <2pt> <4br> <dict> M S10e S110 S265 S2fb
2021-11-23 14:45:22,052 - INFO - joeynmt.training - 	Reference:  ▁expl or ar
2021-11-23 14:45:22,053 - INFO - joeynmt.training - 	Hypothesis: ▁c oc ar
2021-11-23 14:45:22,053 - INFO - joeynmt.training - Example #6
2021-11-23 14:45:22,053 - DEBUG - joeynmt.training - 	Raw source:     ['<2en>', '<4us>', '<dict>', 'M', 'S20340', 'S21800']
2021-11-23 14:45:22,053 - DEBUG - joeynmt.training - 	Raw factor:     ['<2en>', '<4us>', '<dict>', 'M', 'S203', 'S218']
2021-11-23 14:45:22,053 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁h', 'at']
2021-11-23 14:45:22,053 - INFO - joeynmt.training - 	Source:     <2en> <4us> <dict> M S20340 S21800
2021-11-23 14:45:22,053 - INFO - joeynmt.training - 	Factor:     <2en> <4us> <dict> M S203 S218
2021-11-23 14:45:22,053 - INFO - joeynmt.training - 	Reference:  ▁m il k
2021-11-23 14:45:22,053 - INFO - joeynmt.training - 	Hypothesis: ▁h at
2021-11-23 14:45:22,053 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step    15000: bleu:   1.07, loss: 111990.2109, ppl:  27.2967, duration: 225.6833s
2021-11-23 14:45:34,838 - INFO - joeynmt.training - Epoch   5, Step:    15100, Batch Loss:     3.316766, Tokens per Sec:     2290, Lr: 0.000100
2021-11-23 14:45:47,869 - INFO - joeynmt.training - Epoch   5, Step:    15200, Batch Loss:     3.274800, Tokens per Sec:     2243, Lr: 0.000100
2021-11-23 14:46:00,489 - INFO - joeynmt.training - Epoch   5, Step:    15300, Batch Loss:     3.274520, Tokens per Sec:     2285, Lr: 0.000100
2021-11-23 14:46:13,139 - INFO - joeynmt.training - Epoch   5, Step:    15400, Batch Loss:     3.146896, Tokens per Sec:     2258, Lr: 0.000100
2021-11-23 14:46:25,823 - INFO - joeynmt.training - Epoch   5, Step:    15500, Batch Loss:     3.399563, Tokens per Sec:     2262, Lr: 0.000100
2021-11-23 14:46:38,397 - INFO - joeynmt.training - Epoch   5, Step:    15600, Batch Loss:     3.353383, Tokens per Sec:     2222, Lr: 0.000100
2021-11-23 14:46:51,057 - INFO - joeynmt.training - Epoch   5, Step:    15700, Batch Loss:     3.182179, Tokens per Sec:     2189, Lr: 0.000100
2021-11-23 14:47:03,655 - INFO - joeynmt.training - Epoch   5, Step:    15800, Batch Loss:     3.392185, Tokens per Sec:     2162, Lr: 0.000100
2021-11-23 14:47:16,458 - INFO - joeynmt.training - Epoch   5, Step:    15900, Batch Loss:     3.197652, Tokens per Sec:     2304, Lr: 0.000100
2021-11-23 14:47:29,070 - INFO - joeynmt.training - Epoch   5, Step:    16000, Batch Loss:     3.211882, Tokens per Sec:     2154, Lr: 0.000100
2021-11-23 14:51:10,590 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-11-23 14:51:10,590 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-11-23 14:51:10,590 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-11-23 14:51:10,611 - INFO - joeynmt.training - Example #0
2021-11-23 14:51:10,611 - DEBUG - joeynmt.training - 	Raw source:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a07', 'S1f010', 'S26507', 'M', 'S11e20', 'S1a520', 'P', 'S38800', 'M', 'S15a56', 'S15a41', 'S2b700', 'S37c06', 'S36d01', 'S30d00', 'M', 'S10040', 'P', 'S38700', 'M', 'S37806', 'S10050', 'S20356', 'S36d01', 'M', 'S15a37', 'S1ce51', 'S26a07', 'P', 'S38700', 'M', 'S19220', 'S2a20c', 'S11520', 'S10120', 'S1f720', 'S20320', 'S36d01', 'M', 'S10018', 'S26505', 'S10641', 'M', 'S1f000', 'S1f720', 'S1dc20', 'S19220', 'S14a20', 'S1dc20', 'S14a20', 'P', 'S38800', 'M', 'S10041', 'S30d00', 'S36d01', 'M', 'S15a31', 'S10018', 'S26503', 'M', 'S2e74c', 'S14220', 'S2e700', 'S14228', 'M', 'S26521', 'S1f548', 'S1f540', 'M', 'S10041', 'P', 'S38700', 'M', 'S10021', 'S10029', 'S22a07', 'S22a11', 'S36d01', 'M', 'S2ea3e', 'S10000', 'M', 'S15a18', 'S10051', 'S26601', 'M', 'S10058', 'S26526', 'S10050', 'S20500', 'S20500', 'P', 'S38900', 'M', 'S15a37', 'S15a01', 'S2e900', 'S36d01', 'M', 'S15a20', 'S26501', 'M', 'S26521', 'S23b0a', 'S1f548', 'S15a40', 'S15a48', 'S23b1a', 'S1f540', 'M', 'S14c57', 'S14c5f', 'S26511', 'S26507', 'S22527', 'S20314', 'S22521', 'S2031c', 'P', 'S38800']
2021-11-23 14:51:10,611 - DEBUG - joeynmt.training - 	Raw factor:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a', 'S1f0', 'S265', 'M', 'S11e', 'S1a5', 'P', 'S388', 'M', 'S15a', 'S15a', 'S2b7', 'S37c', 'S36d', 'S30d', 'M', 'S100', 'P', 'S387', 'M', 'S378', 'S100', 'S203', 'S36d', 'M', 'S15a', 'S1ce', 'S26a', 'P', 'S387', 'M', 'S192', 'S2a2', 'S115', 'S101', 'S1f7', 'S203', 'S36d', 'M', 'S100', 'S265', 'S106', 'M', 'S1f0', 'S1f7', 'S1dc', 'S192', 'S14a', 'S1dc', 'S14a', 'P', 'S388', 'M', 'S100', 'S30d', 'S36d', 'M', 'S15a', 'S100', 'S265', 'M', 'S2e7', 'S142', 'S2e7', 'S142', 'M', 'S265', 'S1f5', 'S1f5', 'M', 'S100', 'P', 'S387', 'M', 'S100', 'S100', 'S22a', 'S22a', 'S36d', 'M', 'S2ea', 'S100', 'M', 'S15a', 'S100', 'S266', 'M', 'S100', 'S265', 'S100', 'S205', 'S205', 'P', 'S389', 'M', 'S15a', 'S15a', 'S2e9', 'S36d', 'M', 'S15a', 'S265', 'M', 'S265', 'S23b', 'S1f5', 'S15a', 'S15a', 'S23b', 'S1f5', 'M', 'S14c', 'S14c', 'S265', 'S265', 'S225', 'S203', 'S225', 'S203', 'P', 'S388']
2021-11-23 14:51:10,611 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁Verse', '▁3', '0.', '▁Then', '▁the', '▁LORD', '▁said', ',', '▁"', 'I', '▁am', '▁the', '▁LORD', "'", 's', '▁son', '▁of', '▁the', '▁LORD', "'", 's', '▁son', '▁of', '▁the', '▁LORD', "'", 's', '▁son', '▁of', '▁the', '▁LORD', '."']
2021-11-23 14:51:10,611 - INFO - joeynmt.training - 	Source:     <2en> <4us> <sent> M S15a07 S1f010 S26507 M S11e20 S1a520 P S38800 M S15a56 S15a41 S2b700 S37c06 S36d01 S30d00 M S10040 P S38700 M S37806 S10050 S20356 S36d01 M S15a37 S1ce51 S26a07 P S38700 M S19220 S2a20c S11520 S10120 S1f720 S20320 S36d01 M S10018 S26505 S10641 M S1f000 S1f720 S1dc20 S19220 S14a20 S1dc20 S14a20 P S38800 M S10041 S30d00 S36d01 M S15a31 S10018 S26503 M S2e74c S14220 S2e700 S14228 M S26521 S1f548 S1f540 M S10041 P S38700 M S10021 S10029 S22a07 S22a11 S36d01 M S2ea3e S10000 M S15a18 S10051 S26601 M S10058 S26526 S10050 S20500 S20500 P S38900 M S15a37 S15a01 S2e900 S36d01 M S15a20 S26501 M S26521 S23b0a S1f548 S15a40 S15a48 S23b1a S1f540 M S14c57 S14c5f S26511 S26507 S22527 S20314 S22521 S2031c P S38800
2021-11-23 14:51:10,611 - INFO - joeynmt.training - 	Factor:     <2en> <4us> <sent> M S15a S1f0 S265 M S11e S1a5 P S388 M S15a S15a S2b7 S37c S36d S30d M S100 P S387 M S378 S100 S203 S36d M S15a S1ce S26a P S387 M S192 S2a2 S115 S101 S1f7 S203 S36d M S100 S265 S106 M S1f0 S1f7 S1dc S192 S14a S1dc S14a P S388 M S100 S30d S36d M S15a S100 S265 M S2e7 S142 S2e7 S142 M S265 S1f5 S1f5 M S100 P S387 M S100 S100 S22a S22a S36d M S2ea S100 M S15a S100 S266 M S100 S265 S100 S205 S205 P S389 M S15a S15a S2e9 S36d M S15a S265 M S265 S23b S1f5 S15a S15a S23b S1f5 M S14c S14c S265 S265 S225 S203 S225 S203 P S388
2021-11-23 14:51:10,611 - INFO - joeynmt.training - 	Reference:  ▁Verse ▁3 7. ▁After ▁him , ▁at ▁the ▁time ▁of ▁the ▁c ens us , ▁there ▁was ▁Jud as ▁of ▁Gal ile e . ▁He ▁g ot ▁people ▁to ▁follow ▁him , ▁but ▁he ▁was ▁k illed , ▁too , ▁and ▁all ▁his ▁follow ers ▁were ▁sc atter ed .
2021-11-23 14:51:10,611 - INFO - joeynmt.training - 	Hypothesis: ▁Verse ▁3 0. ▁Then ▁the ▁LORD ▁said , ▁" I ▁am ▁the ▁LORD ' s ▁son ▁of ▁the ▁LORD ' s ▁son ▁of ▁the ▁LORD ' s ▁son ▁of ▁the ▁LORD ."
2021-11-23 14:51:10,612 - INFO - joeynmt.training - Example #1
2021-11-23 14:51:10,612 - DEBUG - joeynmt.training - 	Raw source:     ['<2pt>', '<4br>', '<dict>', 'M', 'S34700', 'S21100', 'S1f410']
2021-11-23 14:51:10,612 - DEBUG - joeynmt.training - 	Raw factor:     ['<2pt>', '<4br>', '<dict>', 'M', 'S347', 'S211', 'S1f4']
2021-11-23 14:51:10,612 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁E', 'N', 'I']
2021-11-23 14:51:10,612 - INFO - joeynmt.training - 	Source:     <2pt> <4br> <dict> M S34700 S21100 S1f410
2021-11-23 14:51:10,612 - INFO - joeynmt.training - 	Factor:     <2pt> <4br> <dict> M S347 S211 S1f4
2021-11-23 14:51:10,612 - INFO - joeynmt.training - 	Reference:  ▁L á p is
2021-11-23 14:51:10,612 - INFO - joeynmt.training - 	Hypothesis: ▁E N I
2021-11-23 14:51:10,612 - INFO - joeynmt.training - Example #2
2021-11-23 14:51:10,612 - DEBUG - joeynmt.training - 	Raw source:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a07', 'S1f010', 'S26507', 'M', 'S10e00', 'P', 'S38800', 'M', 'S1dc0a', 'S10003', 'S20500', 'S20500', 'S30a00', 'M', 'S20302', 'S2030a', 'S20340', 'S20348', 'S30e00', 'S30124', 'M', 'S20500', 'S10013', 'M', 'S30124', 'S10010', 'S26500', 'M', 'S22f10', 'S20500', 'S15a0a', 'S22f00', 'S15a02', 'S20500', 'P', 'S38900', 'M', 'S17911', 'S17919', 'S2c300', 'S2c311', 'S30a00', 'P', 'S38700', 'M', 'S15a06', 'S15a41', 'S23d04', 'M', 'S2ff00', 'S10050', 'S10058', 'S10011', 'S22a04', 'M', 'S1f522', 'S2ea46', 'S2ea02', 'S1f50a', 'P', 'S38700', 'R', 'S3770b', 'S20305', 'S20500', 'S20303', 'S20500', 'S37713', 'S30e00', 'S30124', 'R', 'S1f522', 'S2ea46', 'S2ea02', 'S1f50a', 'P', 'S38700', 'M', 'S20356', 'S20350', 'S37706', 'S22f04', 'S30e00', 'S30124', 'M', 'S1f740', 'S1f748', 'S26a20', 'M', 'S1814b', 'S20500', 'S20500', 'S18143', 'R', 'S10000', 'S26500', 'R', 'S10011', 'S2ff00', 'P', 'S38700', 'M', 'S10000', 'S26500', 'M', 'S15a18', 'S15a18', 'S10e22', 'S10e02', 'P', 'S38800']
2021-11-23 14:51:10,612 - DEBUG - joeynmt.training - 	Raw factor:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a', 'S1f0', 'S265', 'M', 'S10e', 'P', 'S388', 'M', 'S1dc', 'S100', 'S205', 'S205', 'S30a', 'M', 'S203', 'S203', 'S203', 'S203', 'S30e', 'S301', 'M', 'S205', 'S100', 'M', 'S301', 'S100', 'S265', 'M', 'S22f', 'S205', 'S15a', 'S22f', 'S15a', 'S205', 'P', 'S389', 'M', 'S179', 'S179', 'S2c3', 'S2c3', 'S30a', 'P', 'S387', 'M', 'S15a', 'S15a', 'S23d', 'M', 'S2ff', 'S100', 'S100', 'S100', 'S22a', 'M', 'S1f5', 'S2ea', 'S2ea', 'S1f5', 'P', 'S387', 'R', 'S377', 'S203', 'S205', 'S203', 'S205', 'S377', 'S30e', 'S301', 'R', 'S1f5', 'S2ea', 'S2ea', 'S1f5', 'P', 'S387', 'M', 'S203', 'S203', 'S377', 'S22f', 'S30e', 'S301', 'M', 'S1f7', 'S1f7', 'S26a', 'M', 'S181', 'S205', 'S205', 'S181', 'R', 'S100', 'S265', 'R', 'S100', 'S2ff', 'P', 'S387', 'M', 'S100', 'S265', 'M', 'S15a', 'S15a', 'S10e', 'S10e', 'P', 'S388']
2021-11-23 14:51:10,612 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁Verse', '▁3.', '▁"', 'I', '▁am', '▁you', '▁to', '▁you', ',', '▁you', '▁have', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been']
2021-11-23 14:51:10,612 - INFO - joeynmt.training - 	Source:     <2en> <4us> <sent> M S15a07 S1f010 S26507 M S10e00 P S38800 M S1dc0a S10003 S20500 S20500 S30a00 M S20302 S2030a S20340 S20348 S30e00 S30124 M S20500 S10013 M S30124 S10010 S26500 M S22f10 S20500 S15a0a S22f00 S15a02 S20500 P S38900 M S17911 S17919 S2c300 S2c311 S30a00 P S38700 M S15a06 S15a41 S23d04 M S2ff00 S10050 S10058 S10011 S22a04 M S1f522 S2ea46 S2ea02 S1f50a P S38700 R S3770b S20305 S20500 S20303 S20500 S37713 S30e00 S30124 R S1f522 S2ea46 S2ea02 S1f50a P S38700 M S20356 S20350 S37706 S22f04 S30e00 S30124 M S1f740 S1f748 S26a20 M S1814b S20500 S20500 S18143 R S10000 S26500 R S10011 S2ff00 P S38700 M S10000 S26500 M S15a18 S15a18 S10e22 S10e02 P S38800
2021-11-23 14:51:10,612 - INFO - joeynmt.training - 	Factor:     <2en> <4us> <sent> M S15a S1f0 S265 M S10e P S388 M S1dc S100 S205 S205 S30a M S203 S203 S203 S203 S30e S301 M S205 S100 M S301 S100 S265 M S22f S205 S15a S22f S15a S205 P S389 M S179 S179 S2c3 S2c3 S30a P S387 M S15a S15a S23d M S2ff S100 S100 S100 S22a M S1f5 S2ea S2ea S1f5 P S387 R S377 S203 S205 S203 S205 S377 S30e S301 R S1f5 S2ea S2ea S1f5 P S387 M S203 S203 S377 S22f S30e S301 M S1f7 S1f7 S26a M S181 S205 S205 S181 R S100 S265 R S100 S2ff P S387 M S100 S265 M S15a S15a S10e S10e P S388
2021-11-23 14:51:10,612 - INFO - joeynmt.training - 	Reference:  ▁Verse ▁2. ▁Then ▁make ▁me ▁tr ul y ▁hap p y ▁by ▁ag ree ing ▁whole he art ed ly ▁with ▁each ▁other , ▁lo v ing ▁one ▁another , ▁and ▁work ing ▁together ▁with ▁one ▁m ind ▁and ▁p ur p ose .
2021-11-23 14:51:10,612 - INFO - joeynmt.training - 	Hypothesis: ▁Verse ▁3. ▁" I ▁am ▁you ▁to ▁you , ▁you ▁have ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been
2021-11-23 14:51:10,612 - INFO - joeynmt.training - Example #3
2021-11-23 14:51:10,612 - DEBUG - joeynmt.training - 	Raw source:     ['<2pt>', '<4br>', '<dict>', 'M', 'S10e40', 'S11040', 'S26504', 'S2fb04']
2021-11-23 14:51:10,612 - DEBUG - joeynmt.training - 	Raw factor:     ['<2pt>', '<4br>', '<dict>', 'M', 'S10e', 'S110', 'S265', 'S2fb']
2021-11-23 14:51:10,612 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁c', 'er', 'ar']
2021-11-23 14:51:10,612 - INFO - joeynmt.training - 	Source:     <2pt> <4br> <dict> M S10e40 S11040 S26504 S2fb04
2021-11-23 14:51:10,612 - INFO - joeynmt.training - 	Factor:     <2pt> <4br> <dict> M S10e S110 S265 S2fb
2021-11-23 14:51:10,612 - INFO - joeynmt.training - 	Reference:  ▁expl or ar
2021-11-23 14:51:10,613 - INFO - joeynmt.training - 	Hypothesis: ▁c er ar
2021-11-23 14:51:10,613 - INFO - joeynmt.training - Example #6
2021-11-23 14:51:10,613 - DEBUG - joeynmt.training - 	Raw source:     ['<2en>', '<4us>', '<dict>', 'M', 'S20340', 'S21800']
2021-11-23 14:51:10,613 - DEBUG - joeynmt.training - 	Raw factor:     ['<2en>', '<4us>', '<dict>', 'M', 'S203', 'S218']
2021-11-23 14:51:10,613 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁d', 'in']
2021-11-23 14:51:10,613 - INFO - joeynmt.training - 	Source:     <2en> <4us> <dict> M S20340 S21800
2021-11-23 14:51:10,613 - INFO - joeynmt.training - 	Factor:     <2en> <4us> <dict> M S203 S218
2021-11-23 14:51:10,613 - INFO - joeynmt.training - 	Reference:  ▁m il k
2021-11-23 14:51:10,613 - INFO - joeynmt.training - 	Hypothesis: ▁d in
2021-11-23 14:51:10,613 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step    16000: bleu:   0.87, loss: 110792.8359, ppl:  26.3485, duration: 221.5424s
2021-11-23 14:51:23,544 - INFO - joeynmt.training - Epoch   5, Step:    16100, Batch Loss:     3.447322, Tokens per Sec:     2248, Lr: 0.000100
2021-11-23 14:51:36,369 - INFO - joeynmt.training - Epoch   5, Step:    16200, Batch Loss:     3.319811, Tokens per Sec:     2271, Lr: 0.000100
2021-11-23 14:51:48,946 - INFO - joeynmt.training - Epoch   5, Step:    16300, Batch Loss:     3.339436, Tokens per Sec:     2328, Lr: 0.000100
2021-11-23 14:52:01,763 - INFO - joeynmt.training - Epoch   5, Step:    16400, Batch Loss:     3.426081, Tokens per Sec:     2301, Lr: 0.000100
2021-11-23 14:52:14,375 - INFO - joeynmt.training - Epoch   5, Step:    16500, Batch Loss:     3.242471, Tokens per Sec:     2189, Lr: 0.000100
2021-11-23 14:52:27,009 - INFO - joeynmt.training - Epoch   5, Step:    16600, Batch Loss:     3.241771, Tokens per Sec:     2268, Lr: 0.000100
2021-11-23 14:52:39,842 - INFO - joeynmt.training - Epoch   5, Step:    16700, Batch Loss:     3.227114, Tokens per Sec:     2298, Lr: 0.000100
2021-11-23 14:52:52,470 - INFO - joeynmt.training - Epoch   5, Step:    16800, Batch Loss:     3.188766, Tokens per Sec:     2189, Lr: 0.000100
2021-11-23 14:53:04,988 - INFO - joeynmt.training - Epoch   5, Step:    16900, Batch Loss:     3.258786, Tokens per Sec:     2210, Lr: 0.000100
2021-11-23 14:53:17,536 - INFO - joeynmt.training - Epoch   5, Step:    17000, Batch Loss:     3.146562, Tokens per Sec:     2226, Lr: 0.000100
2021-11-23 15:00:24,119 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-11-23 15:00:24,119 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-11-23 15:00:24,119 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-11-23 15:00:24,140 - INFO - joeynmt.training - Example #0
2021-11-23 15:00:24,140 - DEBUG - joeynmt.training - 	Raw source:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a07', 'S1f010', 'S26507', 'M', 'S11e20', 'S1a520', 'P', 'S38800', 'M', 'S15a56', 'S15a41', 'S2b700', 'S37c06', 'S36d01', 'S30d00', 'M', 'S10040', 'P', 'S38700', 'M', 'S37806', 'S10050', 'S20356', 'S36d01', 'M', 'S15a37', 'S1ce51', 'S26a07', 'P', 'S38700', 'M', 'S19220', 'S2a20c', 'S11520', 'S10120', 'S1f720', 'S20320', 'S36d01', 'M', 'S10018', 'S26505', 'S10641', 'M', 'S1f000', 'S1f720', 'S1dc20', 'S19220', 'S14a20', 'S1dc20', 'S14a20', 'P', 'S38800', 'M', 'S10041', 'S30d00', 'S36d01', 'M', 'S15a31', 'S10018', 'S26503', 'M', 'S2e74c', 'S14220', 'S2e700', 'S14228', 'M', 'S26521', 'S1f548', 'S1f540', 'M', 'S10041', 'P', 'S38700', 'M', 'S10021', 'S10029', 'S22a07', 'S22a11', 'S36d01', 'M', 'S2ea3e', 'S10000', 'M', 'S15a18', 'S10051', 'S26601', 'M', 'S10058', 'S26526', 'S10050', 'S20500', 'S20500', 'P', 'S38900', 'M', 'S15a37', 'S15a01', 'S2e900', 'S36d01', 'M', 'S15a20', 'S26501', 'M', 'S26521', 'S23b0a', 'S1f548', 'S15a40', 'S15a48', 'S23b1a', 'S1f540', 'M', 'S14c57', 'S14c5f', 'S26511', 'S26507', 'S22527', 'S20314', 'S22521', 'S2031c', 'P', 'S38800']
2021-11-23 15:00:24,140 - DEBUG - joeynmt.training - 	Raw factor:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a', 'S1f0', 'S265', 'M', 'S11e', 'S1a5', 'P', 'S388', 'M', 'S15a', 'S15a', 'S2b7', 'S37c', 'S36d', 'S30d', 'M', 'S100', 'P', 'S387', 'M', 'S378', 'S100', 'S203', 'S36d', 'M', 'S15a', 'S1ce', 'S26a', 'P', 'S387', 'M', 'S192', 'S2a2', 'S115', 'S101', 'S1f7', 'S203', 'S36d', 'M', 'S100', 'S265', 'S106', 'M', 'S1f0', 'S1f7', 'S1dc', 'S192', 'S14a', 'S1dc', 'S14a', 'P', 'S388', 'M', 'S100', 'S30d', 'S36d', 'M', 'S15a', 'S100', 'S265', 'M', 'S2e7', 'S142', 'S2e7', 'S142', 'M', 'S265', 'S1f5', 'S1f5', 'M', 'S100', 'P', 'S387', 'M', 'S100', 'S100', 'S22a', 'S22a', 'S36d', 'M', 'S2ea', 'S100', 'M', 'S15a', 'S100', 'S266', 'M', 'S100', 'S265', 'S100', 'S205', 'S205', 'P', 'S389', 'M', 'S15a', 'S15a', 'S2e9', 'S36d', 'M', 'S15a', 'S265', 'M', 'S265', 'S23b', 'S1f5', 'S15a', 'S15a', 'S23b', 'S1f5', 'M', 'S14c', 'S14c', 'S265', 'S265', 'S225', 'S203', 'S225', 'S203', 'P', 'S388']
2021-11-23 15:00:24,140 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁Verse', '▁3', '1.', '▁"', 'I', '▁have', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been']
2021-11-23 15:00:24,140 - INFO - joeynmt.training - 	Source:     <2en> <4us> <sent> M S15a07 S1f010 S26507 M S11e20 S1a520 P S38800 M S15a56 S15a41 S2b700 S37c06 S36d01 S30d00 M S10040 P S38700 M S37806 S10050 S20356 S36d01 M S15a37 S1ce51 S26a07 P S38700 M S19220 S2a20c S11520 S10120 S1f720 S20320 S36d01 M S10018 S26505 S10641 M S1f000 S1f720 S1dc20 S19220 S14a20 S1dc20 S14a20 P S38800 M S10041 S30d00 S36d01 M S15a31 S10018 S26503 M S2e74c S14220 S2e700 S14228 M S26521 S1f548 S1f540 M S10041 P S38700 M S10021 S10029 S22a07 S22a11 S36d01 M S2ea3e S10000 M S15a18 S10051 S26601 M S10058 S26526 S10050 S20500 S20500 P S38900 M S15a37 S15a01 S2e900 S36d01 M S15a20 S26501 M S26521 S23b0a S1f548 S15a40 S15a48 S23b1a S1f540 M S14c57 S14c5f S26511 S26507 S22527 S20314 S22521 S2031c P S38800
2021-11-23 15:00:24,140 - INFO - joeynmt.training - 	Factor:     <2en> <4us> <sent> M S15a S1f0 S265 M S11e S1a5 P S388 M S15a S15a S2b7 S37c S36d S30d M S100 P S387 M S378 S100 S203 S36d M S15a S1ce S26a P S387 M S192 S2a2 S115 S101 S1f7 S203 S36d M S100 S265 S106 M S1f0 S1f7 S1dc S192 S14a S1dc S14a P S388 M S100 S30d S36d M S15a S100 S265 M S2e7 S142 S2e7 S142 M S265 S1f5 S1f5 M S100 P S387 M S100 S100 S22a S22a S36d M S2ea S100 M S15a S100 S266 M S100 S265 S100 S205 S205 P S389 M S15a S15a S2e9 S36d M S15a S265 M S265 S23b S1f5 S15a S15a S23b S1f5 M S14c S14c S265 S265 S225 S203 S225 S203 P S388
2021-11-23 15:00:24,140 - INFO - joeynmt.training - 	Reference:  ▁Verse ▁3 7. ▁After ▁him , ▁at ▁the ▁time ▁of ▁the ▁c ens us , ▁there ▁was ▁Jud as ▁of ▁Gal ile e . ▁He ▁g ot ▁people ▁to ▁follow ▁him , ▁but ▁he ▁was ▁k illed , ▁too , ▁and ▁all ▁his ▁follow ers ▁were ▁sc atter ed .
2021-11-23 15:00:24,140 - INFO - joeynmt.training - 	Hypothesis: ▁Verse ▁3 1. ▁" I ▁have ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been
2021-11-23 15:00:24,140 - INFO - joeynmt.training - Example #1
2021-11-23 15:00:24,140 - DEBUG - joeynmt.training - 	Raw source:     ['<2pt>', '<4br>', '<dict>', 'M', 'S34700', 'S21100', 'S1f410']
2021-11-23 15:00:24,140 - DEBUG - joeynmt.training - 	Raw factor:     ['<2pt>', '<4br>', '<dict>', 'M', 'S347', 'S211', 'S1f4']
2021-11-23 15:00:24,140 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁S', 'ão']
2021-11-23 15:00:24,140 - INFO - joeynmt.training - 	Source:     <2pt> <4br> <dict> M S34700 S21100 S1f410
2021-11-23 15:00:24,140 - INFO - joeynmt.training - 	Factor:     <2pt> <4br> <dict> M S347 S211 S1f4
2021-11-23 15:00:24,140 - INFO - joeynmt.training - 	Reference:  ▁L á p is
2021-11-23 15:00:24,140 - INFO - joeynmt.training - 	Hypothesis: ▁S ão
2021-11-23 15:00:24,140 - INFO - joeynmt.training - Example #2
2021-11-23 15:00:24,140 - DEBUG - joeynmt.training - 	Raw source:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a07', 'S1f010', 'S26507', 'M', 'S10e00', 'P', 'S38800', 'M', 'S1dc0a', 'S10003', 'S20500', 'S20500', 'S30a00', 'M', 'S20302', 'S2030a', 'S20340', 'S20348', 'S30e00', 'S30124', 'M', 'S20500', 'S10013', 'M', 'S30124', 'S10010', 'S26500', 'M', 'S22f10', 'S20500', 'S15a0a', 'S22f00', 'S15a02', 'S20500', 'P', 'S38900', 'M', 'S17911', 'S17919', 'S2c300', 'S2c311', 'S30a00', 'P', 'S38700', 'M', 'S15a06', 'S15a41', 'S23d04', 'M', 'S2ff00', 'S10050', 'S10058', 'S10011', 'S22a04', 'M', 'S1f522', 'S2ea46', 'S2ea02', 'S1f50a', 'P', 'S38700', 'R', 'S3770b', 'S20305', 'S20500', 'S20303', 'S20500', 'S37713', 'S30e00', 'S30124', 'R', 'S1f522', 'S2ea46', 'S2ea02', 'S1f50a', 'P', 'S38700', 'M', 'S20356', 'S20350', 'S37706', 'S22f04', 'S30e00', 'S30124', 'M', 'S1f740', 'S1f748', 'S26a20', 'M', 'S1814b', 'S20500', 'S20500', 'S18143', 'R', 'S10000', 'S26500', 'R', 'S10011', 'S2ff00', 'P', 'S38700', 'M', 'S10000', 'S26500', 'M', 'S15a18', 'S15a18', 'S10e22', 'S10e02', 'P', 'S38800']
2021-11-23 15:00:24,141 - DEBUG - joeynmt.training - 	Raw factor:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a', 'S1f0', 'S265', 'M', 'S10e', 'P', 'S388', 'M', 'S1dc', 'S100', 'S205', 'S205', 'S30a', 'M', 'S203', 'S203', 'S203', 'S203', 'S30e', 'S301', 'M', 'S205', 'S100', 'M', 'S301', 'S100', 'S265', 'M', 'S22f', 'S205', 'S15a', 'S22f', 'S15a', 'S205', 'P', 'S389', 'M', 'S179', 'S179', 'S2c3', 'S2c3', 'S30a', 'P', 'S387', 'M', 'S15a', 'S15a', 'S23d', 'M', 'S2ff', 'S100', 'S100', 'S100', 'S22a', 'M', 'S1f5', 'S2ea', 'S2ea', 'S1f5', 'P', 'S387', 'R', 'S377', 'S203', 'S205', 'S203', 'S205', 'S377', 'S30e', 'S301', 'R', 'S1f5', 'S2ea', 'S2ea', 'S1f5', 'P', 'S387', 'M', 'S203', 'S203', 'S377', 'S22f', 'S30e', 'S301', 'M', 'S1f7', 'S1f7', 'S26a', 'M', 'S181', 'S205', 'S205', 'S181', 'R', 'S100', 'S265', 'R', 'S100', 'S2ff', 'P', 'S387', 'M', 'S100', 'S265', 'M', 'S15a', 'S15a', 'S10e', 'S10e', 'P', 'S388']
2021-11-23 15:00:24,141 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁Verse', '▁3', '1.', '▁"', 'I', '▁have', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been', '▁been']
2021-11-23 15:00:24,141 - INFO - joeynmt.training - 	Source:     <2en> <4us> <sent> M S15a07 S1f010 S26507 M S10e00 P S38800 M S1dc0a S10003 S20500 S20500 S30a00 M S20302 S2030a S20340 S20348 S30e00 S30124 M S20500 S10013 M S30124 S10010 S26500 M S22f10 S20500 S15a0a S22f00 S15a02 S20500 P S38900 M S17911 S17919 S2c300 S2c311 S30a00 P S38700 M S15a06 S15a41 S23d04 M S2ff00 S10050 S10058 S10011 S22a04 M S1f522 S2ea46 S2ea02 S1f50a P S38700 R S3770b S20305 S20500 S20303 S20500 S37713 S30e00 S30124 R S1f522 S2ea46 S2ea02 S1f50a P S38700 M S20356 S20350 S37706 S22f04 S30e00 S30124 M S1f740 S1f748 S26a20 M S1814b S20500 S20500 S18143 R S10000 S26500 R S10011 S2ff00 P S38700 M S10000 S26500 M S15a18 S15a18 S10e22 S10e02 P S38800
2021-11-23 15:00:24,141 - INFO - joeynmt.training - 	Factor:     <2en> <4us> <sent> M S15a S1f0 S265 M S10e P S388 M S1dc S100 S205 S205 S30a M S203 S203 S203 S203 S30e S301 M S205 S100 M S301 S100 S265 M S22f S205 S15a S22f S15a S205 P S389 M S179 S179 S2c3 S2c3 S30a P S387 M S15a S15a S23d M S2ff S100 S100 S100 S22a M S1f5 S2ea S2ea S1f5 P S387 R S377 S203 S205 S203 S205 S377 S30e S301 R S1f5 S2ea S2ea S1f5 P S387 M S203 S203 S377 S22f S30e S301 M S1f7 S1f7 S26a M S181 S205 S205 S181 R S100 S265 R S100 S2ff P S387 M S100 S265 M S15a S15a S10e S10e P S388
2021-11-23 15:00:24,141 - INFO - joeynmt.training - 	Reference:  ▁Verse ▁2. ▁Then ▁make ▁me ▁tr ul y ▁hap p y ▁by ▁ag ree ing ▁whole he art ed ly ▁with ▁each ▁other , ▁lo v ing ▁one ▁another , ▁and ▁work ing ▁together ▁with ▁one ▁m ind ▁and ▁p ur p ose .
2021-11-23 15:00:24,141 - INFO - joeynmt.training - 	Hypothesis: ▁Verse ▁3 1. ▁" I ▁have ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been ▁been
2021-11-23 15:00:24,141 - INFO - joeynmt.training - Example #3
2021-11-23 15:00:24,141 - DEBUG - joeynmt.training - 	Raw source:     ['<2pt>', '<4br>', '<dict>', 'M', 'S10e40', 'S11040', 'S26504', 'S2fb04']
2021-11-23 15:00:24,141 - DEBUG - joeynmt.training - 	Raw factor:     ['<2pt>', '<4br>', '<dict>', 'M', 'S10e', 'S110', 'S265', 'S2fb']
2021-11-23 15:00:24,141 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁m', 'ão']
2021-11-23 15:00:24,141 - INFO - joeynmt.training - 	Source:     <2pt> <4br> <dict> M S10e40 S11040 S26504 S2fb04
2021-11-23 15:00:24,141 - INFO - joeynmt.training - 	Factor:     <2pt> <4br> <dict> M S10e S110 S265 S2fb
2021-11-23 15:00:24,141 - INFO - joeynmt.training - 	Reference:  ▁expl or ar
2021-11-23 15:00:24,141 - INFO - joeynmt.training - 	Hypothesis: ▁m ão
2021-11-23 15:00:24,141 - INFO - joeynmt.training - Example #6
2021-11-23 15:00:24,141 - DEBUG - joeynmt.training - 	Raw source:     ['<2en>', '<4us>', '<dict>', 'M', 'S20340', 'S21800']
2021-11-23 15:00:24,141 - DEBUG - joeynmt.training - 	Raw factor:     ['<2en>', '<4us>', '<dict>', 'M', 'S203', 'S218']
2021-11-23 15:00:24,141 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁s', 'ix']
2021-11-23 15:00:24,141 - INFO - joeynmt.training - 	Source:     <2en> <4us> <dict> M S20340 S21800
2021-11-23 15:00:24,141 - INFO - joeynmt.training - 	Factor:     <2en> <4us> <dict> M S203 S218
2021-11-23 15:00:24,141 - INFO - joeynmt.training - 	Reference:  ▁m il k
2021-11-23 15:00:24,141 - INFO - joeynmt.training - 	Hypothesis: ▁s ix
2021-11-23 15:00:24,142 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step    17000: bleu:   0.36, loss: 109871.7891, ppl:  25.6415, duration: 426.6053s
2021-11-23 15:00:36,885 - INFO - joeynmt.training - Epoch   5, Step:    17100, Batch Loss:     3.183886, Tokens per Sec:     2284, Lr: 0.000100
2021-11-23 15:00:49,761 - INFO - joeynmt.training - Epoch   5, Step:    17200, Batch Loss:     3.301687, Tokens per Sec:     2267, Lr: 0.000100
2021-11-23 15:01:02,501 - INFO - joeynmt.training - Epoch   5, Step:    17300, Batch Loss:     3.188737, Tokens per Sec:     2335, Lr: 0.000100
2021-11-23 15:01:14,969 - INFO - joeynmt.training - Epoch   5, Step:    17400, Batch Loss:     3.107850, Tokens per Sec:     2315, Lr: 0.000100
2021-11-23 15:01:27,705 - INFO - joeynmt.training - Epoch   5, Step:    17500, Batch Loss:     3.566866, Tokens per Sec:     2215, Lr: 0.000100
2021-11-23 15:01:40,636 - INFO - joeynmt.training - Epoch   5, Step:    17600, Batch Loss:     3.354903, Tokens per Sec:     2362, Lr: 0.000100
2021-11-23 15:01:53,080 - INFO - joeynmt.training - Epoch   5, Step:    17700, Batch Loss:     3.212671, Tokens per Sec:     2308, Lr: 0.000100
2021-11-23 15:02:05,870 - INFO - joeynmt.training - Epoch   5, Step:    17800, Batch Loss:     3.323063, Tokens per Sec:     2293, Lr: 0.000100
2021-11-23 15:02:18,632 - INFO - joeynmt.training - Epoch   5, Step:    17900, Batch Loss:     3.247892, Tokens per Sec:     2366, Lr: 0.000100
2021-11-23 15:02:31,290 - INFO - joeynmt.training - Epoch   5, Step:    18000, Batch Loss:     3.139906, Tokens per Sec:     2321, Lr: 0.000100
