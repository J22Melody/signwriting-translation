2021-11-23 15:18:38,820 - INFO - root - Hello! This is Joey-NMT (version 1.3).
2021-11-23 15:18:38,953 - INFO - joeynmt.data - Loading training data...
2021-11-23 15:18:45,056 - INFO - joeynmt.data - Building vocabulary...
2021-11-23 15:18:49,733 - INFO - joeynmt.data - Loading dev data...
2021-11-23 15:18:49,831 - INFO - joeynmt.data - Loading test data...
2021-11-23 15:18:49,896 - INFO - joeynmt.data - Data loaded.
2021-11-23 15:18:49,896 - INFO - joeynmt.model - Building an encoder-decoder model...
2021-11-23 15:18:50,653 - INFO - joeynmt.model - Enc-dec model built.
2021-11-23 15:18:54,115 - DEBUG - tensorflow - Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.
2021-11-23 15:18:54,870 - DEBUG - h5py._conv - Creating converter from 7 to 5
2021-11-23 15:18:54,871 - DEBUG - h5py._conv - Creating converter from 5 to 7
2021-11-23 15:18:54,871 - DEBUG - h5py._conv - Creating converter from 7 to 5
2021-11-23 15:18:54,871 - DEBUG - h5py._conv - Creating converter from 5 to 7
2021-11-23 15:19:02,376 - INFO - joeynmt.training - Total params: 49881760
2021-11-23 15:19:02,377 - DEBUG - joeynmt.training - Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'encoder.layers.4.feed_forward.layer_norm.bias', 'encoder.layers.4.feed_forward.layer_norm.weight', 'encoder.layers.4.feed_forward.pwff_layer.0.bias', 'encoder.layers.4.feed_forward.pwff_layer.0.weight', 'encoder.layers.4.feed_forward.pwff_layer.3.bias', 'encoder.layers.4.feed_forward.pwff_layer.3.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.4.src_src_att.k_layer.bias', 'encoder.layers.4.src_src_att.k_layer.weight', 'encoder.layers.4.src_src_att.output_layer.bias', 'encoder.layers.4.src_src_att.output_layer.weight', 'encoder.layers.4.src_src_att.q_layer.bias', 'encoder.layers.4.src_src_att.q_layer.weight', 'encoder.layers.4.src_src_att.v_layer.bias', 'encoder.layers.4.src_src_att.v_layer.weight', 'encoder.layers.5.feed_forward.layer_norm.bias', 'encoder.layers.5.feed_forward.layer_norm.weight', 'encoder.layers.5.feed_forward.pwff_layer.0.bias', 'encoder.layers.5.feed_forward.pwff_layer.0.weight', 'encoder.layers.5.feed_forward.pwff_layer.3.bias', 'encoder.layers.5.feed_forward.pwff_layer.3.weight', 'encoder.layers.5.layer_norm.bias', 'encoder.layers.5.layer_norm.weight', 'encoder.layers.5.src_src_att.k_layer.bias', 'encoder.layers.5.src_src_att.k_layer.weight', 'encoder.layers.5.src_src_att.output_layer.bias', 'encoder.layers.5.src_src_att.output_layer.weight', 'encoder.layers.5.src_src_att.q_layer.bias', 'encoder.layers.5.src_src_att.q_layer.weight', 'encoder.layers.5.src_src_att.v_layer.bias', 'encoder.layers.5.src_src_att.v_layer.weight', 'factor_embeds.0.lut.weight', 'factor_embeds.1.lut.weight', 'factor_embeds.2.lut.weight', 'factor_embeds.3.lut.weight', 'factor_embeds.4.lut.weight', 'factor_embeds.5.lut.weight', 'factor_embeds.6.lut.weight', 'src_embed.lut.weight', 'trg_embed.lut.weight']
2021-11-23 15:19:02,379 - WARNING - joeynmt.training - `keep_last_ckpts` option is outdated. Please use `keep_best_ckpts`, instead.
2021-11-23 15:19:09,187 - INFO - joeynmt.helpers - cfg.name                           : baseline_multilingual_mem
2021-11-23 15:19:09,198 - INFO - joeynmt.helpers - cfg.data.src                       : sign
2021-11-23 15:19:09,198 - INFO - joeynmt.helpers - cfg.data.trg                       : spm.spoken
2021-11-23 15:19:09,199 - INFO - joeynmt.helpers - cfg.data.factors                   : ['sign+', 'feat_col', 'feat_row', 'feat_x', 'feat_y', 'feat_x_rel', 'feat_y_rel']
2021-11-23 15:19:09,199 - INFO - joeynmt.helpers - cfg.data.train                     : data/train
2021-11-23 15:19:09,199 - INFO - joeynmt.helpers - cfg.data.dev                       : data/dev
2021-11-23 15:19:09,199 - INFO - joeynmt.helpers - cfg.data.test                      : data/test
2021-11-23 15:19:09,199 - INFO - joeynmt.helpers - cfg.data.level                     : word
2021-11-23 15:19:09,200 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False
2021-11-23 15:19:09,200 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 200
2021-11-23 15:19:09,201 - INFO - joeynmt.helpers - cfg.data.factor_voc_limit          : 10000
2021-11-23 15:19:09,201 - INFO - joeynmt.helpers - cfg.data.factor_voc_min_freq       : 1
2021-11-23 15:19:09,201 - INFO - joeynmt.helpers - cfg.data.use_factor                : True
2021-11-23 15:19:09,201 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5
2021-11-23 15:19:09,201 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0
2021-11-23 15:19:09,201 - INFO - joeynmt.helpers - cfg.testing.postprocess            : False
2021-11-23 15:19:09,201 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42
2021-11-23 15:19:09,202 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam
2021-11-23 15:19:09,202 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens
2021-11-23 15:19:09,202 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]
2021-11-23 15:19:09,202 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau
2021-11-23 15:19:09,202 - INFO - joeynmt.helpers - cfg.training.patience              : 5
2021-11-23 15:19:09,202 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7
2021-11-23 15:19:09,203 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy
2021-11-23 15:19:09,203 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0001
2021-11-23 15:19:09,203 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08
2021-11-23 15:19:09,203 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0
2021-11-23 15:19:09,203 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.2
2021-11-23 15:19:09,203 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096
2021-11-23 15:19:09,203 - INFO - joeynmt.helpers - cfg.training.batch_type            : token
2021-11-23 15:19:09,204 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 2048
2021-11-23 15:19:09,204 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token
2021-11-23 15:19:09,204 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1
2021-11-23 15:19:09,204 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : eval_metric
2021-11-23 15:19:09,204 - INFO - joeynmt.helpers - cfg.training.epochs                : 200
2021-11-23 15:19:09,204 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 3600
2021-11-23 15:19:09,204 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 360
2021-11-23 15:19:09,205 - INFO - joeynmt.helpers - cfg.training.eval_metric           : chrf
2021-11-23 15:19:09,205 - INFO - joeynmt.helpers - cfg.training.overwrite             : True
2021-11-23 15:19:09,205 - INFO - joeynmt.helpers - cfg.training.shuffle               : True
2021-11-23 15:19:09,205 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True
2021-11-23 15:19:09,205 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 200
2021-11-23 15:19:09,205 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3, 6]
2021-11-23 15:19:09,205 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 1
2021-11-23 15:19:09,206 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/baseline_multilingual_mem
2021-11-23 15:19:09,206 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier
2021-11-23 15:19:09,206 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros
2021-11-23 15:19:09,206 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0
2021-11-23 15:19:09,206 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier
2021-11-23 15:19:09,206 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0
2021-11-23 15:19:09,206 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True
2021-11-23 15:19:09,207 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer
2021-11-23 15:19:09,207 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6
2021-11-23 15:19:09,207 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 8
2021-11-23 15:19:09,207 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 400
2021-11-23 15:19:09,207 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True
2021-11-23 15:19:09,207 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.5
2021-11-23 15:19:09,207 - INFO - joeynmt.helpers - cfg.model.encoder.factor_embeddings.embedding_dim : 16
2021-11-23 15:19:09,208 - INFO - joeynmt.helpers - cfg.model.encoder.factor_embeddings.scale : True
2021-11-23 15:19:09,208 - INFO - joeynmt.helpers - cfg.model.encoder.factor_embeddings.dropout : 0.5
2021-11-23 15:19:09,208 - INFO - joeynmt.helpers - cfg.model.encoder.factor_combine   : concatenate
2021-11-23 15:19:09,208 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 512
2021-11-23 15:19:09,208 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 2048
2021-11-23 15:19:09,208 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.5
2021-11-23 15:19:09,209 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer
2021-11-23 15:19:09,209 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6
2021-11-23 15:19:09,209 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 8
2021-11-23 15:19:09,209 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 512
2021-11-23 15:19:09,209 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True
2021-11-23 15:19:09,209 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.5
2021-11-23 15:19:09,209 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 512
2021-11-23 15:19:09,210 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 2048
2021-11-23 15:19:09,210 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.5
2021-11-23 15:19:09,211 - INFO - joeynmt.helpers - Data set sizes: 
	train 108087,
	valid 3426,
	test 2286
2021-11-23 15:19:09,211 - INFO - joeynmt.helpers - First training example:
	[SRC] <2pt> <4br> <dict> M S1dc02 S17610 S1f502 S20320
	[SRC_FACTOR0] <2pt> <4br> <dict> M S1dc S176 S1f5 S203
	[TRG] ▁2 01 8
2021-11-23 15:19:09,212 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) M (5) <dict> (6) P (7) S20500 (8) S2ff00 (9) <2pt>
2021-11-23 15:19:09,214 - INFO - joeynmt.helpers - First 10 words (src_factor0): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) M (5) S15a (6) <dict> (7) S100 (8) P (9) S265
2021-11-23 15:19:09,214 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) ▁the (6) . (7) ▁and (8) - (9) s
2021-11-23 15:19:09,214 - INFO - joeynmt.helpers - Number of Src words (types): 11765
2021-11-23 15:19:09,234 - INFO - joeynmt.helpers - Number of Src_factor0 words (types): 653
2021-11-23 15:19:09,235 - INFO - joeynmt.helpers - Number of Trg words (types): 1969
2021-11-23 15:19:09,237 - INFO - joeynmt.training - Model(
	encoder=TransformerEncoder(num_layers=6, num_heads=8),
	decoder=TransformerDecoder(num_layers=6, num_heads=8),
	src_embed=Embeddings(embedding_dim=400, vocab_size=11765),
	factor_embeds=ModuleList(
  (0): Embeddings(embedding_dim=16, vocab_size=653)
  (1): Embeddings(embedding_dim=16, vocab_size=11)
  (2): Embeddings(embedding_dim=16, vocab_size=21)
  (3): Embeddings(embedding_dim=16, vocab_size=394)
  (4): Embeddings(embedding_dim=16, vocab_size=457)
  (5): Embeddings(embedding_dim=16, vocab_size=83)
  (6): Embeddings(embedding_dim=16, vocab_size=74)
),
	trg_embed=Embeddings(embedding_dim=512, vocab_size=1969))
2021-11-23 15:19:09,290 - INFO - joeynmt.training - Train stats:
	device: cuda
	n_gpu: 1
	16-bits training: False
	gradient accumulation: 1
	batch size per device: 4096
	total batch size (w. parallel & accumulation): 4096
2021-11-23 15:19:09,291 - INFO - joeynmt.training - EPOCH 1
2021-11-23 15:19:55,585 - INFO - joeynmt.training - Epoch   1, Step:      360, Batch Loss:     4.312042, Tokens per Sec:     2272, Lr: 0.000100
2021-11-23 15:20:40,713 - INFO - joeynmt.training - Epoch   1, Step:      720, Batch Loss:     4.103721, Tokens per Sec:     2232, Lr: 0.000100
2021-11-23 15:21:26,222 - INFO - joeynmt.training - Epoch   1, Step:     1080, Batch Loss:     4.141672, Tokens per Sec:     2279, Lr: 0.000100
2021-11-23 15:22:11,550 - INFO - joeynmt.training - Epoch   1, Step:     1440, Batch Loss:     4.315182, Tokens per Sec:     2263, Lr: 0.000100
2021-11-23 15:22:56,910 - INFO - joeynmt.training - Epoch   1, Step:     1800, Batch Loss:     4.045748, Tokens per Sec:     2277, Lr: 0.000100
2021-11-23 15:23:42,655 - INFO - joeynmt.training - Epoch   1, Step:     2160, Batch Loss:     4.121461, Tokens per Sec:     2286, Lr: 0.000100
2021-11-23 15:24:28,398 - INFO - joeynmt.training - Epoch   1, Step:     2520, Batch Loss:     3.978220, Tokens per Sec:     2316, Lr: 0.000100
2021-11-23 15:25:13,975 - INFO - joeynmt.training - Epoch   1, Step:     2880, Batch Loss:     3.978344, Tokens per Sec:     2275, Lr: 0.000100
2021-11-23 15:25:59,557 - INFO - joeynmt.training - Epoch   1, Step:     3240, Batch Loss:     3.762781, Tokens per Sec:     2264, Lr: 0.000100
2021-11-23 15:26:45,534 - INFO - joeynmt.training - Epoch   1, Step:     3600, Batch Loss:     3.840024, Tokens per Sec:     2263, Lr: 0.000100
2021-11-23 15:32:04,817 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-11-23 15:32:06,605 - INFO - joeynmt.training - Example #0
2021-11-23 15:32:06,605 - DEBUG - joeynmt.training - 	Raw source:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a07', 'S1f010', 'S26507', 'M', 'S11e20', 'S1a520', 'P', 'S38800', 'M', 'S15a56', 'S15a41', 'S2b700', 'S37c06', 'S36d01', 'S30d00', 'M', 'S10040', 'P', 'S38700', 'M', 'S37806', 'S10050', 'S20356', 'S36d01', 'M', 'S15a37', 'S1ce51', 'S26a07', 'P', 'S38700', 'M', 'S19220', 'S2a20c', 'S11520', 'S10120', 'S1f720', 'S20320', 'S36d01', 'M', 'S10018', 'S26505', 'S10641', 'M', 'S1f000', 'S1f720', 'S1dc20', 'S19220', 'S14a20', 'S1dc20', 'S14a20', 'P', 'S38800', 'M', 'S10041', 'S30d00', 'S36d01', 'M', 'S15a31', 'S10018', 'S26503', 'M', 'S2e74c', 'S14220', 'S2e700', 'S14228', 'M', 'S26521', 'S1f548', 'S1f540', 'M', 'S10041', 'P', 'S38700', 'M', 'S10021', 'S10029', 'S22a07', 'S22a11', 'S36d01', 'M', 'S2ea3e', 'S10000', 'M', 'S15a18', 'S10051', 'S26601', 'M', 'S10058', 'S26526', 'S10050', 'S20500', 'S20500', 'P', 'S38900', 'M', 'S15a37', 'S15a01', 'S2e900', 'S36d01', 'M', 'S15a20', 'S26501', 'M', 'S26521', 'S23b0a', 'S1f548', 'S15a40', 'S15a48', 'S23b1a', 'S1f540', 'M', 'S14c57', 'S14c5f', 'S26511', 'S26507', 'S22527', 'S20314', 'S22521', 'S2031c', 'P', 'S38800']
2021-11-23 15:32:06,605 - DEBUG - joeynmt.training - 	Raw factor:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a', 'S1f0', 'S265', 'M', 'S11e', 'S1a5', 'P', 'S388', 'M', 'S15a', 'S15a', 'S2b7', 'S37c', 'S36d', 'S30d', 'M', 'S100', 'P', 'S387', 'M', 'S378', 'S100', 'S203', 'S36d', 'M', 'S15a', 'S1ce', 'S26a', 'P', 'S387', 'M', 'S192', 'S2a2', 'S115', 'S101', 'S1f7', 'S203', 'S36d', 'M', 'S100', 'S265', 'S106', 'M', 'S1f0', 'S1f7', 'S1dc', 'S192', 'S14a', 'S1dc', 'S14a', 'P', 'S388', 'M', 'S100', 'S30d', 'S36d', 'M', 'S15a', 'S100', 'S265', 'M', 'S2e7', 'S142', 'S2e7', 'S142', 'M', 'S265', 'S1f5', 'S1f5', 'M', 'S100', 'P', 'S387', 'M', 'S100', 'S100', 'S22a', 'S22a', 'S36d', 'M', 'S2ea', 'S100', 'M', 'S15a', 'S100', 'S266', 'M', 'S100', 'S265', 'S100', 'S205', 'S205', 'P', 'S389', 'M', 'S15a', 'S15a', 'S2e9', 'S36d', 'M', 'S15a', 'S265', 'M', 'S265', 'S23b', 'S1f5', 'S15a', 'S15a', 'S23b', 'S1f5', 'M', 'S14c', 'S14c', 'S265', 'S265', 'S225', 'S203', 'S225', 'S203', 'P', 'S388']
2021-11-23 15:32:06,605 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁Verse', '▁8.', '▁The', '▁LORD', ',', '▁"', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', '▁¶', '▁¶', '▁¶', '▁¶', '▁¶', '▁¶', '▁¶', '▁¶', '▁¶', '▁¶', '▁¶', '▁¶', '▁¶', '▁¶', '▁¶']
2021-11-23 15:32:06,606 - INFO - joeynmt.training - 	Source:     <2en> <4us> <sent> M S15a07 S1f010 S26507 M S11e20 S1a520 P S38800 M S15a56 S15a41 S2b700 S37c06 S36d01 S30d00 M S10040 P S38700 M S37806 S10050 S20356 S36d01 M S15a37 S1ce51 S26a07 P S38700 M S19220 S2a20c S11520 S10120 S1f720 S20320 S36d01 M S10018 S26505 S10641 M S1f000 S1f720 S1dc20 S19220 S14a20 S1dc20 S14a20 P S38800 M S10041 S30d00 S36d01 M S15a31 S10018 S26503 M S2e74c S14220 S2e700 S14228 M S26521 S1f548 S1f540 M S10041 P S38700 M S10021 S10029 S22a07 S22a11 S36d01 M S2ea3e S10000 M S15a18 S10051 S26601 M S10058 S26526 S10050 S20500 S20500 P S38900 M S15a37 S15a01 S2e900 S36d01 M S15a20 S26501 M S26521 S23b0a S1f548 S15a40 S15a48 S23b1a S1f540 M S14c57 S14c5f S26511 S26507 S22527 S20314 S22521 S2031c P S38800
2021-11-23 15:32:06,606 - INFO - joeynmt.training - 	Factor:     <2en> <4us> <sent> M S15a S1f0 S265 M S11e S1a5 P S388 M S15a S15a S2b7 S37c S36d S30d M S100 P S387 M S378 S100 S203 S36d M S15a S1ce S26a P S387 M S192 S2a2 S115 S101 S1f7 S203 S36d M S100 S265 S106 M S1f0 S1f7 S1dc S192 S14a S1dc S14a P S388 M S100 S30d S36d M S15a S100 S265 M S2e7 S142 S2e7 S142 M S265 S1f5 S1f5 M S100 P S387 M S100 S100 S22a S22a S36d M S2ea S100 M S15a S100 S266 M S100 S265 S100 S205 S205 P S389 M S15a S15a S2e9 S36d M S15a S265 M S265 S23b S1f5 S15a S15a S23b S1f5 M S14c S14c S265 S265 S225 S203 S225 S203 P S388
2021-11-23 15:32:06,606 - INFO - joeynmt.training - 	Reference:  ▁Verse ▁3 7. ▁After ▁him , ▁at ▁the ▁time ▁of ▁the ▁c ens us , ▁there ▁was ▁Jud as ▁of ▁Gal ile e . ▁He ▁g ot ▁people ▁to ▁follow ▁him , ▁but ▁he ▁was ▁k illed , ▁too , ▁and ▁all ▁his ▁follow ers ▁were ▁sc atter ed .
2021-11-23 15:32:06,606 - INFO - joeynmt.training - 	Hypothesis: ▁Verse ▁8. ▁The ▁LORD , ▁" I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I ▁¶ ▁¶ ▁¶ ▁¶ ▁¶ ▁¶ ▁¶ ▁¶ ▁¶ ▁¶ ▁¶ ▁¶ ▁¶ ▁¶ ▁¶
2021-11-23 15:32:06,606 - INFO - joeynmt.training - Example #1
2021-11-23 15:32:06,606 - DEBUG - joeynmt.training - 	Raw source:     ['<2pt>', '<4br>', '<dict>', 'M', 'S34700', 'S21100', 'S1f410']
2021-11-23 15:32:06,606 - DEBUG - joeynmt.training - 	Raw factor:     ['<2pt>', '<4br>', '<dict>', 'M', 'S347', 'S211', 'S1f4']
2021-11-23 15:32:06,606 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁P', 'or', 'ar']
2021-11-23 15:32:06,606 - INFO - joeynmt.training - 	Source:     <2pt> <4br> <dict> M S34700 S21100 S1f410
2021-11-23 15:32:06,606 - INFO - joeynmt.training - 	Factor:     <2pt> <4br> <dict> M S347 S211 S1f4
2021-11-23 15:32:06,606 - INFO - joeynmt.training - 	Reference:  ▁L á p is
2021-11-23 15:32:06,606 - INFO - joeynmt.training - 	Hypothesis: ▁P or ar
2021-11-23 15:32:06,606 - INFO - joeynmt.training - Example #2
2021-11-23 15:32:06,606 - DEBUG - joeynmt.training - 	Raw source:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a07', 'S1f010', 'S26507', 'M', 'S10e00', 'P', 'S38800', 'M', 'S1dc0a', 'S10003', 'S20500', 'S20500', 'S30a00', 'M', 'S20302', 'S2030a', 'S20340', 'S20348', 'S30e00', 'S30124', 'M', 'S20500', 'S10013', 'M', 'S30124', 'S10010', 'S26500', 'M', 'S22f10', 'S20500', 'S15a0a', 'S22f00', 'S15a02', 'S20500', 'P', 'S38900', 'M', 'S17911', 'S17919', 'S2c300', 'S2c311', 'S30a00', 'P', 'S38700', 'M', 'S15a06', 'S15a41', 'S23d04', 'M', 'S2ff00', 'S10050', 'S10058', 'S10011', 'S22a04', 'M', 'S1f522', 'S2ea46', 'S2ea02', 'S1f50a', 'P', 'S38700', 'R', 'S3770b', 'S20305', 'S20500', 'S20303', 'S20500', 'S37713', 'S30e00', 'S30124', 'R', 'S1f522', 'S2ea46', 'S2ea02', 'S1f50a', 'P', 'S38700', 'M', 'S20356', 'S20350', 'S37706', 'S22f04', 'S30e00', 'S30124', 'M', 'S1f740', 'S1f748', 'S26a20', 'M', 'S1814b', 'S20500', 'S20500', 'S18143', 'R', 'S10000', 'S26500', 'R', 'S10011', 'S2ff00', 'P', 'S38700', 'M', 'S10000', 'S26500', 'M', 'S15a18', 'S15a18', 'S10e22', 'S10e02', 'P', 'S38800']
2021-11-23 15:32:06,607 - DEBUG - joeynmt.training - 	Raw factor:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a', 'S1f0', 'S265', 'M', 'S10e', 'P', 'S388', 'M', 'S1dc', 'S100', 'S205', 'S205', 'S30a', 'M', 'S203', 'S203', 'S203', 'S203', 'S30e', 'S301', 'M', 'S205', 'S100', 'M', 'S301', 'S100', 'S265', 'M', 'S22f', 'S205', 'S15a', 'S22f', 'S15a', 'S205', 'P', 'S389', 'M', 'S179', 'S179', 'S2c3', 'S2c3', 'S30a', 'P', 'S387', 'M', 'S15a', 'S15a', 'S23d', 'M', 'S2ff', 'S100', 'S100', 'S100', 'S22a', 'M', 'S1f5', 'S2ea', 'S2ea', 'S1f5', 'P', 'S387', 'R', 'S377', 'S203', 'S205', 'S203', 'S205', 'S377', 'S30e', 'S301', 'R', 'S1f5', 'S2ea', 'S2ea', 'S1f5', 'P', 'S387', 'M', 'S203', 'S203', 'S377', 'S22f', 'S30e', 'S301', 'M', 'S1f7', 'S1f7', 'S26a', 'M', 'S181', 'S205', 'S205', 'S181', 'R', 'S100', 'S265', 'R', 'S100', 'S2ff', 'P', 'S387', 'M', 'S100', 'S265', 'M', 'S15a', 'S15a', 'S10e', 'S10e', 'P', 'S388']
2021-11-23 15:32:06,607 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁Verse', '▁3', '▁3', '▁And', '▁the', '▁LORD', ',', '▁"', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I']
2021-11-23 15:32:06,607 - INFO - joeynmt.training - 	Source:     <2en> <4us> <sent> M S15a07 S1f010 S26507 M S10e00 P S38800 M S1dc0a S10003 S20500 S20500 S30a00 M S20302 S2030a S20340 S20348 S30e00 S30124 M S20500 S10013 M S30124 S10010 S26500 M S22f10 S20500 S15a0a S22f00 S15a02 S20500 P S38900 M S17911 S17919 S2c300 S2c311 S30a00 P S38700 M S15a06 S15a41 S23d04 M S2ff00 S10050 S10058 S10011 S22a04 M S1f522 S2ea46 S2ea02 S1f50a P S38700 R S3770b S20305 S20500 S20303 S20500 S37713 S30e00 S30124 R S1f522 S2ea46 S2ea02 S1f50a P S38700 M S20356 S20350 S37706 S22f04 S30e00 S30124 M S1f740 S1f748 S26a20 M S1814b S20500 S20500 S18143 R S10000 S26500 R S10011 S2ff00 P S38700 M S10000 S26500 M S15a18 S15a18 S10e22 S10e02 P S38800
2021-11-23 15:32:06,607 - INFO - joeynmt.training - 	Factor:     <2en> <4us> <sent> M S15a S1f0 S265 M S10e P S388 M S1dc S100 S205 S205 S30a M S203 S203 S203 S203 S30e S301 M S205 S100 M S301 S100 S265 M S22f S205 S15a S22f S15a S205 P S389 M S179 S179 S2c3 S2c3 S30a P S387 M S15a S15a S23d M S2ff S100 S100 S100 S22a M S1f5 S2ea S2ea S1f5 P S387 R S377 S203 S205 S203 S205 S377 S30e S301 R S1f5 S2ea S2ea S1f5 P S387 M S203 S203 S377 S22f S30e S301 M S1f7 S1f7 S26a M S181 S205 S205 S181 R S100 S265 R S100 S2ff P S387 M S100 S265 M S15a S15a S10e S10e P S388
2021-11-23 15:32:06,607 - INFO - joeynmt.training - 	Reference:  ▁Verse ▁2. ▁Then ▁make ▁me ▁tr ul y ▁hap p y ▁by ▁ag ree ing ▁whole he art ed ly ▁with ▁each ▁other , ▁lo v ing ▁one ▁another , ▁and ▁work ing ▁together ▁with ▁one ▁m ind ▁and ▁p ur p ose .
2021-11-23 15:32:06,607 - INFO - joeynmt.training - 	Hypothesis: ▁Verse ▁3 ▁3 ▁And ▁the ▁LORD , ▁" I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I
2021-11-23 15:32:06,607 - INFO - joeynmt.training - Example #3
2021-11-23 15:32:06,607 - DEBUG - joeynmt.training - 	Raw source:     ['<2pt>', '<4br>', '<dict>', 'M', 'S10e40', 'S11040', 'S26504', 'S2fb04']
2021-11-23 15:32:06,607 - DEBUG - joeynmt.training - 	Raw factor:     ['<2pt>', '<4br>', '<dict>', 'M', 'S10e', 'S110', 'S265', 'S2fb']
2021-11-23 15:32:06,607 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁P', 'or', 'ar']
2021-11-23 15:32:06,607 - INFO - joeynmt.training - 	Source:     <2pt> <4br> <dict> M S10e40 S11040 S26504 S2fb04
2021-11-23 15:32:06,607 - INFO - joeynmt.training - 	Factor:     <2pt> <4br> <dict> M S10e S110 S265 S2fb
2021-11-23 15:32:06,607 - INFO - joeynmt.training - 	Reference:  ▁expl or ar
2021-11-23 15:32:06,607 - INFO - joeynmt.training - 	Hypothesis: ▁P or ar
2021-11-23 15:32:06,607 - INFO - joeynmt.training - Example #6
2021-11-23 15:32:06,607 - DEBUG - joeynmt.training - 	Raw source:     ['<2en>', '<4us>', '<dict>', 'M', 'S20340', 'S21800']
2021-11-23 15:32:06,608 - DEBUG - joeynmt.training - 	Raw factor:     ['<2en>', '<4us>', '<dict>', 'M', 'S203', 'S218']
2021-11-23 15:32:06,608 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁b', 'or']
2021-11-23 15:32:06,608 - INFO - joeynmt.training - 	Source:     <2en> <4us> <dict> M S20340 S21800
2021-11-23 15:32:06,608 - INFO - joeynmt.training - 	Factor:     <2en> <4us> <dict> M S203 S218
2021-11-23 15:32:06,608 - INFO - joeynmt.training - 	Reference:  ▁m il k
2021-11-23 15:32:06,608 - INFO - joeynmt.training - 	Hypothesis: ▁b or
2021-11-23 15:32:06,608 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step     3600: chrf:   0.09, loss: 128781.7812, ppl:  44.8164, duration: 321.0727s
2021-11-23 15:32:06,970 - INFO - joeynmt.training - Epoch   1: total training loss 14856.17
2021-11-23 15:32:06,970 - INFO - joeynmt.training - EPOCH 2
2021-11-23 15:32:52,242 - INFO - joeynmt.training - Epoch   2, Step:     3960, Batch Loss:     3.949399, Tokens per Sec:     2250, Lr: 0.000100
2021-11-23 15:33:37,658 - INFO - joeynmt.training - Epoch   2, Step:     4320, Batch Loss:     3.712070, Tokens per Sec:     2315, Lr: 0.000100
2021-11-23 15:34:22,987 - INFO - joeynmt.training - Epoch   2, Step:     4680, Batch Loss:     4.075233, Tokens per Sec:     2278, Lr: 0.000100
2021-11-23 15:35:08,563 - INFO - joeynmt.training - Epoch   2, Step:     5040, Batch Loss:     3.709465, Tokens per Sec:     2283, Lr: 0.000100
2021-11-23 15:35:53,648 - INFO - joeynmt.training - Epoch   2, Step:     5400, Batch Loss:     3.656118, Tokens per Sec:     2275, Lr: 0.000100
2021-11-23 15:36:39,228 - INFO - joeynmt.training - Epoch   2, Step:     5760, Batch Loss:     3.756019, Tokens per Sec:     2215, Lr: 0.000100
2021-11-23 15:37:24,647 - INFO - joeynmt.training - Epoch   2, Step:     6120, Batch Loss:     3.872624, Tokens per Sec:     2269, Lr: 0.000100
2021-11-23 15:38:10,294 - INFO - joeynmt.training - Epoch   2, Step:     6480, Batch Loss:     3.688171, Tokens per Sec:     2241, Lr: 0.000100
2021-11-23 15:38:55,666 - INFO - joeynmt.training - Epoch   2, Step:     6840, Batch Loss:     3.616437, Tokens per Sec:     2308, Lr: 0.000100
2021-11-23 15:39:41,249 - INFO - joeynmt.training - Epoch   2, Step:     7200, Batch Loss:     3.606278, Tokens per Sec:     2309, Lr: 0.000100
2021-11-23 15:41:25,183 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-11-23 15:41:26,807 - INFO - joeynmt.helpers - delete models/baseline_multilingual_mem/3600.ckpt
2021-11-23 15:41:26,814 - INFO - joeynmt.helpers - delete /net/cephfs/home/zifjia/signwriting-translation/models/baseline_multilingual_mem/3600.ckpt
2021-11-23 15:41:26,818 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /net/cephfs/home/zifjia/signwriting-translation/models/baseline_multilingual_mem/3600.ckpt but file does not exist. ([Errno 2] No such file or directory: '/net/cephfs/home/zifjia/signwriting-translation/models/baseline_multilingual_mem/3600.ckpt')
2021-11-23 15:41:26,837 - INFO - joeynmt.training - Example #0
2021-11-23 15:41:26,838 - DEBUG - joeynmt.training - 	Raw source:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a07', 'S1f010', 'S26507', 'M', 'S11e20', 'S1a520', 'P', 'S38800', 'M', 'S15a56', 'S15a41', 'S2b700', 'S37c06', 'S36d01', 'S30d00', 'M', 'S10040', 'P', 'S38700', 'M', 'S37806', 'S10050', 'S20356', 'S36d01', 'M', 'S15a37', 'S1ce51', 'S26a07', 'P', 'S38700', 'M', 'S19220', 'S2a20c', 'S11520', 'S10120', 'S1f720', 'S20320', 'S36d01', 'M', 'S10018', 'S26505', 'S10641', 'M', 'S1f000', 'S1f720', 'S1dc20', 'S19220', 'S14a20', 'S1dc20', 'S14a20', 'P', 'S38800', 'M', 'S10041', 'S30d00', 'S36d01', 'M', 'S15a31', 'S10018', 'S26503', 'M', 'S2e74c', 'S14220', 'S2e700', 'S14228', 'M', 'S26521', 'S1f548', 'S1f540', 'M', 'S10041', 'P', 'S38700', 'M', 'S10021', 'S10029', 'S22a07', 'S22a11', 'S36d01', 'M', 'S2ea3e', 'S10000', 'M', 'S15a18', 'S10051', 'S26601', 'M', 'S10058', 'S26526', 'S10050', 'S20500', 'S20500', 'P', 'S38900', 'M', 'S15a37', 'S15a01', 'S2e900', 'S36d01', 'M', 'S15a20', 'S26501', 'M', 'S26521', 'S23b0a', 'S1f548', 'S15a40', 'S15a48', 'S23b1a', 'S1f540', 'M', 'S14c57', 'S14c5f', 'S26511', 'S26507', 'S22527', 'S20314', 'S22521', 'S2031c', 'P', 'S38800']
2021-11-23 15:41:26,838 - DEBUG - joeynmt.training - 	Raw factor:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a', 'S1f0', 'S265', 'M', 'S11e', 'S1a5', 'P', 'S388', 'M', 'S15a', 'S15a', 'S2b7', 'S37c', 'S36d', 'S30d', 'M', 'S100', 'P', 'S387', 'M', 'S378', 'S100', 'S203', 'S36d', 'M', 'S15a', 'S1ce', 'S26a', 'P', 'S387', 'M', 'S192', 'S2a2', 'S115', 'S101', 'S1f7', 'S203', 'S36d', 'M', 'S100', 'S265', 'S106', 'M', 'S1f0', 'S1f7', 'S1dc', 'S192', 'S14a', 'S1dc', 'S14a', 'P', 'S388', 'M', 'S100', 'S30d', 'S36d', 'M', 'S15a', 'S100', 'S265', 'M', 'S2e7', 'S142', 'S2e7', 'S142', 'M', 'S265', 'S1f5', 'S1f5', 'M', 'S100', 'P', 'S387', 'M', 'S100', 'S100', 'S22a', 'S22a', 'S36d', 'M', 'S2ea', 'S100', 'M', 'S15a', 'S100', 'S266', 'M', 'S100', 'S265', 'S100', 'S205', 'S205', 'P', 'S389', 'M', 'S15a', 'S15a', 'S2e9', 'S36d', 'M', 'S15a', 'S265', 'M', 'S265', 'S23b', 'S1f5', 'S15a', 'S15a', 'S23b', 'S1f5', 'M', 'S14c', 'S14c', 'S265', 'S265', 'S225', 'S203', 'S225', 'S203', 'P', 'S388']
2021-11-23 15:41:26,838 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁Verse', '▁2.', '▁Then', '▁the', '▁LORD', '▁was', '▁the', '▁LORD', ',', '▁"', 'I', '▁am', '▁the', '▁LORD', "'", 's', '▁the', '▁LORD', "'", 's', '▁the', '▁LORD', "'", 's', '▁the', '▁LORD', "'", 's', '▁the', '▁LORD', '.']
2021-11-23 15:41:26,838 - INFO - joeynmt.training - 	Source:     <2en> <4us> <sent> M S15a07 S1f010 S26507 M S11e20 S1a520 P S38800 M S15a56 S15a41 S2b700 S37c06 S36d01 S30d00 M S10040 P S38700 M S37806 S10050 S20356 S36d01 M S15a37 S1ce51 S26a07 P S38700 M S19220 S2a20c S11520 S10120 S1f720 S20320 S36d01 M S10018 S26505 S10641 M S1f000 S1f720 S1dc20 S19220 S14a20 S1dc20 S14a20 P S38800 M S10041 S30d00 S36d01 M S15a31 S10018 S26503 M S2e74c S14220 S2e700 S14228 M S26521 S1f548 S1f540 M S10041 P S38700 M S10021 S10029 S22a07 S22a11 S36d01 M S2ea3e S10000 M S15a18 S10051 S26601 M S10058 S26526 S10050 S20500 S20500 P S38900 M S15a37 S15a01 S2e900 S36d01 M S15a20 S26501 M S26521 S23b0a S1f548 S15a40 S15a48 S23b1a S1f540 M S14c57 S14c5f S26511 S26507 S22527 S20314 S22521 S2031c P S38800
2021-11-23 15:41:26,838 - INFO - joeynmt.training - 	Factor:     <2en> <4us> <sent> M S15a S1f0 S265 M S11e S1a5 P S388 M S15a S15a S2b7 S37c S36d S30d M S100 P S387 M S378 S100 S203 S36d M S15a S1ce S26a P S387 M S192 S2a2 S115 S101 S1f7 S203 S36d M S100 S265 S106 M S1f0 S1f7 S1dc S192 S14a S1dc S14a P S388 M S100 S30d S36d M S15a S100 S265 M S2e7 S142 S2e7 S142 M S265 S1f5 S1f5 M S100 P S387 M S100 S100 S22a S22a S36d M S2ea S100 M S15a S100 S266 M S100 S265 S100 S205 S205 P S389 M S15a S15a S2e9 S36d M S15a S265 M S265 S23b S1f5 S15a S15a S23b S1f5 M S14c S14c S265 S265 S225 S203 S225 S203 P S388
2021-11-23 15:41:26,838 - INFO - joeynmt.training - 	Reference:  ▁Verse ▁3 7. ▁After ▁him , ▁at ▁the ▁time ▁of ▁the ▁c ens us , ▁there ▁was ▁Jud as ▁of ▁Gal ile e . ▁He ▁g ot ▁people ▁to ▁follow ▁him , ▁but ▁he ▁was ▁k illed , ▁too , ▁and ▁all ▁his ▁follow ers ▁were ▁sc atter ed .
2021-11-23 15:41:26,839 - INFO - joeynmt.training - 	Hypothesis: ▁Verse ▁2. ▁Then ▁the ▁LORD ▁was ▁the ▁LORD , ▁" I ▁am ▁the ▁LORD ' s ▁the ▁LORD ' s ▁the ▁LORD ' s ▁the ▁LORD ' s ▁the ▁LORD .
2021-11-23 15:41:26,839 - INFO - joeynmt.training - Example #1
2021-11-23 15:41:26,839 - DEBUG - joeynmt.training - 	Raw source:     ['<2pt>', '<4br>', '<dict>', 'M', 'S34700', 'S21100', 'S1f410']
2021-11-23 15:41:26,839 - DEBUG - joeynmt.training - 	Raw factor:     ['<2pt>', '<4br>', '<dict>', 'M', 'S347', 'S211', 'S1f4']
2021-11-23 15:41:26,839 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁C', 'O']
2021-11-23 15:41:26,839 - INFO - joeynmt.training - 	Source:     <2pt> <4br> <dict> M S34700 S21100 S1f410
2021-11-23 15:41:26,839 - INFO - joeynmt.training - 	Factor:     <2pt> <4br> <dict> M S347 S211 S1f4
2021-11-23 15:41:26,839 - INFO - joeynmt.training - 	Reference:  ▁L á p is
2021-11-23 15:41:26,839 - INFO - joeynmt.training - 	Hypothesis: ▁C O
2021-11-23 15:41:26,840 - INFO - joeynmt.training - Example #2
2021-11-23 15:41:26,840 - DEBUG - joeynmt.training - 	Raw source:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a07', 'S1f010', 'S26507', 'M', 'S10e00', 'P', 'S38800', 'M', 'S1dc0a', 'S10003', 'S20500', 'S20500', 'S30a00', 'M', 'S20302', 'S2030a', 'S20340', 'S20348', 'S30e00', 'S30124', 'M', 'S20500', 'S10013', 'M', 'S30124', 'S10010', 'S26500', 'M', 'S22f10', 'S20500', 'S15a0a', 'S22f00', 'S15a02', 'S20500', 'P', 'S38900', 'M', 'S17911', 'S17919', 'S2c300', 'S2c311', 'S30a00', 'P', 'S38700', 'M', 'S15a06', 'S15a41', 'S23d04', 'M', 'S2ff00', 'S10050', 'S10058', 'S10011', 'S22a04', 'M', 'S1f522', 'S2ea46', 'S2ea02', 'S1f50a', 'P', 'S38700', 'R', 'S3770b', 'S20305', 'S20500', 'S20303', 'S20500', 'S37713', 'S30e00', 'S30124', 'R', 'S1f522', 'S2ea46', 'S2ea02', 'S1f50a', 'P', 'S38700', 'M', 'S20356', 'S20350', 'S37706', 'S22f04', 'S30e00', 'S30124', 'M', 'S1f740', 'S1f748', 'S26a20', 'M', 'S1814b', 'S20500', 'S20500', 'S18143', 'R', 'S10000', 'S26500', 'R', 'S10011', 'S2ff00', 'P', 'S38700', 'M', 'S10000', 'S26500', 'M', 'S15a18', 'S15a18', 'S10e22', 'S10e02', 'P', 'S38800']
2021-11-23 15:41:26,840 - DEBUG - joeynmt.training - 	Raw factor:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a', 'S1f0', 'S265', 'M', 'S10e', 'P', 'S388', 'M', 'S1dc', 'S100', 'S205', 'S205', 'S30a', 'M', 'S203', 'S203', 'S203', 'S203', 'S30e', 'S301', 'M', 'S205', 'S100', 'M', 'S301', 'S100', 'S265', 'M', 'S22f', 'S205', 'S15a', 'S22f', 'S15a', 'S205', 'P', 'S389', 'M', 'S179', 'S179', 'S2c3', 'S2c3', 'S30a', 'P', 'S387', 'M', 'S15a', 'S15a', 'S23d', 'M', 'S2ff', 'S100', 'S100', 'S100', 'S22a', 'M', 'S1f5', 'S2ea', 'S2ea', 'S1f5', 'P', 'S387', 'R', 'S377', 'S203', 'S205', 'S203', 'S205', 'S377', 'S30e', 'S301', 'R', 'S1f5', 'S2ea', 'S2ea', 'S1f5', 'P', 'S387', 'M', 'S203', 'S203', 'S377', 'S22f', 'S30e', 'S301', 'M', 'S1f7', 'S1f7', 'S26a', 'M', 'S181', 'S205', 'S205', 'S181', 'R', 'S100', 'S265', 'R', 'S100', 'S2ff', 'P', 'S387', 'M', 'S100', 'S265', 'M', 'S15a', 'S15a', 'S10e', 'S10e', 'P', 'S388']
2021-11-23 15:41:26,840 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁Verse', '▁2.', '▁"', 'I', '▁am', '▁you', '▁are', '▁you', ',', '▁"', 'I', '▁am', '▁you', '▁will', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '▁be', '.']
2021-11-23 15:41:26,840 - INFO - joeynmt.training - 	Source:     <2en> <4us> <sent> M S15a07 S1f010 S26507 M S10e00 P S38800 M S1dc0a S10003 S20500 S20500 S30a00 M S20302 S2030a S20340 S20348 S30e00 S30124 M S20500 S10013 M S30124 S10010 S26500 M S22f10 S20500 S15a0a S22f00 S15a02 S20500 P S38900 M S17911 S17919 S2c300 S2c311 S30a00 P S38700 M S15a06 S15a41 S23d04 M S2ff00 S10050 S10058 S10011 S22a04 M S1f522 S2ea46 S2ea02 S1f50a P S38700 R S3770b S20305 S20500 S20303 S20500 S37713 S30e00 S30124 R S1f522 S2ea46 S2ea02 S1f50a P S38700 M S20356 S20350 S37706 S22f04 S30e00 S30124 M S1f740 S1f748 S26a20 M S1814b S20500 S20500 S18143 R S10000 S26500 R S10011 S2ff00 P S38700 M S10000 S26500 M S15a18 S15a18 S10e22 S10e02 P S38800
2021-11-23 15:41:26,840 - INFO - joeynmt.training - 	Factor:     <2en> <4us> <sent> M S15a S1f0 S265 M S10e P S388 M S1dc S100 S205 S205 S30a M S203 S203 S203 S203 S30e S301 M S205 S100 M S301 S100 S265 M S22f S205 S15a S22f S15a S205 P S389 M S179 S179 S2c3 S2c3 S30a P S387 M S15a S15a S23d M S2ff S100 S100 S100 S22a M S1f5 S2ea S2ea S1f5 P S387 R S377 S203 S205 S203 S205 S377 S30e S301 R S1f5 S2ea S2ea S1f5 P S387 M S203 S203 S377 S22f S30e S301 M S1f7 S1f7 S26a M S181 S205 S205 S181 R S100 S265 R S100 S2ff P S387 M S100 S265 M S15a S15a S10e S10e P S388
2021-11-23 15:41:26,840 - INFO - joeynmt.training - 	Reference:  ▁Verse ▁2. ▁Then ▁make ▁me ▁tr ul y ▁hap p y ▁by ▁ag ree ing ▁whole he art ed ly ▁with ▁each ▁other , ▁lo v ing ▁one ▁another , ▁and ▁work ing ▁together ▁with ▁one ▁m ind ▁and ▁p ur p ose .
2021-11-23 15:41:26,840 - INFO - joeynmt.training - 	Hypothesis: ▁Verse ▁2. ▁" I ▁am ▁you ▁are ▁you , ▁" I ▁am ▁you ▁will ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be ▁be .
2021-11-23 15:41:26,840 - INFO - joeynmt.training - Example #3
2021-11-23 15:41:26,841 - DEBUG - joeynmt.training - 	Raw source:     ['<2pt>', '<4br>', '<dict>', 'M', 'S10e40', 'S11040', 'S26504', 'S2fb04']
2021-11-23 15:41:26,841 - DEBUG - joeynmt.training - 	Raw factor:     ['<2pt>', '<4br>', '<dict>', 'M', 'S10e', 'S110', 'S265', 'S2fb']
2021-11-23 15:41:26,841 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁c', 'at', 'a']
2021-11-23 15:41:26,841 - INFO - joeynmt.training - 	Source:     <2pt> <4br> <dict> M S10e40 S11040 S26504 S2fb04
2021-11-23 15:41:26,841 - INFO - joeynmt.training - 	Factor:     <2pt> <4br> <dict> M S10e S110 S265 S2fb
2021-11-23 15:41:26,841 - INFO - joeynmt.training - 	Reference:  ▁expl or ar
2021-11-23 15:41:26,841 - INFO - joeynmt.training - 	Hypothesis: ▁c at a
2021-11-23 15:41:26,841 - INFO - joeynmt.training - Example #6
2021-11-23 15:41:26,841 - DEBUG - joeynmt.training - 	Raw source:     ['<2en>', '<4us>', '<dict>', 'M', 'S20340', 'S21800']
2021-11-23 15:41:26,841 - DEBUG - joeynmt.training - 	Raw factor:     ['<2en>', '<4us>', '<dict>', 'M', 'S203', 'S218']
2021-11-23 15:41:26,841 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁b', 'at']
2021-11-23 15:41:26,841 - INFO - joeynmt.training - 	Source:     <2en> <4us> <dict> M S20340 S21800
2021-11-23 15:41:26,841 - INFO - joeynmt.training - 	Factor:     <2en> <4us> <dict> M S203 S218
2021-11-23 15:41:26,841 - INFO - joeynmt.training - 	Reference:  ▁m il k
2021-11-23 15:41:26,842 - INFO - joeynmt.training - 	Hypothesis: ▁b at
2021-11-23 15:41:26,842 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step     7200: chrf:   0.11, loss: 121592.0859, ppl:  36.2443, duration: 105.5928s
2021-11-23 15:41:28,784 - INFO - joeynmt.training - Epoch   2: total training loss 13411.04
2021-11-23 15:41:28,784 - INFO - joeynmt.training - EPOCH 3
2021-11-23 15:42:12,481 - INFO - joeynmt.training - Epoch   3, Step:     7560, Batch Loss:     3.476243, Tokens per Sec:     2277, Lr: 0.000100
2021-11-23 15:42:57,881 - INFO - joeynmt.training - Epoch   3, Step:     7920, Batch Loss:     3.336725, Tokens per Sec:     2267, Lr: 0.000100
2021-11-23 15:43:43,743 - INFO - joeynmt.training - Epoch   3, Step:     8280, Batch Loss:     3.667073, Tokens per Sec:     2297, Lr: 0.000100
2021-11-23 15:44:29,147 - INFO - joeynmt.training - Epoch   3, Step:     8640, Batch Loss:     3.584517, Tokens per Sec:     2229, Lr: 0.000100
2021-11-23 15:45:14,761 - INFO - joeynmt.training - Epoch   3, Step:     9000, Batch Loss:     3.558795, Tokens per Sec:     2292, Lr: 0.000100
2021-11-23 15:46:00,186 - INFO - joeynmt.training - Epoch   3, Step:     9360, Batch Loss:     3.628007, Tokens per Sec:     2284, Lr: 0.000100
2021-11-23 15:46:45,466 - INFO - joeynmt.training - Epoch   3, Step:     9720, Batch Loss:     3.576128, Tokens per Sec:     2276, Lr: 0.000100
2021-11-23 15:47:31,209 - INFO - joeynmt.training - Epoch   3, Step:    10080, Batch Loss:     3.621135, Tokens per Sec:     2302, Lr: 0.000100
2021-11-23 15:48:16,316 - INFO - joeynmt.training - Epoch   3, Step:    10440, Batch Loss:     3.529536, Tokens per Sec:     2270, Lr: 0.000100
2021-11-23 15:49:01,693 - INFO - joeynmt.training - Epoch   3, Step:    10800, Batch Loss:     3.413318, Tokens per Sec:     2266, Lr: 0.000100
2021-11-23 15:52:08,933 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-11-23 15:52:10,359 - INFO - joeynmt.helpers - delete models/baseline_multilingual_mem/7200.ckpt
2021-11-23 15:52:10,361 - INFO - joeynmt.helpers - delete /net/cephfs/home/zifjia/signwriting-translation/models/baseline_multilingual_mem/7200.ckpt
2021-11-23 15:52:10,361 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /net/cephfs/home/zifjia/signwriting-translation/models/baseline_multilingual_mem/7200.ckpt but file does not exist. ([Errno 2] No such file or directory: '/net/cephfs/home/zifjia/signwriting-translation/models/baseline_multilingual_mem/7200.ckpt')
2021-11-23 15:52:10,368 - INFO - joeynmt.training - Example #0
2021-11-23 15:52:10,368 - DEBUG - joeynmt.training - 	Raw source:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a07', 'S1f010', 'S26507', 'M', 'S11e20', 'S1a520', 'P', 'S38800', 'M', 'S15a56', 'S15a41', 'S2b700', 'S37c06', 'S36d01', 'S30d00', 'M', 'S10040', 'P', 'S38700', 'M', 'S37806', 'S10050', 'S20356', 'S36d01', 'M', 'S15a37', 'S1ce51', 'S26a07', 'P', 'S38700', 'M', 'S19220', 'S2a20c', 'S11520', 'S10120', 'S1f720', 'S20320', 'S36d01', 'M', 'S10018', 'S26505', 'S10641', 'M', 'S1f000', 'S1f720', 'S1dc20', 'S19220', 'S14a20', 'S1dc20', 'S14a20', 'P', 'S38800', 'M', 'S10041', 'S30d00', 'S36d01', 'M', 'S15a31', 'S10018', 'S26503', 'M', 'S2e74c', 'S14220', 'S2e700', 'S14228', 'M', 'S26521', 'S1f548', 'S1f540', 'M', 'S10041', 'P', 'S38700', 'M', 'S10021', 'S10029', 'S22a07', 'S22a11', 'S36d01', 'M', 'S2ea3e', 'S10000', 'M', 'S15a18', 'S10051', 'S26601', 'M', 'S10058', 'S26526', 'S10050', 'S20500', 'S20500', 'P', 'S38900', 'M', 'S15a37', 'S15a01', 'S2e900', 'S36d01', 'M', 'S15a20', 'S26501', 'M', 'S26521', 'S23b0a', 'S1f548', 'S15a40', 'S15a48', 'S23b1a', 'S1f540', 'M', 'S14c57', 'S14c5f', 'S26511', 'S26507', 'S22527', 'S20314', 'S22521', 'S2031c', 'P', 'S38800']
2021-11-23 15:52:10,368 - DEBUG - joeynmt.training - 	Raw factor:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a', 'S1f0', 'S265', 'M', 'S11e', 'S1a5', 'P', 'S388', 'M', 'S15a', 'S15a', 'S2b7', 'S37c', 'S36d', 'S30d', 'M', 'S100', 'P', 'S387', 'M', 'S378', 'S100', 'S203', 'S36d', 'M', 'S15a', 'S1ce', 'S26a', 'P', 'S387', 'M', 'S192', 'S2a2', 'S115', 'S101', 'S1f7', 'S203', 'S36d', 'M', 'S100', 'S265', 'S106', 'M', 'S1f0', 'S1f7', 'S1dc', 'S192', 'S14a', 'S1dc', 'S14a', 'P', 'S388', 'M', 'S100', 'S30d', 'S36d', 'M', 'S15a', 'S100', 'S265', 'M', 'S2e7', 'S142', 'S2e7', 'S142', 'M', 'S265', 'S1f5', 'S1f5', 'M', 'S100', 'P', 'S387', 'M', 'S100', 'S100', 'S22a', 'S22a', 'S36d', 'M', 'S2ea', 'S100', 'M', 'S15a', 'S100', 'S266', 'M', 'S100', 'S265', 'S100', 'S205', 'S205', 'P', 'S389', 'M', 'S15a', 'S15a', 'S2e9', 'S36d', 'M', 'S15a', 'S265', 'M', 'S265', 'S23b', 'S1f5', 'S15a', 'S15a', 'S23b', 'S1f5', 'M', 'S14c', 'S14c', 'S265', 'S265', 'S225', 'S203', 'S225', 'S203', 'P', 'S388']
2021-11-23 15:52:10,368 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁Verse', '▁3', '0.', '▁"', 'I', '▁will', '▁be', '▁be', '▁a', '▁man', ',', '▁and', '▁the', '▁king', '▁of', '▁the', '▁king', "'", 's', '▁of', '▁the', '▁king', "'", 's', '▁of', '▁the', '▁king', "'", 's', '▁of', '▁the', '▁king', "'", 's', '▁of', '▁the', '▁king', "'", 's', '▁of', '▁the', '▁king', "'", 's', '▁of', '▁the', '▁king', "'", 's', '▁of', '▁the', '▁king', "'", 's', '."']
2021-11-23 15:52:10,368 - INFO - joeynmt.training - 	Source:     <2en> <4us> <sent> M S15a07 S1f010 S26507 M S11e20 S1a520 P S38800 M S15a56 S15a41 S2b700 S37c06 S36d01 S30d00 M S10040 P S38700 M S37806 S10050 S20356 S36d01 M S15a37 S1ce51 S26a07 P S38700 M S19220 S2a20c S11520 S10120 S1f720 S20320 S36d01 M S10018 S26505 S10641 M S1f000 S1f720 S1dc20 S19220 S14a20 S1dc20 S14a20 P S38800 M S10041 S30d00 S36d01 M S15a31 S10018 S26503 M S2e74c S14220 S2e700 S14228 M S26521 S1f548 S1f540 M S10041 P S38700 M S10021 S10029 S22a07 S22a11 S36d01 M S2ea3e S10000 M S15a18 S10051 S26601 M S10058 S26526 S10050 S20500 S20500 P S38900 M S15a37 S15a01 S2e900 S36d01 M S15a20 S26501 M S26521 S23b0a S1f548 S15a40 S15a48 S23b1a S1f540 M S14c57 S14c5f S26511 S26507 S22527 S20314 S22521 S2031c P S38800
2021-11-23 15:52:10,368 - INFO - joeynmt.training - 	Factor:     <2en> <4us> <sent> M S15a S1f0 S265 M S11e S1a5 P S388 M S15a S15a S2b7 S37c S36d S30d M S100 P S387 M S378 S100 S203 S36d M S15a S1ce S26a P S387 M S192 S2a2 S115 S101 S1f7 S203 S36d M S100 S265 S106 M S1f0 S1f7 S1dc S192 S14a S1dc S14a P S388 M S100 S30d S36d M S15a S100 S265 M S2e7 S142 S2e7 S142 M S265 S1f5 S1f5 M S100 P S387 M S100 S100 S22a S22a S36d M S2ea S100 M S15a S100 S266 M S100 S265 S100 S205 S205 P S389 M S15a S15a S2e9 S36d M S15a S265 M S265 S23b S1f5 S15a S15a S23b S1f5 M S14c S14c S265 S265 S225 S203 S225 S203 P S388
2021-11-23 15:52:10,368 - INFO - joeynmt.training - 	Reference:  ▁Verse ▁3 7. ▁After ▁him , ▁at ▁the ▁time ▁of ▁the ▁c ens us , ▁there ▁was ▁Jud as ▁of ▁Gal ile e . ▁He ▁g ot ▁people ▁to ▁follow ▁him , ▁but ▁he ▁was ▁k illed , ▁too , ▁and ▁all ▁his ▁follow ers ▁were ▁sc atter ed .
2021-11-23 15:52:10,368 - INFO - joeynmt.training - 	Hypothesis: ▁Verse ▁3 0. ▁" I ▁will ▁be ▁be ▁a ▁man , ▁and ▁the ▁king ▁of ▁the ▁king ' s ▁of ▁the ▁king ' s ▁of ▁the ▁king ' s ▁of ▁the ▁king ' s ▁of ▁the ▁king ' s ▁of ▁the ▁king ' s ▁of ▁the ▁king ' s ▁of ▁the ▁king ' s ."
2021-11-23 15:52:10,368 - INFO - joeynmt.training - Example #1
2021-11-23 15:52:10,368 - DEBUG - joeynmt.training - 	Raw source:     ['<2pt>', '<4br>', '<dict>', 'M', 'S34700', 'S21100', 'S1f410']
2021-11-23 15:52:10,368 - DEBUG - joeynmt.training - 	Raw factor:     ['<2pt>', '<4br>', '<dict>', 'M', 'S347', 'S211', 'S1f4']
2021-11-23 15:52:10,368 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁B', 'or']
2021-11-23 15:52:10,368 - INFO - joeynmt.training - 	Source:     <2pt> <4br> <dict> M S34700 S21100 S1f410
2021-11-23 15:52:10,368 - INFO - joeynmt.training - 	Factor:     <2pt> <4br> <dict> M S347 S211 S1f4
2021-11-23 15:52:10,368 - INFO - joeynmt.training - 	Reference:  ▁L á p is
2021-11-23 15:52:10,368 - INFO - joeynmt.training - 	Hypothesis: ▁B or
2021-11-23 15:52:10,368 - INFO - joeynmt.training - Example #2
2021-11-23 15:52:10,368 - DEBUG - joeynmt.training - 	Raw source:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a07', 'S1f010', 'S26507', 'M', 'S10e00', 'P', 'S38800', 'M', 'S1dc0a', 'S10003', 'S20500', 'S20500', 'S30a00', 'M', 'S20302', 'S2030a', 'S20340', 'S20348', 'S30e00', 'S30124', 'M', 'S20500', 'S10013', 'M', 'S30124', 'S10010', 'S26500', 'M', 'S22f10', 'S20500', 'S15a0a', 'S22f00', 'S15a02', 'S20500', 'P', 'S38900', 'M', 'S17911', 'S17919', 'S2c300', 'S2c311', 'S30a00', 'P', 'S38700', 'M', 'S15a06', 'S15a41', 'S23d04', 'M', 'S2ff00', 'S10050', 'S10058', 'S10011', 'S22a04', 'M', 'S1f522', 'S2ea46', 'S2ea02', 'S1f50a', 'P', 'S38700', 'R', 'S3770b', 'S20305', 'S20500', 'S20303', 'S20500', 'S37713', 'S30e00', 'S30124', 'R', 'S1f522', 'S2ea46', 'S2ea02', 'S1f50a', 'P', 'S38700', 'M', 'S20356', 'S20350', 'S37706', 'S22f04', 'S30e00', 'S30124', 'M', 'S1f740', 'S1f748', 'S26a20', 'M', 'S1814b', 'S20500', 'S20500', 'S18143', 'R', 'S10000', 'S26500', 'R', 'S10011', 'S2ff00', 'P', 'S38700', 'M', 'S10000', 'S26500', 'M', 'S15a18', 'S15a18', 'S10e22', 'S10e02', 'P', 'S38800']
2021-11-23 15:52:10,368 - DEBUG - joeynmt.training - 	Raw factor:     ['<2en>', '<4us>', '<sent>', 'M', 'S15a', 'S1f0', 'S265', 'M', 'S10e', 'P', 'S388', 'M', 'S1dc', 'S100', 'S205', 'S205', 'S30a', 'M', 'S203', 'S203', 'S203', 'S203', 'S30e', 'S301', 'M', 'S205', 'S100', 'M', 'S301', 'S100', 'S265', 'M', 'S22f', 'S205', 'S15a', 'S22f', 'S15a', 'S205', 'P', 'S389', 'M', 'S179', 'S179', 'S2c3', 'S2c3', 'S30a', 'P', 'S387', 'M', 'S15a', 'S15a', 'S23d', 'M', 'S2ff', 'S100', 'S100', 'S100', 'S22a', 'M', 'S1f5', 'S2ea', 'S2ea', 'S1f5', 'P', 'S387', 'R', 'S377', 'S203', 'S205', 'S203', 'S205', 'S377', 'S30e', 'S301', 'R', 'S1f5', 'S2ea', 'S2ea', 'S1f5', 'P', 'S387', 'M', 'S203', 'S203', 'S377', 'S22f', 'S30e', 'S301', 'M', 'S1f7', 'S1f7', 'S26a', 'M', 'S181', 'S205', 'S205', 'S181', 'R', 'S100', 'S265', 'R', 'S100', 'S2ff', 'P', 'S387', 'M', 'S100', 'S265', 'M', 'S15a', 'S15a', 'S10e', 'S10e', 'P', 'S388']
2021-11-23 15:52:10,368 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁Verse', '▁10.', '▁"', 'I', '▁am', '▁you', ',', '▁you', '▁have', '▁be', '▁be', '▁not', '▁not', '▁not', '▁not', '▁not', '▁not', '▁not', '▁not', '▁not', '▁not', '▁not', '▁not', '▁not', '▁not', '▁not', '.']
2021-11-23 15:52:10,368 - INFO - joeynmt.training - 	Source:     <2en> <4us> <sent> M S15a07 S1f010 S26507 M S10e00 P S38800 M S1dc0a S10003 S20500 S20500 S30a00 M S20302 S2030a S20340 S20348 S30e00 S30124 M S20500 S10013 M S30124 S10010 S26500 M S22f10 S20500 S15a0a S22f00 S15a02 S20500 P S38900 M S17911 S17919 S2c300 S2c311 S30a00 P S38700 M S15a06 S15a41 S23d04 M S2ff00 S10050 S10058 S10011 S22a04 M S1f522 S2ea46 S2ea02 S1f50a P S38700 R S3770b S20305 S20500 S20303 S20500 S37713 S30e00 S30124 R S1f522 S2ea46 S2ea02 S1f50a P S38700 M S20356 S20350 S37706 S22f04 S30e00 S30124 M S1f740 S1f748 S26a20 M S1814b S20500 S20500 S18143 R S10000 S26500 R S10011 S2ff00 P S38700 M S10000 S26500 M S15a18 S15a18 S10e22 S10e02 P S38800
2021-11-23 15:52:10,368 - INFO - joeynmt.training - 	Factor:     <2en> <4us> <sent> M S15a S1f0 S265 M S10e P S388 M S1dc S100 S205 S205 S30a M S203 S203 S203 S203 S30e S301 M S205 S100 M S301 S100 S265 M S22f S205 S15a S22f S15a S205 P S389 M S179 S179 S2c3 S2c3 S30a P S387 M S15a S15a S23d M S2ff S100 S100 S100 S22a M S1f5 S2ea S2ea S1f5 P S387 R S377 S203 S205 S203 S205 S377 S30e S301 R S1f5 S2ea S2ea S1f5 P S387 M S203 S203 S377 S22f S30e S301 M S1f7 S1f7 S26a M S181 S205 S205 S181 R S100 S265 R S100 S2ff P S387 M S100 S265 M S15a S15a S10e S10e P S388
2021-11-23 15:52:10,368 - INFO - joeynmt.training - 	Reference:  ▁Verse ▁2. ▁Then ▁make ▁me ▁tr ul y ▁hap p y ▁by ▁ag ree ing ▁whole he art ed ly ▁with ▁each ▁other , ▁lo v ing ▁one ▁another , ▁and ▁work ing ▁together ▁with ▁one ▁m ind ▁and ▁p ur p ose .
2021-11-23 15:52:10,369 - INFO - joeynmt.training - 	Hypothesis: ▁Verse ▁10. ▁" I ▁am ▁you , ▁you ▁have ▁be ▁be ▁not ▁not ▁not ▁not ▁not ▁not ▁not ▁not ▁not ▁not ▁not ▁not ▁not ▁not ▁not .
2021-11-23 15:52:10,369 - INFO - joeynmt.training - Example #3
2021-11-23 15:52:10,369 - DEBUG - joeynmt.training - 	Raw source:     ['<2pt>', '<4br>', '<dict>', 'M', 'S10e40', 'S11040', 'S26504', 'S2fb04']
2021-11-23 15:52:10,369 - DEBUG - joeynmt.training - 	Raw factor:     ['<2pt>', '<4br>', '<dict>', 'M', 'S10e', 'S110', 'S265', 'S2fb']
2021-11-23 15:52:10,369 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁p', 'u']
2021-11-23 15:52:10,369 - INFO - joeynmt.training - 	Source:     <2pt> <4br> <dict> M S10e40 S11040 S26504 S2fb04
2021-11-23 15:52:10,369 - INFO - joeynmt.training - 	Factor:     <2pt> <4br> <dict> M S10e S110 S265 S2fb
2021-11-23 15:52:10,369 - INFO - joeynmt.training - 	Reference:  ▁expl or ar
2021-11-23 15:52:10,369 - INFO - joeynmt.training - 	Hypothesis: ▁p u
2021-11-23 15:52:10,369 - INFO - joeynmt.training - Example #6
2021-11-23 15:52:10,369 - DEBUG - joeynmt.training - 	Raw source:     ['<2en>', '<4us>', '<dict>', 'M', 'S20340', 'S21800']
2021-11-23 15:52:10,369 - DEBUG - joeynmt.training - 	Raw factor:     ['<2en>', '<4us>', '<dict>', 'M', 'S203', 'S218']
2021-11-23 15:52:10,369 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁p', 'an']
2021-11-23 15:52:10,369 - INFO - joeynmt.training - 	Source:     <2en> <4us> <dict> M S20340 S21800
2021-11-23 15:52:10,369 - INFO - joeynmt.training - 	Factor:     <2en> <4us> <dict> M S203 S218
2021-11-23 15:52:10,369 - INFO - joeynmt.training - 	Reference:  ▁m il k
2021-11-23 15:52:10,369 - INFO - joeynmt.training - 	Hypothesis: ▁p an
2021-11-23 15:52:10,369 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step    10800: chrf:   0.11, loss: 116524.0781, ppl:  31.2068, duration: 188.6760s
2021-11-23 15:52:13,338 - INFO - joeynmt.training - Epoch   3: total training loss 12751.70
2021-11-23 15:52:13,338 - INFO - joeynmt.training - EPOCH 4
2021-11-23 15:52:55,811 - INFO - joeynmt.training - Epoch   4, Step:    11160, Batch Loss:     3.493527, Tokens per Sec:     2286, Lr: 0.000100
2021-11-23 15:53:41,288 - INFO - joeynmt.training - Epoch   4, Step:    11520, Batch Loss:     3.504811, Tokens per Sec:     2273, Lr: 0.000100
2021-11-23 15:54:26,974 - INFO - joeynmt.training - Epoch   4, Step:    11880, Batch Loss:     3.449094, Tokens per Sec:     2276, Lr: 0.000100
2021-11-23 15:55:12,522 - INFO - joeynmt.training - Epoch   4, Step:    12240, Batch Loss:     3.219866, Tokens per Sec:     2292, Lr: 0.000100
2021-11-23 15:55:58,144 - INFO - joeynmt.training - Epoch   4, Step:    12600, Batch Loss:     3.320179, Tokens per Sec:     2271, Lr: 0.000100
2021-11-23 15:56:43,760 - INFO - joeynmt.training - Epoch   4, Step:    12960, Batch Loss:     3.312541, Tokens per Sec:     2271, Lr: 0.000100
2021-11-23 15:57:29,180 - INFO - joeynmt.training - Epoch   4, Step:    13320, Batch Loss:     3.466140, Tokens per Sec:     2263, Lr: 0.000100
2021-11-23 15:58:14,488 - INFO - joeynmt.training - Epoch   4, Step:    13680, Batch Loss:     3.118797, Tokens per Sec:     2232, Lr: 0.000100
2021-11-23 15:59:00,070 - INFO - joeynmt.training - Epoch   4, Step:    14040, Batch Loss:     3.462774, Tokens per Sec:     2268, Lr: 0.000100
2021-11-23 15:59:44,897 - INFO - joeynmt.training - Epoch   4, Step:    14400, Batch Loss:     3.307575, Tokens per Sec:     2307, Lr: 0.000100
