2021-10-13 20:09:41,564 - INFO - root - Hello! This is Joey-NMT (version 1.3).
2021-10-13 20:09:41,566 - INFO - joeynmt.data - Loading training data...
2021-10-13 20:09:42,221 - INFO - joeynmt.data - Building vocabulary...
2021-10-13 20:09:42,224 - INFO - joeynmt.data - Loading dev data...
2021-10-13 20:09:42,225 - INFO - joeynmt.data - Loading test data...
2021-10-13 20:09:42,225 - INFO - joeynmt.data - Data loaded.
2021-10-13 20:09:42,277 - INFO - joeynmt.model - Building an encoder-decoder model...
2021-10-13 20:09:42,296 - INFO - joeynmt.model - Enc-dec model built.
2021-10-13 20:09:45,011 - DEBUG - tensorflow - Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.
2021-10-13 20:09:45,371 - DEBUG - h5py._conv - Creating converter from 7 to 5
2021-10-13 20:09:45,371 - DEBUG - h5py._conv - Creating converter from 5 to 7
2021-10-13 20:09:45,371 - DEBUG - h5py._conv - Creating converter from 7 to 5
2021-10-13 20:09:45,371 - DEBUG - h5py._conv - Creating converter from 5 to 7
2021-10-13 20:09:46,168 - INFO - joeynmt.training - Total params: 56016
2021-10-13 20:09:46,169 - DEBUG - joeynmt.training - Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'encoder.layers.4.feed_forward.layer_norm.bias', 'encoder.layers.4.feed_forward.layer_norm.weight', 'encoder.layers.4.feed_forward.pwff_layer.0.bias', 'encoder.layers.4.feed_forward.pwff_layer.0.weight', 'encoder.layers.4.feed_forward.pwff_layer.3.bias', 'encoder.layers.4.feed_forward.pwff_layer.3.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.4.src_src_att.k_layer.bias', 'encoder.layers.4.src_src_att.k_layer.weight', 'encoder.layers.4.src_src_att.output_layer.bias', 'encoder.layers.4.src_src_att.output_layer.weight', 'encoder.layers.4.src_src_att.q_layer.bias', 'encoder.layers.4.src_src_att.q_layer.weight', 'encoder.layers.4.src_src_att.v_layer.bias', 'encoder.layers.4.src_src_att.v_layer.weight', 'encoder.layers.5.feed_forward.layer_norm.bias', 'encoder.layers.5.feed_forward.layer_norm.weight', 'encoder.layers.5.feed_forward.pwff_layer.0.bias', 'encoder.layers.5.feed_forward.pwff_layer.0.weight', 'encoder.layers.5.feed_forward.pwff_layer.3.bias', 'encoder.layers.5.feed_forward.pwff_layer.3.weight', 'encoder.layers.5.layer_norm.bias', 'encoder.layers.5.layer_norm.weight', 'encoder.layers.5.src_src_att.k_layer.bias', 'encoder.layers.5.src_src_att.k_layer.weight', 'encoder.layers.5.src_src_att.output_layer.bias', 'encoder.layers.5.src_src_att.output_layer.weight', 'encoder.layers.5.src_src_att.q_layer.bias', 'encoder.layers.5.src_src_att.q_layer.weight', 'encoder.layers.5.src_src_att.v_layer.bias', 'encoder.layers.5.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.lut.weight']
2021-10-13 20:09:46,171 - WARNING - joeynmt.training - `keep_last_ckpts` option is outdated. Please use `keep_best_ckpts`, instead.
2021-10-13 20:09:46,172 - INFO - joeynmt.helpers - cfg.name                           : baseline_transformer
2021-10-13 20:09:46,172 - INFO - joeynmt.helpers - cfg.data.src                       : sign
2021-10-13 20:09:46,172 - INFO - joeynmt.helpers - cfg.data.trg                       : en
2021-10-13 20:09:46,172 - INFO - joeynmt.helpers - cfg.data.train                     : data/train
2021-10-13 20:09:46,172 - INFO - joeynmt.helpers - cfg.data.dev                       : data/tiny
2021-10-13 20:09:46,172 - INFO - joeynmt.helpers - cfg.data.test                      : data/tiny
2021-10-13 20:09:46,172 - INFO - joeynmt.helpers - cfg.data.level                     : word
2021-10-13 20:09:46,172 - INFO - joeynmt.helpers - cfg.data.lowercase                 : True
2021-10-13 20:09:46,172 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 500
2021-10-13 20:09:46,172 - INFO - joeynmt.helpers - cfg.data.random_train_subset       : 10
2021-10-13 20:09:46,172 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5
2021-10-13 20:09:46,172 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0
2021-10-13 20:09:46,172 - INFO - joeynmt.helpers - cfg.testing.postprocess            : False
2021-10-13 20:09:46,172 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42
2021-10-13 20:09:46,172 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam
2021-10-13 20:09:46,177 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens
2021-10-13 20:09:46,177 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]
2021-10-13 20:09:46,178 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau
2021-10-13 20:09:46,178 - INFO - joeynmt.helpers - cfg.training.patience              : 5
2021-10-13 20:09:46,178 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7
2021-10-13 20:09:46,178 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy
2021-10-13 20:09:46,178 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0001
2021-10-13 20:09:46,178 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08
2021-10-13 20:09:46,178 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0
2021-10-13 20:09:46,178 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1
2021-10-13 20:09:46,178 - INFO - joeynmt.helpers - cfg.training.batch_size            : 2
2021-10-13 20:09:46,178 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 2
2021-10-13 20:09:46,178 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1
2021-10-13 20:09:46,178 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : loss
2021-10-13 20:09:46,178 - INFO - joeynmt.helpers - cfg.training.epochs                : 200
2021-10-13 20:09:46,179 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 1
2021-10-13 20:09:46,179 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 1
2021-10-13 20:09:46,179 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu
2021-10-13 20:09:46,179 - INFO - joeynmt.helpers - cfg.training.overwrite             : True
2021-10-13 20:09:46,179 - INFO - joeynmt.helpers - cfg.training.shuffle               : True
2021-10-13 20:09:46,179 - INFO - joeynmt.helpers - cfg.training.use_cuda              : False
2021-10-13 20:09:46,180 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 200
2021-10-13 20:09:46,180 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3, 6]
2021-10-13 20:09:46,180 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3
2021-10-13 20:09:46,180 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/baseline_transformer_debug
2021-10-13 20:09:46,180 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier
2021-10-13 20:09:46,180 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros
2021-10-13 20:09:46,180 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0
2021-10-13 20:09:46,180 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier
2021-10-13 20:09:46,180 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0
2021-10-13 20:09:46,180 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer
2021-10-13 20:09:46,180 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6
2021-10-13 20:09:46,180 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 8
2021-10-13 20:09:46,180 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 16
2021-10-13 20:09:46,180 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True
2021-10-13 20:09:46,180 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.0
2021-10-13 20:09:46,181 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 16
2021-10-13 20:09:46,181 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 64
2021-10-13 20:09:46,181 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.1
2021-10-13 20:09:46,181 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer
2021-10-13 20:09:46,181 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6
2021-10-13 20:09:46,181 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 8
2021-10-13 20:09:46,181 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 16
2021-10-13 20:09:46,181 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True
2021-10-13 20:09:46,181 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.0
2021-10-13 20:09:46,181 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 16
2021-10-13 20:09:46,181 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 64
2021-10-13 20:09:46,181 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.1
2021-10-13 20:09:46,181 - INFO - joeynmt.helpers - Data set sizes: 
	train 10,
	valid 1,
	test 1
2021-10-13 20:09:46,181 - INFO - joeynmt.helpers - First training example:
	[SRC] s15a07 s1f010 s26507 s1bb20 s1f540 s38800 s10021 s10029 s22a07 s22a11 s36d01 s30a00 s26500 s1f540 s30122 s10000 s36b00 s1d410 s20800 s15a20 s2d60e s2ff00 s15a10 s2b700 s15a30 s15a50 s2a208 s2a218 s30146 s38810
	[TRG] verse 18 . but not a hair of your head will perish !
2021-10-13 20:09:46,181 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) s2ff00 (5) s20500 (6) s30a00 (7) s38700 (8) s38800 (9) s22a04
2021-10-13 20:09:46,182 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) of (7) the (8) a (9) god
2021-10-13 20:09:46,182 - INFO - joeynmt.helpers - Number of Src words (types): 295
2021-10-13 20:09:46,182 - INFO - joeynmt.helpers - Number of Trg words (types): 161
2021-10-13 20:09:46,182 - INFO - joeynmt.training - Model(
	encoder=TransformerEncoder(num_layers=6, num_heads=8),
	decoder=TransformerDecoder(num_layers=6, num_heads=8),
	src_embed=Embeddings(embedding_dim=16, vocab_size=295),
	trg_embed=Embeddings(embedding_dim=16, vocab_size=161))
2021-10-13 20:09:46,183 - INFO - joeynmt.training - Train stats:
	device: cpu
	n_gpu: 0
	16-bits training: False
	gradient accumulation: 1
	batch size per device: 2
	total batch size (w. parallel & accumulation): 2
2021-10-13 20:09:46,183 - INFO - joeynmt.training - EPOCH 1
2021-10-13 20:09:46,286 - INFO - joeynmt.training - Epoch   1, Step:        1, Batch Loss:     4.274466, Tokens per Sec:      517, Lr: 0.000100
2021-10-13 20:09:48,248 - INFO - joeynmt.training - Hooray! New best validation result [loss]!
2021-10-13 20:09:48,283 - INFO - joeynmt.training - Example #0
2021-10-13 20:09:48,283 - DEBUG - joeynmt.training - 	Raw source:     ['s15a07', 's1f010', 's26507', 's1f540', 's38800', 's32107', 's15a37', 's15a37', 's15a18', 's14c01', 's22a07', 's20301', 's20500', 's20500', 's10043', 's2d600', 's10018', 's26505', 's10641', 's15a30', 's15a50', 's2a208', 's2a218', 's1f710', 's20302', 's2ea08', 's38900', 's15a37', 's18250', 's28902', 's32107', 's15a37', 's15a37', 's2ff00', 's15a10', 's26500', 's15a18', 's14c01', 's22a07', 's20301', 's20500', 's20500', 's10043', 's2d600', 's38800', 's18217', 's1821f', 's20500', 's2d600', 's20500', 's30a00', 's20330', 's20330', 's15039', 's15031', 's26520', 's20500', 's20500', 's10043', 's2d600', 's18520', 's2b707', 's18528', 's2b717', 's32107', 's15a37', 's15a37', 's38900', 's32107', 's15a37', 's15a37', 's2ff00', 's15a10', 's26500', 's1f720', 's1f728', 's26520', 's15a18', 's14c01', 's22a07', 's20301', 's20500', 's20500', 's10043', 's2d600', 's38800']
2021-10-13 20:09:48,283 - DEBUG - joeynmt.training - 	Raw hypothesis: ['always', 'best', 'land', 'romans', 'romans', 'romans', 'you', 'belong', 'belong', 'belong', 'belong', 'belong', 'belong', 'belong', 'belong', 'belong', 'belong', 'belong', 'belong', 'belong', 'belong', 'belong', 'belong', 'belong', 'belong', 'belong', 'belong', 'belong', 'belong', 'belong', 'belong', 'belong', 'belong', 'belong', 'romans', 'romans', 'romans', 'romans', 'romans', 'best', 'price', 'romans', 'romans', 'romans', 'romans', 'price', 'romans', 'romans', 'romans', 'romans', 'jericho', 'best', 'price', 'romans', 'romans', 'romans', 'you', 'romans', 'romans', 'romans', 'romans', 'romans', 'you', 'best', 'price', 'romans', 'romans', 'you', 'romans', 'jericho', 'price', 'romans', 'you', 'romans', 'you', 'you', 'you', 'best', 'price', 'romans', 'you', 'best', 'price', 'price', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'you', 'you', 'he', 'be', 'be', 'ignorant']
2021-10-13 20:09:48,283 - INFO - joeynmt.training - 	Source:     s15a07 s1f010 s26507 s1f540 s38800 s32107 s15a37 s15a37 s15a18 s14c01 s22a07 s20301 s20500 s20500 s10043 s2d600 s10018 s26505 s10641 s15a30 s15a50 s2a208 s2a218 s1f710 s20302 s2ea08 s38900 s15a37 s18250 s28902 s32107 s15a37 s15a37 s2ff00 s15a10 s26500 s15a18 s14c01 s22a07 s20301 s20500 s20500 s10043 s2d600 s38800 s18217 s1821f s20500 s2d600 s20500 s30a00 s20330 s20330 s15039 s15031 s26520 s20500 s20500 s10043 s2d600 s18520 s2b707 s18528 s2b717 s32107 s15a37 s15a37 s38900 s32107 s15a37 s15a37 s2ff00 s15a10 s26500 s1f720 s1f728 s26520 s15a18 s14c01 s22a07 s20301 s20500 s20500 s10043 s2d600 s38800
2021-10-13 20:09:48,283 - INFO - joeynmt.training - 	Reference:  verse 10 . and he did rescue us from mortal danger , and he will rescue us again . we have placed our confidence in him , and he will continue to rescue us .
2021-10-13 20:09:48,283 - INFO - joeynmt.training - 	Hypothesis: always best land romans romans romans you belong belong belong belong belong belong belong belong belong belong belong belong belong belong belong belong belong belong belong belong belong belong belong belong belong belong belong romans romans romans romans romans best price romans romans romans romans price romans romans romans romans jericho best price romans romans romans you romans romans romans romans romans you best price romans romans you romans jericho price romans you romans you you you best price romans you best price price he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he you you he be be ignorant
2021-10-13 20:09:48,283 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step        1: bleu:   0.23, loss: 153.1292, ppl:  70.3575, duration: 1.9968s
2021-10-13 20:09:48,284 - INFO - joeynmt.training - []
2021-10-13 20:09:48,361 - INFO - joeynmt.training - Epoch   1, Step:        2, Batch Loss:     4.338610, Tokens per Sec:      802, Lr: 0.000100
2021-10-13 20:09:50,367 - INFO - joeynmt.training - Hooray! New best validation result [loss]!
2021-10-13 20:09:50,463 - INFO - joeynmt.training - Example #0
2021-10-13 20:09:50,463 - DEBUG - joeynmt.training - 	Raw source:     ['s15a07', 's1f010', 's26507', 's1f540', 's38800', 's32107', 's15a37', 's15a37', 's15a18', 's14c01', 's22a07', 's20301', 's20500', 's20500', 's10043', 's2d600', 's10018', 's26505', 's10641', 's15a30', 's15a50', 's2a208', 's2a218', 's1f710', 's20302', 's2ea08', 's38900', 's15a37', 's18250', 's28902', 's32107', 's15a37', 's15a37', 's2ff00', 's15a10', 's26500', 's15a18', 's14c01', 's22a07', 's20301', 's20500', 's20500', 's10043', 's2d600', 's38800', 's18217', 's1821f', 's20500', 's2d600', 's20500', 's30a00', 's20330', 's20330', 's15039', 's15031', 's26520', 's20500', 's20500', 's10043', 's2d600', 's18520', 's2b707', 's18528', 's2b717', 's32107', 's15a37', 's15a37', 's38900', 's32107', 's15a37', 's15a37', 's2ff00', 's15a10', 's26500', 's1f720', 's1f728', 's26520', 's15a18', 's14c01', 's22a07', 's20301', 's20500', 's20500', 's10043', 's2d600', 's38800']
2021-10-13 20:09:50,463 - DEBUG - joeynmt.training - 	Raw hypothesis: ['always', 'land', 'best', 'romans', 'romans', 'romans', 'jericho', 'best', 'gave', 'gave', 'gave', 'gave', 'you', 'you', 'romans', 'you', 'romans', 'you', 'you', 'belong', 'best', 'best', 'price', 'romans', 'jericho', 'price', 'price', 'price', 'romans', 'romans', 'jericho', 'price', 'romans', 'jericho', 'price', 'romans', 'romans', 'romans', 'jericho', 'price', 'best', 'price', 'romans', 'romans', 'jericho', 'price', 'romans', 'romans', 'romans', 'romans', 'jericho', 'price', 'romans', 'romans', 'romans', 'romans', 'romans', 'jericho', 'price', 'romans', 'romans', 'romans', 'jericho', 'price', 'romans', 'jericho', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'you', 'you', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'gave', 'gave', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave']
2021-10-13 20:09:50,463 - INFO - joeynmt.training - 	Source:     s15a07 s1f010 s26507 s1f540 s38800 s32107 s15a37 s15a37 s15a18 s14c01 s22a07 s20301 s20500 s20500 s10043 s2d600 s10018 s26505 s10641 s15a30 s15a50 s2a208 s2a218 s1f710 s20302 s2ea08 s38900 s15a37 s18250 s28902 s32107 s15a37 s15a37 s2ff00 s15a10 s26500 s15a18 s14c01 s22a07 s20301 s20500 s20500 s10043 s2d600 s38800 s18217 s1821f s20500 s2d600 s20500 s30a00 s20330 s20330 s15039 s15031 s26520 s20500 s20500 s10043 s2d600 s18520 s2b707 s18528 s2b717 s32107 s15a37 s15a37 s38900 s32107 s15a37 s15a37 s2ff00 s15a10 s26500 s1f720 s1f728 s26520 s15a18 s14c01 s22a07 s20301 s20500 s20500 s10043 s2d600 s38800
2021-10-13 20:09:50,463 - INFO - joeynmt.training - 	Reference:  verse 10 . and he did rescue us from mortal danger , and he will rescue us again . we have placed our confidence in him , and he will continue to rescue us .
2021-10-13 20:09:50,463 - INFO - joeynmt.training - 	Hypothesis: always land best romans romans romans jericho best gave gave gave gave you you romans you romans you you belong best best price romans jericho price price price romans romans jericho price romans jericho price romans romans romans jericho price best price romans romans jericho price romans romans romans romans jericho price romans romans romans romans romans jericho price romans romans romans jericho price romans jericho he he he he he he he he you you he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he gave gave he he he he he he he he he he he he he he he he he he he he he he he he he he he he he gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave
2021-10-13 20:09:50,463 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step        2: bleu:   0.23, loss: 152.5383, ppl:  69.2120, duration: 2.1021s
2021-10-13 20:09:50,464 - INFO - joeynmt.training - []
2021-10-13 20:09:50,571 - INFO - joeynmt.training - Epoch   1, Step:        3, Batch Loss:     4.354711, Tokens per Sec:     1064, Lr: 0.000100
2021-10-13 20:09:52,334 - INFO - joeynmt.training - Hooray! New best validation result [loss]!
2021-10-13 20:09:52,360 - INFO - joeynmt.training - Example #0
2021-10-13 20:09:52,360 - DEBUG - joeynmt.training - 	Raw source:     ['s15a07', 's1f010', 's26507', 's1f540', 's38800', 's32107', 's15a37', 's15a37', 's15a18', 's14c01', 's22a07', 's20301', 's20500', 's20500', 's10043', 's2d600', 's10018', 's26505', 's10641', 's15a30', 's15a50', 's2a208', 's2a218', 's1f710', 's20302', 's2ea08', 's38900', 's15a37', 's18250', 's28902', 's32107', 's15a37', 's15a37', 's2ff00', 's15a10', 's26500', 's15a18', 's14c01', 's22a07', 's20301', 's20500', 's20500', 's10043', 's2d600', 's38800', 's18217', 's1821f', 's20500', 's2d600', 's20500', 's30a00', 's20330', 's20330', 's15039', 's15031', 's26520', 's20500', 's20500', 's10043', 's2d600', 's18520', 's2b707', 's18528', 's2b717', 's32107', 's15a37', 's15a37', 's38900', 's32107', 's15a37', 's15a37', 's2ff00', 's15a10', 's26500', 's1f720', 's1f728', 's26520', 's15a18', 's14c01', 's22a07', 's20301', 's20500', 's20500', 's10043', 's2d600', 's38800']
2021-10-13 20:09:52,360 - DEBUG - joeynmt.training - 	Raw hypothesis: ['always', 'land', 'romans', 'romans', 'romans', 'romans', 'jericho', 'price', 'gave', 'gave', 'gave', 'gave', 'you', 'you', 'romans', 'you', 'romans', 'you', 'you', 'you', 'best', 'price', 'romans', 'romans', 'jericho', 'price', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'gave', 'be', 'romans', 'romans', 'price', 'best', 'be', 'price', 'romans', 'jericho', 'price', 'gave', 'be', 'gave', 'gave', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'be', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave']
2021-10-13 20:09:52,360 - INFO - joeynmt.training - 	Source:     s15a07 s1f010 s26507 s1f540 s38800 s32107 s15a37 s15a37 s15a18 s14c01 s22a07 s20301 s20500 s20500 s10043 s2d600 s10018 s26505 s10641 s15a30 s15a50 s2a208 s2a218 s1f710 s20302 s2ea08 s38900 s15a37 s18250 s28902 s32107 s15a37 s15a37 s2ff00 s15a10 s26500 s15a18 s14c01 s22a07 s20301 s20500 s20500 s10043 s2d600 s38800 s18217 s1821f s20500 s2d600 s20500 s30a00 s20330 s20330 s15039 s15031 s26520 s20500 s20500 s10043 s2d600 s18520 s2b707 s18528 s2b717 s32107 s15a37 s15a37 s38900 s32107 s15a37 s15a37 s2ff00 s15a10 s26500 s1f720 s1f728 s26520 s15a18 s14c01 s22a07 s20301 s20500 s20500 s10043 s2d600 s38800
2021-10-13 20:09:52,360 - INFO - joeynmt.training - 	Reference:  verse 10 . and he did rescue us from mortal danger , and he will rescue us again . we have placed our confidence in him , and he will continue to rescue us .
2021-10-13 20:09:52,360 - INFO - joeynmt.training - 	Hypothesis: always land romans romans romans romans jericho price gave gave gave gave you you romans you romans you you you best price romans romans jericho price he he he he he he he he he he he he he he he he he he he he he he he he he he he he gave be romans romans price best be price romans jericho price gave be gave gave he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave and and and and and and and and be gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave
2021-10-13 20:09:52,360 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step        3: bleu:   0.28, loss: 151.9942, ppl:  68.1739, duration: 1.7895s
2021-10-13 20:09:52,361 - INFO - joeynmt.training - []
2021-10-13 20:09:52,419 - INFO - joeynmt.training - Epoch   1, Step:        4, Batch Loss:     4.309996, Tokens per Sec:      734, Lr: 0.000100
2021-10-13 20:09:54,148 - INFO - joeynmt.training - Hooray! New best validation result [loss]!
2021-10-13 20:09:54,174 - INFO - joeynmt.helpers - delete models/baseline_transformer_debug/1.ckpt
2021-10-13 20:09:54,175 - INFO - joeynmt.training - Example #0
2021-10-13 20:09:54,175 - DEBUG - joeynmt.training - 	Raw source:     ['s15a07', 's1f010', 's26507', 's1f540', 's38800', 's32107', 's15a37', 's15a37', 's15a18', 's14c01', 's22a07', 's20301', 's20500', 's20500', 's10043', 's2d600', 's10018', 's26505', 's10641', 's15a30', 's15a50', 's2a208', 's2a218', 's1f710', 's20302', 's2ea08', 's38900', 's15a37', 's18250', 's28902', 's32107', 's15a37', 's15a37', 's2ff00', 's15a10', 's26500', 's15a18', 's14c01', 's22a07', 's20301', 's20500', 's20500', 's10043', 's2d600', 's38800', 's18217', 's1821f', 's20500', 's2d600', 's20500', 's30a00', 's20330', 's20330', 's15039', 's15031', 's26520', 's20500', 's20500', 's10043', 's2d600', 's18520', 's2b707', 's18528', 's2b717', 's32107', 's15a37', 's15a37', 's38900', 's32107', 's15a37', 's15a37', 's2ff00', 's15a10', 's26500', 's1f720', 's1f728', 's26520', 's15a18', 's14c01', 's22a07', 's20301', 's20500', 's20500', 's10043', 's2d600', 's38800']
2021-10-13 20:09:54,175 - DEBUG - joeynmt.training - 	Raw hypothesis: ['always', 'land', 'romans', 'romans', 'romans', 'romans', 'jericho', 'price', 'gave', 'gave', 'gave', 'gave', 'you', 'you', 'romans', 'you', 'romans', 'you', 'you', 'you', 'best', 'price', 'romans', 'romans', 'jericho', 'price', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'gave', 'be', 'romans', 'romans', 'price', 'best', 'be', 'be', 'price', 'gave', 'he', 'he', 'gave', 'gave', 'gave', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'gave', 'gave', 'he', 'he', 'he', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave']
2021-10-13 20:09:54,175 - INFO - joeynmt.training - 	Source:     s15a07 s1f010 s26507 s1f540 s38800 s32107 s15a37 s15a37 s15a18 s14c01 s22a07 s20301 s20500 s20500 s10043 s2d600 s10018 s26505 s10641 s15a30 s15a50 s2a208 s2a218 s1f710 s20302 s2ea08 s38900 s15a37 s18250 s28902 s32107 s15a37 s15a37 s2ff00 s15a10 s26500 s15a18 s14c01 s22a07 s20301 s20500 s20500 s10043 s2d600 s38800 s18217 s1821f s20500 s2d600 s20500 s30a00 s20330 s20330 s15039 s15031 s26520 s20500 s20500 s10043 s2d600 s18520 s2b707 s18528 s2b717 s32107 s15a37 s15a37 s38900 s32107 s15a37 s15a37 s2ff00 s15a10 s26500 s1f720 s1f728 s26520 s15a18 s14c01 s22a07 s20301 s20500 s20500 s10043 s2d600 s38800
2021-10-13 20:09:54,175 - INFO - joeynmt.training - 	Reference:  verse 10 . and he did rescue us from mortal danger , and he will rescue us again . we have placed our confidence in him , and he will continue to rescue us .
2021-10-13 20:09:54,175 - INFO - joeynmt.training - 	Hypothesis: always land romans romans romans romans jericho price gave gave gave gave you you romans you romans you you you best price romans romans jericho price he he he he he he he he he he he he he he he he he he he he he he he he he he he he gave be romans romans price best be be price gave he he gave gave gave he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he gave gave he he he gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave
2021-10-13 20:09:54,175 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step        4: bleu:   0.23, loss: 151.3136, ppl:  66.8970, duration: 1.7561s
2021-10-13 20:09:54,176 - INFO - joeynmt.training - []
2021-10-13 20:09:54,226 - INFO - joeynmt.training - Epoch   1, Step:        5, Batch Loss:     4.426696, Tokens per Sec:      756, Lr: 0.000100
2021-10-13 20:09:56,138 - INFO - joeynmt.training - Hooray! New best validation result [loss]!
2021-10-13 20:09:56,167 - INFO - joeynmt.helpers - delete models/baseline_transformer_debug/2.ckpt
2021-10-13 20:09:56,168 - INFO - joeynmt.training - Example #0
2021-10-13 20:09:56,168 - DEBUG - joeynmt.training - 	Raw source:     ['s15a07', 's1f010', 's26507', 's1f540', 's38800', 's32107', 's15a37', 's15a37', 's15a18', 's14c01', 's22a07', 's20301', 's20500', 's20500', 's10043', 's2d600', 's10018', 's26505', 's10641', 's15a30', 's15a50', 's2a208', 's2a218', 's1f710', 's20302', 's2ea08', 's38900', 's15a37', 's18250', 's28902', 's32107', 's15a37', 's15a37', 's2ff00', 's15a10', 's26500', 's15a18', 's14c01', 's22a07', 's20301', 's20500', 's20500', 's10043', 's2d600', 's38800', 's18217', 's1821f', 's20500', 's2d600', 's20500', 's30a00', 's20330', 's20330', 's15039', 's15031', 's26520', 's20500', 's20500', 's10043', 's2d600', 's18520', 's2b707', 's18528', 's2b717', 's32107', 's15a37', 's15a37', 's38900', 's32107', 's15a37', 's15a37', 's2ff00', 's15a10', 's26500', 's1f720', 's1f728', 's26520', 's15a18', 's14c01', 's22a07', 's20301', 's20500', 's20500', 's10043', 's2d600', 's38800']
2021-10-13 20:09:56,168 - DEBUG - joeynmt.training - 	Raw hypothesis: ['always', 'land', 'romans', 'romans', 'romans', 'romans', 'jericho', 'price', 'gave', 'gave', 'gave', 'gave', 'gave', 'he', 'gave', 'gave', 'gave', 'gave', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'be', 'be', 'price', 'price', 'price', 'price', 'gave', 'be', 'be', 'gave', 'he', 'he', 'he', 'gave', 'gave', 'gave', 'gave', 'he', 'he', 'be', 'be', 'be', 'price', 'gave', 'be', 'be', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'gave', 'gave', 'he', 'he', 'he', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'he', 'he', 'he', 'he', 'gave', 'gave', 'he', 'he', 'he', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave', 'gave']
2021-10-13 20:09:56,168 - INFO - joeynmt.training - 	Source:     s15a07 s1f010 s26507 s1f540 s38800 s32107 s15a37 s15a37 s15a18 s14c01 s22a07 s20301 s20500 s20500 s10043 s2d600 s10018 s26505 s10641 s15a30 s15a50 s2a208 s2a218 s1f710 s20302 s2ea08 s38900 s15a37 s18250 s28902 s32107 s15a37 s15a37 s2ff00 s15a10 s26500 s15a18 s14c01 s22a07 s20301 s20500 s20500 s10043 s2d600 s38800 s18217 s1821f s20500 s2d600 s20500 s30a00 s20330 s20330 s15039 s15031 s26520 s20500 s20500 s10043 s2d600 s18520 s2b707 s18528 s2b717 s32107 s15a37 s15a37 s38900 s32107 s15a37 s15a37 s2ff00 s15a10 s26500 s1f720 s1f728 s26520 s15a18 s14c01 s22a07 s20301 s20500 s20500 s10043 s2d600 s38800
2021-10-13 20:09:56,168 - INFO - joeynmt.training - 	Reference:  verse 10 . and he did rescue us from mortal danger , and he will rescue us again . we have placed our confidence in him , and he will continue to rescue us .
2021-10-13 20:09:56,169 - INFO - joeynmt.training - 	Hypothesis: always land romans romans romans romans jericho price gave gave gave gave gave he gave gave gave gave he he he he he he he he he he he he he he he he he he he he he he be be price price price price gave be be gave he he he gave gave gave gave he he be be be price gave be be gave gave gave gave gave gave gave gave gave he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he he gave gave he he he gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave he he he he gave gave he he he gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave gave
2021-10-13 20:09:56,169 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step        5: bleu:   0.23, loss: 150.8700, ppl:  66.0779, duration: 1.9427s
2021-10-13 20:09:56,169 - INFO - joeynmt.training - []
2021-10-13 20:09:56,169 - INFO - joeynmt.training - Epoch   1: total training loss 21.70
2021-10-13 20:09:56,170 - INFO - joeynmt.training - EPOCH 2
2021-10-13 20:09:56,235 - INFO - joeynmt.training - Epoch   2, Step:        6, Batch Loss:     4.391980, Tokens per Sec:      609, Lr: 0.000100
