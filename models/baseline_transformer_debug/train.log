2021-10-18 17:04:41,900 - INFO - root - Hello! This is Joey-NMT (version 1.3).
2021-10-18 17:04:41,901 - INFO - joeynmt.data - Loading training data...
2021-10-18 17:04:42,558 - INFO - joeynmt.data - Building vocabulary...
2021-10-18 17:04:42,561 - INFO - joeynmt.data - Loading dev data...
2021-10-18 17:04:42,562 - INFO - joeynmt.data - Loading test data...
2021-10-18 17:04:42,562 - INFO - joeynmt.data - Data loaded.
2021-10-18 17:04:42,623 - INFO - joeynmt.model - Building an encoder-decoder model...
2021-10-18 17:04:42,643 - INFO - joeynmt.model - Enc-dec model built.
2021-10-18 17:04:44,863 - DEBUG - tensorflow - Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.
2021-10-18 17:04:45,042 - DEBUG - h5py._conv - Creating converter from 7 to 5
2021-10-18 17:04:45,042 - DEBUG - h5py._conv - Creating converter from 5 to 7
2021-10-18 17:04:45,042 - DEBUG - h5py._conv - Creating converter from 7 to 5
2021-10-18 17:04:45,042 - DEBUG - h5py._conv - Creating converter from 5 to 7
2021-10-18 17:04:45,445 - INFO - joeynmt.training - Total params: 58128
2021-10-18 17:04:45,446 - DEBUG - joeynmt.training - Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'encoder.layers.4.feed_forward.layer_norm.bias', 'encoder.layers.4.feed_forward.layer_norm.weight', 'encoder.layers.4.feed_forward.pwff_layer.0.bias', 'encoder.layers.4.feed_forward.pwff_layer.0.weight', 'encoder.layers.4.feed_forward.pwff_layer.3.bias', 'encoder.layers.4.feed_forward.pwff_layer.3.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.4.src_src_att.k_layer.bias', 'encoder.layers.4.src_src_att.k_layer.weight', 'encoder.layers.4.src_src_att.output_layer.bias', 'encoder.layers.4.src_src_att.output_layer.weight', 'encoder.layers.4.src_src_att.q_layer.bias', 'encoder.layers.4.src_src_att.q_layer.weight', 'encoder.layers.4.src_src_att.v_layer.bias', 'encoder.layers.4.src_src_att.v_layer.weight', 'encoder.layers.5.feed_forward.layer_norm.bias', 'encoder.layers.5.feed_forward.layer_norm.weight', 'encoder.layers.5.feed_forward.pwff_layer.0.bias', 'encoder.layers.5.feed_forward.pwff_layer.0.weight', 'encoder.layers.5.feed_forward.pwff_layer.3.bias', 'encoder.layers.5.feed_forward.pwff_layer.3.weight', 'encoder.layers.5.layer_norm.bias', 'encoder.layers.5.layer_norm.weight', 'encoder.layers.5.src_src_att.k_layer.bias', 'encoder.layers.5.src_src_att.k_layer.weight', 'encoder.layers.5.src_src_att.output_layer.bias', 'encoder.layers.5.src_src_att.output_layer.weight', 'encoder.layers.5.src_src_att.q_layer.bias', 'encoder.layers.5.src_src_att.q_layer.weight', 'encoder.layers.5.src_src_att.v_layer.bias', 'encoder.layers.5.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.lut.weight']
2021-10-18 17:04:45,447 - WARNING - joeynmt.training - `keep_last_ckpts` option is outdated. Please use `keep_best_ckpts`, instead.
2021-10-18 17:04:45,448 - INFO - joeynmt.helpers - cfg.name                           : baseline_transformer
2021-10-18 17:04:45,448 - INFO - joeynmt.helpers - cfg.data.src                       : sign
2021-10-18 17:04:45,449 - INFO - joeynmt.helpers - cfg.data.trg                       : spm.en
2021-10-18 17:04:45,449 - INFO - joeynmt.helpers - cfg.data.train                     : data/train
2021-10-18 17:04:45,449 - INFO - joeynmt.helpers - cfg.data.dev                       : data/tiny
2021-10-18 17:04:45,449 - INFO - joeynmt.helpers - cfg.data.test                      : data/tiny
2021-10-18 17:04:45,449 - INFO - joeynmt.helpers - cfg.data.level                     : word
2021-10-18 17:04:45,449 - INFO - joeynmt.helpers - cfg.data.lowercase                 : True
2021-10-18 17:04:45,449 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 500
2021-10-18 17:04:45,449 - INFO - joeynmt.helpers - cfg.data.random_train_subset       : 10
2021-10-18 17:04:45,449 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5
2021-10-18 17:04:45,449 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0
2021-10-18 17:04:45,450 - INFO - joeynmt.helpers - cfg.testing.postprocess            : False
2021-10-18 17:04:45,450 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42
2021-10-18 17:04:45,453 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam
2021-10-18 17:04:45,453 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens
2021-10-18 17:04:45,453 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]
2021-10-18 17:04:45,453 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau
2021-10-18 17:04:45,453 - INFO - joeynmt.helpers - cfg.training.patience              : 5
2021-10-18 17:04:45,453 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7
2021-10-18 17:04:45,453 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy
2021-10-18 17:04:45,453 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0001
2021-10-18 17:04:45,454 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08
2021-10-18 17:04:45,454 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0
2021-10-18 17:04:45,454 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1
2021-10-18 17:04:45,454 - INFO - joeynmt.helpers - cfg.training.batch_size            : 2
2021-10-18 17:04:45,454 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 2
2021-10-18 17:04:45,454 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1
2021-10-18 17:04:45,454 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : loss
2021-10-18 17:04:45,454 - INFO - joeynmt.helpers - cfg.training.epochs                : 2
2021-10-18 17:04:45,454 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 1
2021-10-18 17:04:45,454 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 1
2021-10-18 17:04:45,455 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu
2021-10-18 17:04:45,455 - INFO - joeynmt.helpers - cfg.training.overwrite             : True
2021-10-18 17:04:45,455 - INFO - joeynmt.helpers - cfg.training.shuffle               : True
2021-10-18 17:04:45,455 - INFO - joeynmt.helpers - cfg.training.use_cuda              : False
2021-10-18 17:04:45,456 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 200
2021-10-18 17:04:45,456 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3, 6]
2021-10-18 17:04:45,456 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3
2021-10-18 17:04:45,456 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/baseline_transformer_debug
2021-10-18 17:04:45,456 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier
2021-10-18 17:04:45,456 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros
2021-10-18 17:04:45,457 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0
2021-10-18 17:04:45,457 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier
2021-10-18 17:04:45,457 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0
2021-10-18 17:04:45,457 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer
2021-10-18 17:04:45,457 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6
2021-10-18 17:04:45,457 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 8
2021-10-18 17:04:45,457 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 16
2021-10-18 17:04:45,457 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True
2021-10-18 17:04:45,457 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.0
2021-10-18 17:04:45,457 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 16
2021-10-18 17:04:45,458 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 64
2021-10-18 17:04:45,458 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.1
2021-10-18 17:04:45,458 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer
2021-10-18 17:04:45,458 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6
2021-10-18 17:04:45,458 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 8
2021-10-18 17:04:45,458 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 16
2021-10-18 17:04:45,458 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True
2021-10-18 17:04:45,458 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.0
2021-10-18 17:04:45,458 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 16
2021-10-18 17:04:45,458 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 64
2021-10-18 17:04:45,458 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.1
2021-10-18 17:04:45,459 - INFO - joeynmt.helpers - Data set sizes: 
	train 10,
	valid 1,
	test 1
2021-10-18 17:04:45,459 - INFO - joeynmt.helpers - First training example:
	[SRC] s15a07 s1f010 s26507 s1bb20 s1f540 s38800 s10021 s10029 s22a07 s22a11 s36d01 s30a00 s26500 s1f540 s30122 s10000 s36b00 s1d410 s20800 s15a20 s2d60e s2ff00 s15a10 s2b700 s15a30 s15a50 s2a208 s2a218 s30146 s38810
	[TRG] ▁verse ▁18. ▁but ▁not ▁a ▁ha ir ▁of ▁your ▁head ▁will ▁per ish !
2021-10-18 17:04:45,459 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) s2ff00 (5) s20500 (6) s30a00 (7) s38700 (8) s38800 (9) s22a04
2021-10-18 17:04:45,459 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) ▁of (7) ▁the (8) ▁a (9) ▁god
2021-10-18 17:04:45,459 - INFO - joeynmt.helpers - Number of Src words (types): 295
2021-10-18 17:04:45,459 - INFO - joeynmt.helpers - Number of Trg words (types): 227
2021-10-18 17:04:45,459 - INFO - joeynmt.training - Model(
	encoder=TransformerEncoder(num_layers=6, num_heads=8),
	decoder=TransformerDecoder(num_layers=6, num_heads=8),
	src_embed=Embeddings(embedding_dim=16, vocab_size=295),
	trg_embed=Embeddings(embedding_dim=16, vocab_size=227))
2021-10-18 17:04:45,460 - INFO - joeynmt.training - Train stats:
	device: cpu
	n_gpu: 0
	16-bits training: False
	gradient accumulation: 1
	batch size per device: 2
	total batch size (w. parallel & accumulation): 2
2021-10-18 17:04:45,460 - INFO - joeynmt.training - EPOCH 1
2021-10-18 17:04:45,547 - INFO - joeynmt.training - Epoch   1, Step:        1, Batch Loss:     4.598906, Tokens per Sec:      697, Lr: 0.000100
2021-10-18 17:04:47,563 - INFO - joeynmt.training - Hooray! New best validation result [loss]!
2021-10-18 17:04:47,591 - INFO - joeynmt.training - Example #0
2021-10-18 17:04:47,591 - DEBUG - joeynmt.training - 	Raw source:     ['s15a07', 's1f010', 's26507', 's1f540', 's38800', 's32107', 's15a37', 's15a37', 's15a18', 's14c01', 's22a07', 's20301', 's20500', 's20500', 's10043', 's2d600', 's10018', 's26505', 's10641', 's15a30', 's15a50', 's2a208', 's2a218', 's1f710', 's20302', 's2ea08', 's38900', 's15a37', 's18250', 's28902', 's32107', 's15a37', 's15a37', 's2ff00', 's15a10', 's26500', 's15a18', 's14c01', 's22a07', 's20301', 's20500', 's20500', 's10043', 's2d600', 's38800', 's18217', 's1821f', 's20500', 's2d600', 's20500', 's30a00', 's20330', 's20330', 's15039', 's15031', 's26520', 's20500', 's20500', 's10043', 's2d600', 's18520', 's2b707', 's18528', 's2b717', 's32107', 's15a37', 's15a37', 's38900', 's32107', 's15a37', 's15a37', 's2ff00', 's15a10', 's26500', 's1f720', 's1f728', 's26520', 's15a18', 's14c01', 's22a07', 's20301', 's20500', 's20500', 's10043', 's2d600', 's38800']
2021-10-18 17:04:47,591 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁"', 'ent', '▁"', '▁"', '▁"', '▁rom', '▁rom', '▁rom', 'ant', 'ant', 'ant', 'ant', 'ant', 'ant', 'ant', 'ant', 'ant', 'ant', 'ant', 'ant', 'ant', 'ant', 'ant', 'ant', '▁come', '▁come', '▁come', '▁come', '▁come', '▁come', '▁come', '▁come', '▁come', '▁come', '▁fool', 'ant', 'ant', 'ant', '▁will', '.', '▁come', '▁come', '▁come', '▁come', '▁come', '▁come', '▁come', '▁come', '▁come', '▁come', '▁come', '▁come', '▁come', '▁come', '▁come', '▁come', '▁come', '▁come', '▁come', '▁3.', '▁3.', '▁3.', '▁3.', '▁come', '▁come', '▁3.', '▁3.', '▁3.', '▁come', '▁come', '▁come', '▁come', '▁come', '▁3.', '▁3.', '▁come', '▁come', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁come', '▁come', '▁come', '▁3.', '▁3.', '▁3.', '▁come', '.', '▁pri', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', '▁whole', '▁come', '.', '.', '.', '▁pri', '▁3.', '▁3.', '▁3.', '.', '▁pri', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', 't', '▁3.', '▁3.', '▁3.', '.', '.', '▁pri', ':', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', 'lf', '▁3.', 'lf', '▁3.', 'lf', '▁back', '▁back', '▁back', '▁3.', '▁3.', '▁3.', '▁back', '▁back', '▁back', '▁3.', ':', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', ':', 'lf', '▁back', '▁back', '▁back', '▁back', '▁3.', '▁whole', '▁3.', '.', '.', '▁pri', ':', ':', '▁3.', '▁3.', '▁3.', ':', 'lf', '▁3.', '▁3.', '▁3.', 'lf', 'lf', 'lf', 'lf', '▁3.', '▁3.', '.', 'lf', '▁3.', ':', ':']
2021-10-18 17:04:47,591 - INFO - joeynmt.training - 	Source:     s15a07 s1f010 s26507 s1f540 s38800 s32107 s15a37 s15a37 s15a18 s14c01 s22a07 s20301 s20500 s20500 s10043 s2d600 s10018 s26505 s10641 s15a30 s15a50 s2a208 s2a218 s1f710 s20302 s2ea08 s38900 s15a37 s18250 s28902 s32107 s15a37 s15a37 s2ff00 s15a10 s26500 s15a18 s14c01 s22a07 s20301 s20500 s20500 s10043 s2d600 s38800 s18217 s1821f s20500 s2d600 s20500 s30a00 s20330 s20330 s15039 s15031 s26520 s20500 s20500 s10043 s2d600 s18520 s2b707 s18528 s2b717 s32107 s15a37 s15a37 s38900 s32107 s15a37 s15a37 s2ff00 s15a10 s26500 s1f720 s1f728 s26520 s15a18 s14c01 s22a07 s20301 s20500 s20500 s10043 s2d600 s38800
2021-10-18 17:04:47,592 - INFO - joeynmt.training - 	Reference:  ▁proverbs ▁3 1 :4 ▁it ▁is ▁not ▁for ▁kings , ▁o ▁l em u el , ▁it ▁is ▁not ▁for ▁kings ▁to ▁drink ▁wine ; ▁nor ▁for ▁pr in ces ▁strong ▁drink \ n \ n o h ▁you ▁l em u el , ▁wine ▁kings ▁should ▁drink ▁not , ▁and ▁pr in ces ▁should ▁al c o h ol ▁drink ▁not .
2021-10-18 17:04:47,592 - INFO - joeynmt.training - 	Hypothesis: ▁" ent ▁" ▁" ▁" ▁rom ▁rom ▁rom ant ant ant ant ant ant ant ant ant ant ant ant ant ant ant ant ▁come ▁come ▁come ▁come ▁come ▁come ▁come ▁come ▁come ▁come ▁fool ant ant ant ▁will . ▁come ▁come ▁come ▁come ▁come ▁come ▁come ▁come ▁come ▁come ▁come ▁come ▁come ▁come ▁come ▁come ▁come ▁come ▁come ▁3. ▁3. ▁3. ▁3. ▁come ▁come ▁3. ▁3. ▁3. ▁come ▁come ▁come ▁come ▁come ▁3. ▁3. ▁come ▁come ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁come ▁come ▁come ▁3. ▁3. ▁3. ▁come . ▁pri ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁whole ▁3. ▁whole ▁come . . . ▁pri ▁3. ▁3. ▁3. . ▁pri ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. t ▁3. ▁3. ▁3. . . ▁pri : ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. lf ▁3. lf ▁3. lf ▁back ▁back ▁back ▁3. ▁3. ▁3. ▁back ▁back ▁back ▁3. : ▁3. ▁3. ▁3. ▁whole ▁3. : lf ▁back ▁back ▁back ▁back ▁3. ▁whole ▁3. . . ▁pri : : ▁3. ▁3. ▁3. : lf ▁3. ▁3. ▁3. lf lf lf lf ▁3. ▁3. . lf ▁3. : :
2021-10-18 17:04:47,592 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step        1: bleu:   0.17, loss: 304.0935, ppl: 115.7533, duration: 2.0442s
2021-10-18 17:04:47,663 - INFO - joeynmt.training - Epoch   1, Step:        2, Batch Loss:     4.651479, Tokens per Sec:     1259, Lr: 0.000100
2021-10-18 17:04:49,399 - INFO - joeynmt.training - Example #0
2021-10-18 17:04:49,399 - DEBUG - joeynmt.training - 	Raw source:     ['s15a07', 's1f010', 's26507', 's1f540', 's38800', 's32107', 's15a37', 's15a37', 's15a18', 's14c01', 's22a07', 's20301', 's20500', 's20500', 's10043', 's2d600', 's10018', 's26505', 's10641', 's15a30', 's15a50', 's2a208', 's2a218', 's1f710', 's20302', 's2ea08', 's38900', 's15a37', 's18250', 's28902', 's32107', 's15a37', 's15a37', 's2ff00', 's15a10', 's26500', 's15a18', 's14c01', 's22a07', 's20301', 's20500', 's20500', 's10043', 's2d600', 's38800', 's18217', 's1821f', 's20500', 's2d600', 's20500', 's30a00', 's20330', 's20330', 's15039', 's15031', 's26520', 's20500', 's20500', 's10043', 's2d600', 's18520', 's2b707', 's18528', 's2b717', 's32107', 's15a37', 's15a37', 's38900', 's32107', 's15a37', 's15a37', 's2ff00', 's15a10', 's26500', 's1f720', 's1f728', 's26520', 's15a18', 's14c01', 's22a07', 's20301', 's20500', 's20500', 's10043', 's2d600', 's38800']
2021-10-18 17:04:49,399 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁"', 'ent', '▁"', '▁"', '▁"', '▁rom', '▁rom', '▁rom', '▁rom', '▁rom', '▁rom', 'ant', 'ant', 'ant', 'ant', 'ant', 'ant', 'ant', 'ant', 'ant', 'ant', 'ant', 'ant', 'ant', 'ant', '▁come', '▁come', '▁come', '▁come', '▁come', '▁come', '▁come', '▁come', '▁fool', '7', '▁3.', '7', 'ant', 'ant', 'ant', '▁come', '▁come', '▁come', '▁come', '▁come', '▁come', '▁come', '▁come', '▁come', '▁come', '▁come', '▁come', '▁come', '▁come', '▁come', '▁come', '▁come', '▁come', '▁come', '▁3.', '▁3.', '▁3.', '▁3.', '▁come', '▁come', '▁3.', '▁3.', '▁3.', '▁come', '▁come', '▁come', '▁come', '▁3.', '▁3.', '▁3.', '▁3.', '▁come', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁come', '▁come', '▁3.', '▁pri', '▁3.', '▁3.', '▁3.', '.', '▁pri', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', '▁whole', '▁3.', '▁whole', '.', '.', '▁pri', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', 't', '▁3.', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', 't', '▁3.', 'lf', '▁3.', 'lf', '▁back', 'lf', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', ':', ':', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', ':', 'lf', '▁back', '▁back', '▁back', '▁back', '▁3.', '▁whole', '▁3.', '.', '.', '▁pri', ':', ':', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', ':', '▁3.', '▁3.', 'lf', 'lf', 'lf', 'lf', '▁3.', '▁3.', '▁3.', 'lf', '▁3.', ':', '▁3.']
2021-10-18 17:04:49,399 - INFO - joeynmt.training - 	Source:     s15a07 s1f010 s26507 s1f540 s38800 s32107 s15a37 s15a37 s15a18 s14c01 s22a07 s20301 s20500 s20500 s10043 s2d600 s10018 s26505 s10641 s15a30 s15a50 s2a208 s2a218 s1f710 s20302 s2ea08 s38900 s15a37 s18250 s28902 s32107 s15a37 s15a37 s2ff00 s15a10 s26500 s15a18 s14c01 s22a07 s20301 s20500 s20500 s10043 s2d600 s38800 s18217 s1821f s20500 s2d600 s20500 s30a00 s20330 s20330 s15039 s15031 s26520 s20500 s20500 s10043 s2d600 s18520 s2b707 s18528 s2b717 s32107 s15a37 s15a37 s38900 s32107 s15a37 s15a37 s2ff00 s15a10 s26500 s1f720 s1f728 s26520 s15a18 s14c01 s22a07 s20301 s20500 s20500 s10043 s2d600 s38800
2021-10-18 17:04:49,399 - INFO - joeynmt.training - 	Reference:  ▁proverbs ▁3 1 :4 ▁it ▁is ▁not ▁for ▁kings , ▁o ▁l em u el , ▁it ▁is ▁not ▁for ▁kings ▁to ▁drink ▁wine ; ▁nor ▁for ▁pr in ces ▁strong ▁drink \ n \ n o h ▁you ▁l em u el , ▁wine ▁kings ▁should ▁drink ▁not , ▁and ▁pr in ces ▁should ▁al c o h ol ▁drink ▁not .
2021-10-18 17:04:49,400 - INFO - joeynmt.training - 	Hypothesis: ▁" ent ▁" ▁" ▁" ▁rom ▁rom ▁rom ▁rom ▁rom ▁rom ant ant ant ant ant ant ant ant ant ant ant ant ant ant ▁come ▁come ▁come ▁come ▁come ▁come ▁come ▁come ▁fool 7 ▁3. 7 ant ant ant ▁come ▁come ▁come ▁come ▁come ▁come ▁come ▁come ▁come ▁come ▁come ▁come ▁come ▁come ▁come ▁come ▁come ▁come ▁come ▁3. ▁3. ▁3. ▁3. ▁come ▁come ▁3. ▁3. ▁3. ▁come ▁come ▁come ▁come ▁3. ▁3. ▁3. ▁3. ▁come ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁come ▁come ▁3. ▁pri ▁3. ▁3. ▁3. . ▁pri ▁3. ▁whole ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁whole ▁3. ▁whole ▁3. ▁whole . . ▁pri ▁3. ▁whole ▁3. ▁3. ▁3. ▁whole ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. t ▁3. ▁3. ▁3. ▁3. ▁whole ▁3. ▁whole ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. t ▁3. lf ▁3. lf ▁back lf ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. : : ▁3. ▁3. ▁3. ▁whole ▁3. : lf ▁back ▁back ▁back ▁back ▁3. ▁whole ▁3. . . ▁pri : : ▁3. ▁3. ▁3. ▁whole ▁3. : ▁3. ▁3. lf lf lf lf ▁3. ▁3. ▁3. lf ▁3. : ▁3.
2021-10-18 17:04:49,400 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step        2: bleu:   0.16, loss: 304.6301, ppl: 116.7279, duration: 1.7368s
2021-10-18 17:04:49,515 - INFO - joeynmt.training - Epoch   1, Step:        3, Batch Loss:     4.656757, Tokens per Sec:     1279, Lr: 0.000100
2021-10-18 17:04:51,205 - INFO - joeynmt.training - Example #0
2021-10-18 17:04:51,205 - DEBUG - joeynmt.training - 	Raw source:     ['s15a07', 's1f010', 's26507', 's1f540', 's38800', 's32107', 's15a37', 's15a37', 's15a18', 's14c01', 's22a07', 's20301', 's20500', 's20500', 's10043', 's2d600', 's10018', 's26505', 's10641', 's15a30', 's15a50', 's2a208', 's2a218', 's1f710', 's20302', 's2ea08', 's38900', 's15a37', 's18250', 's28902', 's32107', 's15a37', 's15a37', 's2ff00', 's15a10', 's26500', 's15a18', 's14c01', 's22a07', 's20301', 's20500', 's20500', 's10043', 's2d600', 's38800', 's18217', 's1821f', 's20500', 's2d600', 's20500', 's30a00', 's20330', 's20330', 's15039', 's15031', 's26520', 's20500', 's20500', 's10043', 's2d600', 's18520', 's2b707', 's18528', 's2b717', 's32107', 's15a37', 's15a37', 's38900', 's32107', 's15a37', 's15a37', 's2ff00', 's15a10', 's26500', 's1f720', 's1f728', 's26520', 's15a18', 's14c01', 's22a07', 's20301', 's20500', 's20500', 's10043', 's2d600', 's38800']
2021-10-18 17:04:51,205 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁"', 'ent', '▁"', '▁"', '▁"', '▁rom', '▁rom', '▁rom', '▁rom', '▁rom', '▁rom', 'ant', 'ant', 'ant', 'ant', 'ant', 'ant', 'ant', 'ant', 'ant', 'ant', 'ant', 'ant', '▁3.', '▁3.', '7', '7', '7', '7', '7', '7', 'ant', '▁will', '▁come', '▁come', '▁come', 'ant', 'ant', '▁will', '7', '7', '▁3.', '▁3.', '▁come', '▁come', '▁come', '▁come', '▁come', '▁come', '▁come', '▁come', '▁come', '▁come', '▁come', '▁come', '▁come', '▁come', '▁come', '▁come', '▁come', '▁3.', '▁3.', '▁3.', '▁come', '▁come', '▁come', '▁3.', '▁3.', '▁come', '▁come', '▁come', '▁come', '▁come', '▁3.', '▁3.', '▁come', '▁come', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁come', '▁come', '▁3.', '▁pri', '▁3.', '▁3.', '▁come', '▁come', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', '▁whole', '▁3.', '▁whole', '.', '.', '▁pri', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', 't', 't', '▁3.', '▁3.', '.', '▁pri', ':', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', 't', '▁3.', 'lf', '▁3.', 'lf', '▁back', 'lf', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', ':', ':', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', ':', 'lf', '▁back', '▁back', '▁back', '▁back', '▁3.', '▁whole', '▁3.', '.', '.', '▁pri', ':', ':', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', ':', '▁3.', '▁3.', 'lf', 'lf', 'lf', 'lf', '▁3.', '▁3.', '▁3.', 'lf', '▁3.', ':', '▁3.']
2021-10-18 17:04:51,205 - INFO - joeynmt.training - 	Source:     s15a07 s1f010 s26507 s1f540 s38800 s32107 s15a37 s15a37 s15a18 s14c01 s22a07 s20301 s20500 s20500 s10043 s2d600 s10018 s26505 s10641 s15a30 s15a50 s2a208 s2a218 s1f710 s20302 s2ea08 s38900 s15a37 s18250 s28902 s32107 s15a37 s15a37 s2ff00 s15a10 s26500 s15a18 s14c01 s22a07 s20301 s20500 s20500 s10043 s2d600 s38800 s18217 s1821f s20500 s2d600 s20500 s30a00 s20330 s20330 s15039 s15031 s26520 s20500 s20500 s10043 s2d600 s18520 s2b707 s18528 s2b717 s32107 s15a37 s15a37 s38900 s32107 s15a37 s15a37 s2ff00 s15a10 s26500 s1f720 s1f728 s26520 s15a18 s14c01 s22a07 s20301 s20500 s20500 s10043 s2d600 s38800
2021-10-18 17:04:51,205 - INFO - joeynmt.training - 	Reference:  ▁proverbs ▁3 1 :4 ▁it ▁is ▁not ▁for ▁kings , ▁o ▁l em u el , ▁it ▁is ▁not ▁for ▁kings ▁to ▁drink ▁wine ; ▁nor ▁for ▁pr in ces ▁strong ▁drink \ n \ n o h ▁you ▁l em u el , ▁wine ▁kings ▁should ▁drink ▁not , ▁and ▁pr in ces ▁should ▁al c o h ol ▁drink ▁not .
2021-10-18 17:04:51,206 - INFO - joeynmt.training - 	Hypothesis: ▁" ent ▁" ▁" ▁" ▁rom ▁rom ▁rom ▁rom ▁rom ▁rom ant ant ant ant ant ant ant ant ant ant ant ant ▁3. ▁3. 7 7 7 7 7 7 ant ▁will ▁come ▁come ▁come ant ant ▁will 7 7 ▁3. ▁3. ▁come ▁come ▁come ▁come ▁come ▁come ▁come ▁come ▁come ▁come ▁come ▁come ▁come ▁come ▁come ▁come ▁come ▁3. ▁3. ▁3. ▁come ▁come ▁come ▁3. ▁3. ▁come ▁come ▁come ▁come ▁come ▁3. ▁3. ▁come ▁come ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁come ▁come ▁3. ▁pri ▁3. ▁3. ▁come ▁come ▁3. ▁whole ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁whole ▁3. ▁whole ▁3. ▁whole . . ▁pri ▁3. ▁whole ▁3. ▁3. ▁3. ▁whole ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. t t ▁3. ▁3. . ▁pri : ▁3. ▁whole ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. t ▁3. lf ▁3. lf ▁back lf ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. : : ▁3. ▁3. ▁3. ▁whole ▁3. : lf ▁back ▁back ▁back ▁back ▁3. ▁whole ▁3. . . ▁pri : : ▁3. ▁3. ▁3. ▁whole ▁3. : ▁3. ▁3. lf lf lf lf ▁3. ▁3. ▁3. lf ▁3. : ▁3.
2021-10-18 17:04:51,206 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step        3: bleu:   0.16, loss: 305.0837, ppl: 117.5582, duration: 1.6908s
2021-10-18 17:04:51,268 - INFO - joeynmt.training - Epoch   1, Step:        4, Batch Loss:     4.623563, Tokens per Sec:      787, Lr: 0.000100
2021-10-18 17:04:53,178 - INFO - joeynmt.training - Example #0
2021-10-18 17:04:53,178 - DEBUG - joeynmt.training - 	Raw source:     ['s15a07', 's1f010', 's26507', 's1f540', 's38800', 's32107', 's15a37', 's15a37', 's15a18', 's14c01', 's22a07', 's20301', 's20500', 's20500', 's10043', 's2d600', 's10018', 's26505', 's10641', 's15a30', 's15a50', 's2a208', 's2a218', 's1f710', 's20302', 's2ea08', 's38900', 's15a37', 's18250', 's28902', 's32107', 's15a37', 's15a37', 's2ff00', 's15a10', 's26500', 's15a18', 's14c01', 's22a07', 's20301', 's20500', 's20500', 's10043', 's2d600', 's38800', 's18217', 's1821f', 's20500', 's2d600', 's20500', 's30a00', 's20330', 's20330', 's15039', 's15031', 's26520', 's20500', 's20500', 's10043', 's2d600', 's18520', 's2b707', 's18528', 's2b717', 's32107', 's15a37', 's15a37', 's38900', 's32107', 's15a37', 's15a37', 's2ff00', 's15a10', 's26500', 's1f720', 's1f728', 's26520', 's15a18', 's14c01', 's22a07', 's20301', 's20500', 's20500', 's10043', 's2d600', 's38800']
2021-10-18 17:04:53,178 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁"', 'ent', '▁"', '▁"', '▁"', '▁rom', '▁rom', '▁rom', '▁rom', '▁rom', '▁rom', '▁rom', 'ant', 'ant', 'ant', 'ant', 'ant', 'ant', 'ant', 'ant', 'ant', 'ant', '▁3.', '▁3.', '▁3.', '7', '7', '7', '7', '7', '7', 'ant', '▁will', '▁come', '▁come', '▁come', 'ant', 'ant', 'ant', 'ant', '▁come', '▁come', '▁come', '▁come', '▁come', '▁come', '▁come', '▁come', '▁come', '▁come', '▁come', '▁come', '▁come', '▁come', '▁come', '▁come', '▁come', '▁come', '▁come', '▁3.', '▁3.', '▁3.', '▁3.', '▁come', '▁come', '▁3.', '▁3.', '▁3.', '▁come', '▁come', '▁come', '▁come', '▁3.', '▁3.', '▁3.', '▁3.', '▁come', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁come', '▁come', '▁3.', '▁pri', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', '▁whole', '▁3.', '▁whole', '.', '.', '▁pri', '▁3.', '▁whole', '▁3.', '▁3.', '▁whole', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', 't', 't', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', 't', '▁3.', '▁3.', '▁3.', '▁3.', '▁whole', 'lf', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', ':', ':', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', ':', 'lf', '▁back', '▁back', '▁back', '▁whole', '▁3.', '▁whole', '▁3.', '.', '.', '▁pri', ':', ':', '▁3.', '▁3.', '▁3.', ':', 'lf', '▁3.', '▁3.', '▁3.', '▁3.', 'lf', 'lf', '▁3.', ':', '▁3.', '▁3.', '▁whole', '▁3.', ':', '▁3.']
2021-10-18 17:04:53,178 - INFO - joeynmt.training - 	Source:     s15a07 s1f010 s26507 s1f540 s38800 s32107 s15a37 s15a37 s15a18 s14c01 s22a07 s20301 s20500 s20500 s10043 s2d600 s10018 s26505 s10641 s15a30 s15a50 s2a208 s2a218 s1f710 s20302 s2ea08 s38900 s15a37 s18250 s28902 s32107 s15a37 s15a37 s2ff00 s15a10 s26500 s15a18 s14c01 s22a07 s20301 s20500 s20500 s10043 s2d600 s38800 s18217 s1821f s20500 s2d600 s20500 s30a00 s20330 s20330 s15039 s15031 s26520 s20500 s20500 s10043 s2d600 s18520 s2b707 s18528 s2b717 s32107 s15a37 s15a37 s38900 s32107 s15a37 s15a37 s2ff00 s15a10 s26500 s1f720 s1f728 s26520 s15a18 s14c01 s22a07 s20301 s20500 s20500 s10043 s2d600 s38800
2021-10-18 17:04:53,178 - INFO - joeynmt.training - 	Reference:  ▁proverbs ▁3 1 :4 ▁it ▁is ▁not ▁for ▁kings , ▁o ▁l em u el , ▁it ▁is ▁not ▁for ▁kings ▁to ▁drink ▁wine ; ▁nor ▁for ▁pr in ces ▁strong ▁drink \ n \ n o h ▁you ▁l em u el , ▁wine ▁kings ▁should ▁drink ▁not , ▁and ▁pr in ces ▁should ▁al c o h ol ▁drink ▁not .
2021-10-18 17:04:53,178 - INFO - joeynmt.training - 	Hypothesis: ▁" ent ▁" ▁" ▁" ▁rom ▁rom ▁rom ▁rom ▁rom ▁rom ▁rom ant ant ant ant ant ant ant ant ant ant ▁3. ▁3. ▁3. 7 7 7 7 7 7 ant ▁will ▁come ▁come ▁come ant ant ant ant ▁come ▁come ▁come ▁come ▁come ▁come ▁come ▁come ▁come ▁come ▁come ▁come ▁come ▁come ▁come ▁come ▁come ▁come ▁come ▁3. ▁3. ▁3. ▁3. ▁come ▁come ▁3. ▁3. ▁3. ▁come ▁come ▁come ▁come ▁3. ▁3. ▁3. ▁3. ▁come ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁come ▁come ▁3. ▁pri ▁3. ▁3. ▁3. ▁whole ▁3. ▁whole ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁whole ▁3. ▁whole ▁3. ▁whole . . ▁pri ▁3. ▁whole ▁3. ▁3. ▁whole ▁3. ▁whole ▁3. ▁3. ▁3. ▁3. ▁3. t t ▁3. ▁3. ▁3. ▁whole ▁3. ▁whole ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. t ▁3. ▁3. ▁3. ▁3. ▁whole lf ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. : : ▁3. ▁3. ▁3. ▁whole ▁3. : lf ▁back ▁back ▁back ▁whole ▁3. ▁whole ▁3. . . ▁pri : : ▁3. ▁3. ▁3. : lf ▁3. ▁3. ▁3. ▁3. lf lf ▁3. : ▁3. ▁3. ▁whole ▁3. : ▁3.
2021-10-18 17:04:53,178 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step        4: bleu:   0.16, loss: 305.5820, ppl: 118.4770, duration: 1.9099s
2021-10-18 17:04:53,234 - INFO - joeynmt.training - Epoch   1, Step:        5, Batch Loss:     4.582821, Tokens per Sec:     1008, Lr: 0.000100
2021-10-18 17:04:54,935 - INFO - joeynmt.training - Example #0
2021-10-18 17:04:54,935 - DEBUG - joeynmt.training - 	Raw source:     ['s15a07', 's1f010', 's26507', 's1f540', 's38800', 's32107', 's15a37', 's15a37', 's15a18', 's14c01', 's22a07', 's20301', 's20500', 's20500', 's10043', 's2d600', 's10018', 's26505', 's10641', 's15a30', 's15a50', 's2a208', 's2a218', 's1f710', 's20302', 's2ea08', 's38900', 's15a37', 's18250', 's28902', 's32107', 's15a37', 's15a37', 's2ff00', 's15a10', 's26500', 's15a18', 's14c01', 's22a07', 's20301', 's20500', 's20500', 's10043', 's2d600', 's38800', 's18217', 's1821f', 's20500', 's2d600', 's20500', 's30a00', 's20330', 's20330', 's15039', 's15031', 's26520', 's20500', 's20500', 's10043', 's2d600', 's18520', 's2b707', 's18528', 's2b717', 's32107', 's15a37', 's15a37', 's38900', 's32107', 's15a37', 's15a37', 's2ff00', 's15a10', 's26500', 's1f720', 's1f728', 's26520', 's15a18', 's14c01', 's22a07', 's20301', 's20500', 's20500', 's10043', 's2d600', 's38800']
2021-10-18 17:04:54,935 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁"', 'ent', '▁"', '▁"', '▁"', '▁rom', '▁rom', '▁rom', '▁rom', '▁rom', '▁rom', '▁rom', 'ant', 'ant', 'ant', 'ant', 'ant', 'ant', 'ant', 'ant', 'ant', 'ant', '▁3.', '▁3.', '▁3.', '7', '7', '7', '▁3.', '▁come', '▁come', '▁come', '▁come', '▁come', '▁come', '▁3.', '▁3.', '7', 'ant', 'ant', '▁come', '▁3.', '▁3.', '▁come', '▁come', '▁come', '▁come', '▁3.', '▁3.', '▁come', '▁come', '▁come', '▁come', '▁whole', '▁3.', '▁3.', '▁3.', '▁come', '▁come', '▁whole', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁come', '▁come', '▁come', '▁come', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁pri', '▁pri', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', '▁whole', '▁3.', '▁whole', '.', '.', '▁pri', '▁3.', '▁whole', '▁3.', '▁3.', '▁whole', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', 't', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', 't', '▁3.', '▁3.', '▁3.', '▁3.', '▁whole', 'lf', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', ':', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', ':', 'lf', '▁back', '▁back', '▁back', '▁whole', '▁3.', '▁whole', '▁3.', '.', '.', '▁pri', ':', ':', '▁3.', '▁3.', '▁3.', ':', 'lf', '▁3.', '▁3.', '▁3.', '▁3.', 'lf', 'lf', '▁3.', ':', '▁3.', '▁3.', '▁whole', '▁3.', ':', '▁3.']
2021-10-18 17:04:54,935 - INFO - joeynmt.training - 	Source:     s15a07 s1f010 s26507 s1f540 s38800 s32107 s15a37 s15a37 s15a18 s14c01 s22a07 s20301 s20500 s20500 s10043 s2d600 s10018 s26505 s10641 s15a30 s15a50 s2a208 s2a218 s1f710 s20302 s2ea08 s38900 s15a37 s18250 s28902 s32107 s15a37 s15a37 s2ff00 s15a10 s26500 s15a18 s14c01 s22a07 s20301 s20500 s20500 s10043 s2d600 s38800 s18217 s1821f s20500 s2d600 s20500 s30a00 s20330 s20330 s15039 s15031 s26520 s20500 s20500 s10043 s2d600 s18520 s2b707 s18528 s2b717 s32107 s15a37 s15a37 s38900 s32107 s15a37 s15a37 s2ff00 s15a10 s26500 s1f720 s1f728 s26520 s15a18 s14c01 s22a07 s20301 s20500 s20500 s10043 s2d600 s38800
2021-10-18 17:04:54,935 - INFO - joeynmt.training - 	Reference:  ▁proverbs ▁3 1 :4 ▁it ▁is ▁not ▁for ▁kings , ▁o ▁l em u el , ▁it ▁is ▁not ▁for ▁kings ▁to ▁drink ▁wine ; ▁nor ▁for ▁pr in ces ▁strong ▁drink \ n \ n o h ▁you ▁l em u el , ▁wine ▁kings ▁should ▁drink ▁not , ▁and ▁pr in ces ▁should ▁al c o h ol ▁drink ▁not .
2021-10-18 17:04:54,935 - INFO - joeynmt.training - 	Hypothesis: ▁" ent ▁" ▁" ▁" ▁rom ▁rom ▁rom ▁rom ▁rom ▁rom ▁rom ant ant ant ant ant ant ant ant ant ant ▁3. ▁3. ▁3. 7 7 7 ▁3. ▁come ▁come ▁come ▁come ▁come ▁come ▁3. ▁3. 7 ant ant ▁come ▁3. ▁3. ▁come ▁come ▁come ▁come ▁3. ▁3. ▁come ▁come ▁come ▁come ▁whole ▁3. ▁3. ▁3. ▁come ▁come ▁whole ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁come ▁come ▁come ▁come ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁pri ▁pri ▁3. ▁3. ▁3. ▁whole ▁3. ▁whole ▁3. ▁3. ▁3. ▁3. ▁whole ▁3. ▁3. ▁3. ▁3. ▁3. ▁whole ▁3. ▁whole ▁3. ▁whole . . ▁pri ▁3. ▁whole ▁3. ▁3. ▁whole ▁3. ▁whole ▁3. ▁3. ▁3. ▁3. ▁whole ▁3. t ▁3. ▁3. ▁3. ▁whole ▁3. ▁whole ▁3. ▁3. ▁3. ▁3. ▁3. ▁whole ▁3. ▁3. ▁3. ▁3. ▁3. t ▁3. ▁3. ▁3. ▁3. ▁whole lf ▁3. ▁3. ▁3. ▁3. ▁3. ▁whole ▁3. : ▁3. ▁3. ▁3. ▁whole ▁3. : lf ▁back ▁back ▁back ▁whole ▁3. ▁whole ▁3. . . ▁pri : : ▁3. ▁3. ▁3. : lf ▁3. ▁3. ▁3. ▁3. lf lf ▁3. : ▁3. ▁3. ▁whole ▁3. : ▁3.
2021-10-18 17:04:54,935 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step        5: bleu:   0.15, loss: 305.5592, ppl: 118.4349, duration: 1.7012s
2021-10-18 17:04:54,936 - INFO - joeynmt.training - Epoch   1: total training loss 23.11
2021-10-18 17:04:54,936 - INFO - joeynmt.training - EPOCH 2
2021-10-18 17:04:55,000 - INFO - joeynmt.training - Epoch   2, Step:        6, Batch Loss:     4.691381, Tokens per Sec:      877, Lr: 0.000100
2021-10-18 17:04:56,819 - INFO - joeynmt.training - Example #0
2021-10-18 17:04:56,819 - DEBUG - joeynmt.training - 	Raw source:     ['s15a07', 's1f010', 's26507', 's1f540', 's38800', 's32107', 's15a37', 's15a37', 's15a18', 's14c01', 's22a07', 's20301', 's20500', 's20500', 's10043', 's2d600', 's10018', 's26505', 's10641', 's15a30', 's15a50', 's2a208', 's2a218', 's1f710', 's20302', 's2ea08', 's38900', 's15a37', 's18250', 's28902', 's32107', 's15a37', 's15a37', 's2ff00', 's15a10', 's26500', 's15a18', 's14c01', 's22a07', 's20301', 's20500', 's20500', 's10043', 's2d600', 's38800', 's18217', 's1821f', 's20500', 's2d600', 's20500', 's30a00', 's20330', 's20330', 's15039', 's15031', 's26520', 's20500', 's20500', 's10043', 's2d600', 's18520', 's2b707', 's18528', 's2b717', 's32107', 's15a37', 's15a37', 's38900', 's32107', 's15a37', 's15a37', 's2ff00', 's15a10', 's26500', 's1f720', 's1f728', 's26520', 's15a18', 's14c01', 's22a07', 's20301', 's20500', 's20500', 's10043', 's2d600', 's38800']
2021-10-18 17:04:56,819 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁"', 'ent', '▁"', '▁"', '▁"', '▁rom', '▁rom', '▁rom', '▁rom', '▁rom', '▁rom', '▁rom', 'ant', 'ant', 'ant', 'ant', 'ant', 'ant', 'ant', 'ant', 'ant', 'ant', '▁3.', '▁3.', '▁3.', '7', '7', '7', '▁3.', '▁3.', '▁come', '▁come', '▁come', '▁come', '▁come', '▁3.', '▁3.', '7', 'ant', 'ant', '▁come', '▁3.', '▁3.', '▁3.', '▁come', '▁come', '▁come', '▁3.', '▁3.', '▁come', '▁come', '▁come', '▁come', '▁whole', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁come', '▁come', '▁come', '▁come', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁pri', '▁3.', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', '▁whole', '▁3.', '▁whole', '.', '.', '▁pri', '▁3.', '▁whole', '▁3.', '▁3.', '▁whole', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', 't', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', 't', '▁3.', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', ':', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', ':', 'lf', '▁back', '▁back', '▁back', '▁whole', '▁3.', '▁whole', '▁3.', '.', '.', '▁pri', ':', ':', '▁3.', '▁3.', '▁3.', ':', 'lf', '▁3.', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', ':', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', ':', '▁3.']
2021-10-18 17:04:56,819 - INFO - joeynmt.training - 	Source:     s15a07 s1f010 s26507 s1f540 s38800 s32107 s15a37 s15a37 s15a18 s14c01 s22a07 s20301 s20500 s20500 s10043 s2d600 s10018 s26505 s10641 s15a30 s15a50 s2a208 s2a218 s1f710 s20302 s2ea08 s38900 s15a37 s18250 s28902 s32107 s15a37 s15a37 s2ff00 s15a10 s26500 s15a18 s14c01 s22a07 s20301 s20500 s20500 s10043 s2d600 s38800 s18217 s1821f s20500 s2d600 s20500 s30a00 s20330 s20330 s15039 s15031 s26520 s20500 s20500 s10043 s2d600 s18520 s2b707 s18528 s2b717 s32107 s15a37 s15a37 s38900 s32107 s15a37 s15a37 s2ff00 s15a10 s26500 s1f720 s1f728 s26520 s15a18 s14c01 s22a07 s20301 s20500 s20500 s10043 s2d600 s38800
2021-10-18 17:04:56,820 - INFO - joeynmt.training - 	Reference:  ▁proverbs ▁3 1 :4 ▁it ▁is ▁not ▁for ▁kings , ▁o ▁l em u el , ▁it ▁is ▁not ▁for ▁kings ▁to ▁drink ▁wine ; ▁nor ▁for ▁pr in ces ▁strong ▁drink \ n \ n o h ▁you ▁l em u el , ▁wine ▁kings ▁should ▁drink ▁not , ▁and ▁pr in ces ▁should ▁al c o h ol ▁drink ▁not .
2021-10-18 17:04:56,820 - INFO - joeynmt.training - 	Hypothesis: ▁" ent ▁" ▁" ▁" ▁rom ▁rom ▁rom ▁rom ▁rom ▁rom ▁rom ant ant ant ant ant ant ant ant ant ant ▁3. ▁3. ▁3. 7 7 7 ▁3. ▁3. ▁come ▁come ▁come ▁come ▁come ▁3. ▁3. 7 ant ant ▁come ▁3. ▁3. ▁3. ▁come ▁come ▁come ▁3. ▁3. ▁come ▁come ▁come ▁come ▁whole ▁3. ▁3. ▁3. ▁whole ▁3. ▁whole ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁come ▁come ▁come ▁come ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁pri ▁3. ▁3. ▁3. ▁3. ▁whole ▁3. ▁whole ▁3. ▁3. ▁3. ▁3. ▁whole ▁3. ▁3. ▁3. ▁3. ▁3. ▁whole ▁3. ▁whole ▁3. ▁whole . . ▁pri ▁3. ▁whole ▁3. ▁3. ▁whole ▁3. ▁whole ▁3. ▁3. ▁3. ▁3. ▁whole ▁3. t ▁3. ▁3. ▁3. ▁whole ▁3. ▁whole ▁3. ▁3. ▁3. ▁3. ▁3. ▁whole ▁3. ▁3. ▁3. ▁3. ▁3. t ▁3. ▁3. ▁3. ▁3. ▁whole ▁3. ▁whole ▁3. ▁3. ▁3. ▁3. ▁whole ▁3. : ▁3. ▁3. ▁3. ▁whole ▁3. : lf ▁back ▁back ▁back ▁whole ▁3. ▁whole ▁3. . . ▁pri : : ▁3. ▁3. ▁3. : lf ▁3. ▁3. ▁3. ▁3. ▁whole ▁3. : ▁3. ▁3. ▁3. ▁whole ▁3. : ▁3.
2021-10-18 17:04:56,820 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step        6: bleu:   0.15, loss: 305.8412, ppl: 118.9578, duration: 1.8200s
2021-10-18 17:04:56,911 - INFO - joeynmt.training - Epoch   2, Step:        7, Batch Loss:     4.613130, Tokens per Sec:      834, Lr: 0.000100
2021-10-18 17:04:58,831 - INFO - joeynmt.training - Example #0
2021-10-18 17:04:58,831 - DEBUG - joeynmt.training - 	Raw source:     ['s15a07', 's1f010', 's26507', 's1f540', 's38800', 's32107', 's15a37', 's15a37', 's15a18', 's14c01', 's22a07', 's20301', 's20500', 's20500', 's10043', 's2d600', 's10018', 's26505', 's10641', 's15a30', 's15a50', 's2a208', 's2a218', 's1f710', 's20302', 's2ea08', 's38900', 's15a37', 's18250', 's28902', 's32107', 's15a37', 's15a37', 's2ff00', 's15a10', 's26500', 's15a18', 's14c01', 's22a07', 's20301', 's20500', 's20500', 's10043', 's2d600', 's38800', 's18217', 's1821f', 's20500', 's2d600', 's20500', 's30a00', 's20330', 's20330', 's15039', 's15031', 's26520', 's20500', 's20500', 's10043', 's2d600', 's18520', 's2b707', 's18528', 's2b717', 's32107', 's15a37', 's15a37', 's38900', 's32107', 's15a37', 's15a37', 's2ff00', 's15a10', 's26500', 's1f720', 's1f728', 's26520', 's15a18', 's14c01', 's22a07', 's20301', 's20500', 's20500', 's10043', 's2d600', 's38800']
2021-10-18 17:04:58,832 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁"', 'ent', '▁"', '▁"', '▁"', '▁rom', '▁rom', '▁rom', '▁rom', '▁rom', '▁rom', '▁rom', 'ant', 'ant', 'ant', 'ant', 'ant', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁come', '▁come', '▁come', '▁come', '▁whole', '▁3.', '▁3.', '▁3.', '▁come', '▁fool', '▁3.', '▁whole', '▁3.', '▁3.', '▁come', '▁come', '▁3.', '▁whole', '▁3.', '▁3.', '▁come', '▁come', '▁whole', '▁whole', '▁3.', '▁whole', '▁3.', '▁whole', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', '▁whole', '▁3.', '▁whole', '▁3.', '▁whole', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', '▁whole', '▁3.', '▁whole', '.', '.', '▁whole', '▁3.', '▁whole', '▁3.', '▁3.', '▁whole', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', 't', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', 't', '▁3.', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', '▁whole', '▁3.', '▁3.', '.', '▁back', '▁whole', '▁3.', '▁whole', '▁3.', '▁3.', '▁whole', '▁3.', ':', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', 't', '▁3.', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', 't', '▁3.', '▁3.', '▁3.', '▁3.', ':', ':', '▁3.']
2021-10-18 17:04:58,832 - INFO - joeynmt.training - 	Source:     s15a07 s1f010 s26507 s1f540 s38800 s32107 s15a37 s15a37 s15a18 s14c01 s22a07 s20301 s20500 s20500 s10043 s2d600 s10018 s26505 s10641 s15a30 s15a50 s2a208 s2a218 s1f710 s20302 s2ea08 s38900 s15a37 s18250 s28902 s32107 s15a37 s15a37 s2ff00 s15a10 s26500 s15a18 s14c01 s22a07 s20301 s20500 s20500 s10043 s2d600 s38800 s18217 s1821f s20500 s2d600 s20500 s30a00 s20330 s20330 s15039 s15031 s26520 s20500 s20500 s10043 s2d600 s18520 s2b707 s18528 s2b717 s32107 s15a37 s15a37 s38900 s32107 s15a37 s15a37 s2ff00 s15a10 s26500 s1f720 s1f728 s26520 s15a18 s14c01 s22a07 s20301 s20500 s20500 s10043 s2d600 s38800
2021-10-18 17:04:58,832 - INFO - joeynmt.training - 	Reference:  ▁proverbs ▁3 1 :4 ▁it ▁is ▁not ▁for ▁kings , ▁o ▁l em u el , ▁it ▁is ▁not ▁for ▁kings ▁to ▁drink ▁wine ; ▁nor ▁for ▁pr in ces ▁strong ▁drink \ n \ n o h ▁you ▁l em u el , ▁wine ▁kings ▁should ▁drink ▁not , ▁and ▁pr in ces ▁should ▁al c o h ol ▁drink ▁not .
2021-10-18 17:04:58,832 - INFO - joeynmt.training - 	Hypothesis: ▁" ent ▁" ▁" ▁" ▁rom ▁rom ▁rom ▁rom ▁rom ▁rom ▁rom ant ant ant ant ant ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁come ▁come ▁come ▁come ▁whole ▁3. ▁3. ▁3. ▁come ▁fool ▁3. ▁whole ▁3. ▁3. ▁come ▁come ▁3. ▁whole ▁3. ▁3. ▁come ▁come ▁whole ▁whole ▁3. ▁whole ▁3. ▁whole ▁3. ▁whole ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁whole ▁3. ▁3. ▁3. ▁3. ▁3. ▁whole ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁whole ▁3. ▁whole ▁3. ▁whole ▁3. ▁whole ▁3. ▁whole ▁3. ▁3. ▁3. ▁whole ▁3. ▁3. ▁3. ▁3. ▁3. ▁whole ▁3. ▁whole ▁3. ▁whole . . ▁whole ▁3. ▁whole ▁3. ▁3. ▁whole ▁3. ▁whole ▁3. ▁3. ▁3. ▁3. ▁whole ▁3. t ▁3. ▁3. ▁3. ▁whole ▁3. ▁whole ▁3. ▁3. ▁3. ▁3. ▁whole ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. t ▁3. ▁3. ▁3. ▁3. ▁whole ▁3. ▁whole ▁3. ▁3. ▁3. ▁3. ▁3. ▁whole ▁3. ▁3. ▁3. ▁3. ▁whole ▁3. ▁whole ▁3. ▁3. . ▁back ▁whole ▁3. ▁whole ▁3. ▁3. ▁whole ▁3. : ▁3. ▁3. ▁3. ▁3. ▁3. t ▁3. ▁3. ▁3. ▁3. ▁whole ▁3. t ▁3. ▁3. ▁3. ▁3. : : ▁3.
2021-10-18 17:04:58,832 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step        7: bleu:   0.14, loss: 306.0296, ppl: 119.3084, duration: 1.9207s
2021-10-18 17:04:58,903 - INFO - joeynmt.training - Epoch   2, Step:        8, Batch Loss:     4.587081, Tokens per Sec:     1075, Lr: 0.000070
2021-10-18 17:05:00,593 - INFO - joeynmt.training - Example #0
2021-10-18 17:05:00,593 - DEBUG - joeynmt.training - 	Raw source:     ['s15a07', 's1f010', 's26507', 's1f540', 's38800', 's32107', 's15a37', 's15a37', 's15a18', 's14c01', 's22a07', 's20301', 's20500', 's20500', 's10043', 's2d600', 's10018', 's26505', 's10641', 's15a30', 's15a50', 's2a208', 's2a218', 's1f710', 's20302', 's2ea08', 's38900', 's15a37', 's18250', 's28902', 's32107', 's15a37', 's15a37', 's2ff00', 's15a10', 's26500', 's15a18', 's14c01', 's22a07', 's20301', 's20500', 's20500', 's10043', 's2d600', 's38800', 's18217', 's1821f', 's20500', 's2d600', 's20500', 's30a00', 's20330', 's20330', 's15039', 's15031', 's26520', 's20500', 's20500', 's10043', 's2d600', 's18520', 's2b707', 's18528', 's2b717', 's32107', 's15a37', 's15a37', 's38900', 's32107', 's15a37', 's15a37', 's2ff00', 's15a10', 's26500', 's1f720', 's1f728', 's26520', 's15a18', 's14c01', 's22a07', 's20301', 's20500', 's20500', 's10043', 's2d600', 's38800']
2021-10-18 17:05:00,593 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁"', 'ent', '▁"', '▁"', '▁"', '▁rom', '▁rom', '▁rom', '▁rom', '▁rom', '▁rom', '▁rom', 'ant', 'ant', 'ant', 'ant', 'ant', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁come', '▁come', '▁come', '▁fool', '▁3.', '▁whole', 'ant', 'ant', 'ant', 'ant', '▁come', '▁3.', '▁3.', '▁3.', '▁come', '▁come', '▁3.', '▁whole', '▁3.', '▁3.', '▁come', '▁come', '▁whole', '▁whole', '▁3.', '▁whole', '▁3.', '▁whole', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁come', '▁come', '▁come', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', '▁whole', '▁3.', '▁3.', '▁whole', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', '▁whole', '▁3.', '▁whole', '.', '.', '▁whole', '▁3.', '▁whole', '▁3.', '▁3.', '▁whole', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', 't', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', 't', '▁3.', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', '▁whole', '▁3.', '▁3.', '.', '▁back', '▁whole', '▁3.', '▁whole', '▁3.', '▁3.', '▁whole', '▁3.', ':', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', 't', '▁3.', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', 't', '▁3.', '▁3.', '▁3.', '▁3.', ':', ':', '▁3.']
2021-10-18 17:05:00,593 - INFO - joeynmt.training - 	Source:     s15a07 s1f010 s26507 s1f540 s38800 s32107 s15a37 s15a37 s15a18 s14c01 s22a07 s20301 s20500 s20500 s10043 s2d600 s10018 s26505 s10641 s15a30 s15a50 s2a208 s2a218 s1f710 s20302 s2ea08 s38900 s15a37 s18250 s28902 s32107 s15a37 s15a37 s2ff00 s15a10 s26500 s15a18 s14c01 s22a07 s20301 s20500 s20500 s10043 s2d600 s38800 s18217 s1821f s20500 s2d600 s20500 s30a00 s20330 s20330 s15039 s15031 s26520 s20500 s20500 s10043 s2d600 s18520 s2b707 s18528 s2b717 s32107 s15a37 s15a37 s38900 s32107 s15a37 s15a37 s2ff00 s15a10 s26500 s1f720 s1f728 s26520 s15a18 s14c01 s22a07 s20301 s20500 s20500 s10043 s2d600 s38800
2021-10-18 17:05:00,594 - INFO - joeynmt.training - 	Reference:  ▁proverbs ▁3 1 :4 ▁it ▁is ▁not ▁for ▁kings , ▁o ▁l em u el , ▁it ▁is ▁not ▁for ▁kings ▁to ▁drink ▁wine ; ▁nor ▁for ▁pr in ces ▁strong ▁drink \ n \ n o h ▁you ▁l em u el , ▁wine ▁kings ▁should ▁drink ▁not , ▁and ▁pr in ces ▁should ▁al c o h ol ▁drink ▁not .
2021-10-18 17:05:00,594 - INFO - joeynmt.training - 	Hypothesis: ▁" ent ▁" ▁" ▁" ▁rom ▁rom ▁rom ▁rom ▁rom ▁rom ▁rom ant ant ant ant ant ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁come ▁come ▁come ▁fool ▁3. ▁whole ant ant ant ant ▁come ▁3. ▁3. ▁3. ▁come ▁come ▁3. ▁whole ▁3. ▁3. ▁come ▁come ▁whole ▁whole ▁3. ▁whole ▁3. ▁whole ▁3. ▁whole ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁come ▁come ▁come ▁3. ▁3. ▁3. ▁3. ▁3. ▁whole ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁whole ▁3. ▁whole ▁3. ▁3. ▁whole ▁3. ▁whole ▁3. ▁3. ▁3. ▁3. ▁whole ▁3. ▁3. ▁3. ▁3. ▁3. ▁whole ▁3. ▁whole ▁3. ▁whole . . ▁whole ▁3. ▁whole ▁3. ▁3. ▁whole ▁3. ▁whole ▁3. ▁3. ▁3. ▁3. ▁whole ▁3. t ▁3. ▁3. ▁3. ▁whole ▁3. ▁whole ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. t ▁3. ▁3. ▁3. ▁3. ▁whole ▁3. ▁whole ▁3. ▁3. ▁3. ▁3. ▁3. ▁whole ▁3. ▁3. ▁3. ▁3. ▁whole ▁3. ▁whole ▁3. ▁3. . ▁back ▁whole ▁3. ▁whole ▁3. ▁3. ▁whole ▁3. : ▁3. ▁3. ▁3. ▁3. ▁3. t ▁3. ▁3. ▁3. ▁3. ▁whole ▁3. t ▁3. ▁3. ▁3. ▁3. : : ▁3.
2021-10-18 17:05:00,594 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step        8: bleu:   0.14, loss: 306.1454, ppl: 119.5246, duration: 1.6907s
2021-10-18 17:05:00,656 - INFO - joeynmt.training - Epoch   2, Step:        9, Batch Loss:     4.664125, Tokens per Sec:      783, Lr: 0.000070
2021-10-18 17:05:02,345 - INFO - joeynmt.training - Example #0
2021-10-18 17:05:02,345 - DEBUG - joeynmt.training - 	Raw source:     ['s15a07', 's1f010', 's26507', 's1f540', 's38800', 's32107', 's15a37', 's15a37', 's15a18', 's14c01', 's22a07', 's20301', 's20500', 's20500', 's10043', 's2d600', 's10018', 's26505', 's10641', 's15a30', 's15a50', 's2a208', 's2a218', 's1f710', 's20302', 's2ea08', 's38900', 's15a37', 's18250', 's28902', 's32107', 's15a37', 's15a37', 's2ff00', 's15a10', 's26500', 's15a18', 's14c01', 's22a07', 's20301', 's20500', 's20500', 's10043', 's2d600', 's38800', 's18217', 's1821f', 's20500', 's2d600', 's20500', 's30a00', 's20330', 's20330', 's15039', 's15031', 's26520', 's20500', 's20500', 's10043', 's2d600', 's18520', 's2b707', 's18528', 's2b717', 's32107', 's15a37', 's15a37', 's38900', 's32107', 's15a37', 's15a37', 's2ff00', 's15a10', 's26500', 's1f720', 's1f728', 's26520', 's15a18', 's14c01', 's22a07', 's20301', 's20500', 's20500', 's10043', 's2d600', 's38800']
2021-10-18 17:05:02,345 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁"', 'ent', '▁"', '▁"', '▁"', '▁rom', '▁rom', '▁rom', '▁rom', '▁rom', '▁rom', '▁rom', '▁rom', 'ant', 'ant', 'ant', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁come', 'ant', '▁come', '▁whole', '▁3.', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', '▁whole', '▁come', '▁come', '▁come', '▁whole', '▁whole', '▁3.', '▁whole', '▁3.', '▁whole', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁come', '▁come', '▁whole', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', '▁whole', '▁3.', '▁3.', '▁whole', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', '▁whole', '▁3.', '▁whole', '▁3.', '▁whole', '▁whole', '▁3.', '▁whole', '▁3.', '▁3.', '▁whole', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', 't', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', 't', '▁3.', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '.', '▁back', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', ':', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', 't', '▁3.', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', 't', '▁3.', '▁3.', '▁3.', '▁3.', ':', ':', '▁3.']
2021-10-18 17:05:02,345 - INFO - joeynmt.training - 	Source:     s15a07 s1f010 s26507 s1f540 s38800 s32107 s15a37 s15a37 s15a18 s14c01 s22a07 s20301 s20500 s20500 s10043 s2d600 s10018 s26505 s10641 s15a30 s15a50 s2a208 s2a218 s1f710 s20302 s2ea08 s38900 s15a37 s18250 s28902 s32107 s15a37 s15a37 s2ff00 s15a10 s26500 s15a18 s14c01 s22a07 s20301 s20500 s20500 s10043 s2d600 s38800 s18217 s1821f s20500 s2d600 s20500 s30a00 s20330 s20330 s15039 s15031 s26520 s20500 s20500 s10043 s2d600 s18520 s2b707 s18528 s2b717 s32107 s15a37 s15a37 s38900 s32107 s15a37 s15a37 s2ff00 s15a10 s26500 s1f720 s1f728 s26520 s15a18 s14c01 s22a07 s20301 s20500 s20500 s10043 s2d600 s38800
2021-10-18 17:05:02,345 - INFO - joeynmt.training - 	Reference:  ▁proverbs ▁3 1 :4 ▁it ▁is ▁not ▁for ▁kings , ▁o ▁l em u el , ▁it ▁is ▁not ▁for ▁kings ▁to ▁drink ▁wine ; ▁nor ▁for ▁pr in ces ▁strong ▁drink \ n \ n o h ▁you ▁l em u el , ▁wine ▁kings ▁should ▁drink ▁not , ▁and ▁pr in ces ▁should ▁al c o h ol ▁drink ▁not .
2021-10-18 17:05:02,345 - INFO - joeynmt.training - 	Hypothesis: ▁" ent ▁" ▁" ▁" ▁rom ▁rom ▁rom ▁rom ▁rom ▁rom ▁rom ▁rom ant ant ant ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁come ant ▁come ▁whole ▁3. ▁3. ▁3. ▁3. ▁whole ▁3. ▁whole ▁3. ▁3. ▁3. ▁3. ▁whole ▁3. ▁whole ▁come ▁come ▁come ▁whole ▁whole ▁3. ▁whole ▁3. ▁whole ▁3. ▁whole ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁come ▁come ▁whole ▁3. ▁3. ▁3. ▁3. ▁3. ▁whole ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁whole ▁3. ▁whole ▁3. ▁3. ▁whole ▁3. ▁whole ▁3. ▁3. ▁3. ▁3. ▁whole ▁3. ▁whole ▁3. ▁3. ▁3. ▁whole ▁3. ▁whole ▁3. ▁whole ▁3. ▁whole ▁whole ▁3. ▁whole ▁3. ▁3. ▁whole ▁3. ▁whole ▁3. ▁3. ▁3. ▁3. ▁whole ▁3. t ▁3. ▁3. ▁3. ▁whole ▁3. ▁whole ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. t ▁3. ▁3. ▁3. ▁3. ▁whole ▁3. ▁whole ▁3. ▁3. ▁3. ▁3. ▁3. ▁whole ▁3. ▁3. ▁3. ▁3. ▁3. ▁whole ▁3. ▁3. ▁3. . ▁back ▁3. ▁whole ▁3. ▁3. ▁3. ▁whole ▁3. : ▁3. ▁3. ▁3. ▁3. ▁3. t ▁3. ▁3. ▁3. ▁3. ▁whole ▁3. t ▁3. ▁3. ▁3. ▁3. : : ▁3.
2021-10-18 17:05:02,345 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step        9: bleu:   0.14, loss: 306.2266, ppl: 119.6762, duration: 1.6889s
2021-10-18 17:05:02,463 - INFO - joeynmt.training - Epoch   2, Step:       10, Batch Loss:     4.664668, Tokens per Sec:     1237, Lr: 0.000070
2021-10-18 17:05:04,154 - INFO - joeynmt.training - Example #0
2021-10-18 17:05:04,154 - DEBUG - joeynmt.training - 	Raw source:     ['s15a07', 's1f010', 's26507', 's1f540', 's38800', 's32107', 's15a37', 's15a37', 's15a18', 's14c01', 's22a07', 's20301', 's20500', 's20500', 's10043', 's2d600', 's10018', 's26505', 's10641', 's15a30', 's15a50', 's2a208', 's2a218', 's1f710', 's20302', 's2ea08', 's38900', 's15a37', 's18250', 's28902', 's32107', 's15a37', 's15a37', 's2ff00', 's15a10', 's26500', 's15a18', 's14c01', 's22a07', 's20301', 's20500', 's20500', 's10043', 's2d600', 's38800', 's18217', 's1821f', 's20500', 's2d600', 's20500', 's30a00', 's20330', 's20330', 's15039', 's15031', 's26520', 's20500', 's20500', 's10043', 's2d600', 's18520', 's2b707', 's18528', 's2b717', 's32107', 's15a37', 's15a37', 's38900', 's32107', 's15a37', 's15a37', 's2ff00', 's15a10', 's26500', 's1f720', 's1f728', 's26520', 's15a18', 's14c01', 's22a07', 's20301', 's20500', 's20500', 's10043', 's2d600', 's38800']
2021-10-18 17:05:04,154 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁"', 'ent', '▁"', '▁"', '▁"', '▁rom', '▁rom', '▁rom', 'ent', '▁rom', '▁rom', '▁rom', '▁rom', 'ant', 'ant', 'ant', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁come', 'ant', '▁come', '▁whole', '▁3.', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', '▁whole', '▁come', '▁come', '▁come', '▁whole', '▁whole', '▁3.', '▁whole', '▁3.', '▁whole', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', '▁whole', '▁3.', '▁whole', '▁3.', '▁whole', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', '▁whole', '▁3.', '▁whole', '▁3.', '▁whole', '▁whole', '▁3.', '▁whole', '▁3.', '▁3.', '▁whole', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', 't', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', 't', '▁3.', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', '▁whole', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', ':', '▁3.', '▁3.', '▁3.', '▁3.', '▁3.', 't', '▁3.', '▁3.', '▁3.', '▁3.', '▁whole', '▁3.', 't', '▁3.', '▁3.', '▁3.', '▁3.', ':', '▁3.', '▁3.']
2021-10-18 17:05:04,154 - INFO - joeynmt.training - 	Source:     s15a07 s1f010 s26507 s1f540 s38800 s32107 s15a37 s15a37 s15a18 s14c01 s22a07 s20301 s20500 s20500 s10043 s2d600 s10018 s26505 s10641 s15a30 s15a50 s2a208 s2a218 s1f710 s20302 s2ea08 s38900 s15a37 s18250 s28902 s32107 s15a37 s15a37 s2ff00 s15a10 s26500 s15a18 s14c01 s22a07 s20301 s20500 s20500 s10043 s2d600 s38800 s18217 s1821f s20500 s2d600 s20500 s30a00 s20330 s20330 s15039 s15031 s26520 s20500 s20500 s10043 s2d600 s18520 s2b707 s18528 s2b717 s32107 s15a37 s15a37 s38900 s32107 s15a37 s15a37 s2ff00 s15a10 s26500 s1f720 s1f728 s26520 s15a18 s14c01 s22a07 s20301 s20500 s20500 s10043 s2d600 s38800
2021-10-18 17:05:04,154 - INFO - joeynmt.training - 	Reference:  ▁proverbs ▁3 1 :4 ▁it ▁is ▁not ▁for ▁kings , ▁o ▁l em u el , ▁it ▁is ▁not ▁for ▁kings ▁to ▁drink ▁wine ; ▁nor ▁for ▁pr in ces ▁strong ▁drink \ n \ n o h ▁you ▁l em u el , ▁wine ▁kings ▁should ▁drink ▁not , ▁and ▁pr in ces ▁should ▁al c o h ol ▁drink ▁not .
2021-10-18 17:05:04,154 - INFO - joeynmt.training - 	Hypothesis: ▁" ent ▁" ▁" ▁" ▁rom ▁rom ▁rom ent ▁rom ▁rom ▁rom ▁rom ant ant ant ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁come ant ▁come ▁whole ▁3. ▁3. ▁3. ▁3. ▁whole ▁3. ▁whole ▁3. ▁3. ▁3. ▁3. ▁whole ▁3. ▁whole ▁come ▁come ▁come ▁whole ▁whole ▁3. ▁whole ▁3. ▁whole ▁3. ▁whole ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁whole ▁3. ▁3. ▁3. ▁3. ▁3. ▁whole ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁whole ▁3. ▁whole ▁3. ▁whole ▁3. ▁whole ▁3. ▁whole ▁3. ▁3. ▁3. ▁whole ▁3. ▁3. ▁3. ▁3. ▁3. ▁whole ▁3. ▁whole ▁3. ▁whole ▁3. ▁whole ▁whole ▁3. ▁whole ▁3. ▁3. ▁whole ▁3. ▁whole ▁3. ▁3. ▁3. ▁3. ▁whole ▁3. t ▁3. ▁3. ▁3. ▁whole ▁3. ▁whole ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. ▁3. t ▁3. ▁3. ▁3. ▁3. ▁whole ▁3. ▁whole ▁3. ▁3. ▁3. ▁3. ▁3. ▁whole ▁3. ▁3. ▁3. ▁3. ▁3. ▁whole ▁3. ▁3. ▁3. ▁3. ▁whole ▁3. ▁whole ▁3. ▁3. ▁3. ▁whole ▁3. : ▁3. ▁3. ▁3. ▁3. ▁3. t ▁3. ▁3. ▁3. ▁3. ▁whole ▁3. t ▁3. ▁3. ▁3. ▁3. : ▁3. ▁3.
2021-10-18 17:05:04,155 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step       10: bleu:   0.14, loss: 306.2990, ppl: 119.8117, duration: 1.6916s
2021-10-18 17:05:04,155 - INFO - joeynmt.training - Epoch   2: total training loss 23.22
2021-10-18 17:05:04,155 - INFO - joeynmt.training - Training ended after   2 epochs.
2021-10-18 17:05:04,155 - INFO - joeynmt.training - Best validation result (greedy) at step        1: 304.09 loss.
2021-10-18 17:05:04,173 - INFO - joeynmt.prediction - Process device: cpu, n_gpu: 0, batch_size per device: 2
2021-10-18 17:05:04,173 - INFO - joeynmt.prediction - Loading model from models/baseline_transformer_debug/1.ckpt
2021-10-18 17:05:04,253 - INFO - joeynmt.model - Building an encoder-decoder model...
2021-10-18 17:05:04,271 - INFO - joeynmt.model - Enc-dec model built.
2021-10-18 17:05:04,288 - INFO - joeynmt.prediction - Decoding on dev set (data/tiny.spm.en)...
2021-10-18 17:05:08,990 - INFO - joeynmt.prediction -  dev bleu[13a]:   0.18 [Beam search decoding with beam size = 5 and alpha = 1.0]
2021-10-18 17:05:08,991 - INFO - joeynmt.prediction - Translations saved to: models/baseline_transformer_debug/00000001.hyps.dev
2021-10-18 17:05:08,991 - INFO - joeynmt.prediction - Decoding on test set (data/tiny.spm.en)...
2021-10-18 17:05:13,666 - INFO - joeynmt.prediction - test bleu[13a]:   0.18 [Beam search decoding with beam size = 5 and alpha = 1.0]
2021-10-18 17:05:13,666 - INFO - joeynmt.prediction - Translations saved to: models/baseline_transformer_debug/00000001.hyps.test
