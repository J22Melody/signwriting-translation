2021-10-20 19:47:24,514 - INFO - root - Hello! This is Joey-NMT (version 1.3).
2021-10-20 19:47:24,630 - INFO - joeynmt.data - Loading training data...
2021-10-20 19:47:25,471 - INFO - joeynmt.data - Building vocabulary...
2021-10-20 19:47:26,085 - INFO - joeynmt.data - Loading dev data...
2021-10-20 19:47:26,124 - INFO - joeynmt.data - Loading test data...
2021-10-20 19:47:26,152 - INFO - joeynmt.data - Data loaded.
2021-10-20 19:47:26,152 - INFO - joeynmt.model - Building an encoder-decoder model...
2021-10-20 19:47:26,780 - INFO - joeynmt.model - Enc-dec model built.
2021-10-20 19:47:29,421 - DEBUG - tensorflow - Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.
2021-10-20 19:47:30,012 - DEBUG - h5py._conv - Creating converter from 7 to 5
2021-10-20 19:47:30,013 - DEBUG - h5py._conv - Creating converter from 5 to 7
2021-10-20 19:47:30,013 - DEBUG - h5py._conv - Creating converter from 7 to 5
2021-10-20 19:47:30,013 - DEBUG - h5py._conv - Creating converter from 5 to 7
2021-10-20 19:47:31,876 - INFO - joeynmt.training - Total params: 49478656
2021-10-20 19:47:31,878 - DEBUG - joeynmt.training - Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'encoder.layers.4.feed_forward.layer_norm.bias', 'encoder.layers.4.feed_forward.layer_norm.weight', 'encoder.layers.4.feed_forward.pwff_layer.0.bias', 'encoder.layers.4.feed_forward.pwff_layer.0.weight', 'encoder.layers.4.feed_forward.pwff_layer.3.bias', 'encoder.layers.4.feed_forward.pwff_layer.3.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.4.src_src_att.k_layer.bias', 'encoder.layers.4.src_src_att.k_layer.weight', 'encoder.layers.4.src_src_att.output_layer.bias', 'encoder.layers.4.src_src_att.output_layer.weight', 'encoder.layers.4.src_src_att.q_layer.bias', 'encoder.layers.4.src_src_att.q_layer.weight', 'encoder.layers.4.src_src_att.v_layer.bias', 'encoder.layers.4.src_src_att.v_layer.weight', 'encoder.layers.5.feed_forward.layer_norm.bias', 'encoder.layers.5.feed_forward.layer_norm.weight', 'encoder.layers.5.feed_forward.pwff_layer.0.bias', 'encoder.layers.5.feed_forward.pwff_layer.0.weight', 'encoder.layers.5.feed_forward.pwff_layer.3.bias', 'encoder.layers.5.feed_forward.pwff_layer.3.weight', 'encoder.layers.5.layer_norm.bias', 'encoder.layers.5.layer_norm.weight', 'encoder.layers.5.src_src_att.k_layer.bias', 'encoder.layers.5.src_src_att.k_layer.weight', 'encoder.layers.5.src_src_att.output_layer.bias', 'encoder.layers.5.src_src_att.output_layer.weight', 'encoder.layers.5.src_src_att.q_layer.bias', 'encoder.layers.5.src_src_att.q_layer.weight', 'encoder.layers.5.src_src_att.v_layer.bias', 'encoder.layers.5.src_src_att.v_layer.weight', 'src_embed.lut.weight', 'trg_embed.lut.weight']
2021-10-20 19:47:31,879 - WARNING - joeynmt.training - `keep_last_ckpts` option is outdated. Please use `keep_best_ckpts`, instead.
2021-10-20 19:47:36,610 - INFO - joeynmt.helpers - cfg.name                           : baseline_transformer_spm
2021-10-20 19:47:36,610 - INFO - joeynmt.helpers - cfg.data.src                       : sign
2021-10-20 19:47:36,610 - INFO - joeynmt.helpers - cfg.data.trg                       : spm.en
2021-10-20 19:47:36,611 - INFO - joeynmt.helpers - cfg.data.train                     : data_full/train.withDict
2021-10-20 19:47:36,611 - INFO - joeynmt.helpers - cfg.data.dev                       : data_full/dev
2021-10-20 19:47:36,611 - INFO - joeynmt.helpers - cfg.data.test                      : data_full/test
2021-10-20 19:47:36,611 - INFO - joeynmt.helpers - cfg.data.level                     : word
2021-10-20 19:47:36,611 - INFO - joeynmt.helpers - cfg.data.lowercase                 : True
2021-10-20 19:47:36,611 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 500
2021-10-20 19:47:36,611 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5
2021-10-20 19:47:36,611 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0
2021-10-20 19:47:36,611 - INFO - joeynmt.helpers - cfg.testing.postprocess            : False
2021-10-20 19:47:36,611 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42
2021-10-20 19:47:36,611 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam
2021-10-20 19:47:36,611 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens
2021-10-20 19:47:36,611 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]
2021-10-20 19:47:36,611 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau
2021-10-20 19:47:36,611 - INFO - joeynmt.helpers - cfg.training.patience              : 5
2021-10-20 19:47:36,611 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7
2021-10-20 19:47:36,611 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy
2021-10-20 19:47:36,611 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0001
2021-10-20 19:47:36,611 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08
2021-10-20 19:47:36,611 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0
2021-10-20 19:47:36,611 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1
2021-10-20 19:47:36,611 - INFO - joeynmt.helpers - cfg.training.batch_size            : 32
2021-10-20 19:47:36,611 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 32
2021-10-20 19:47:36,611 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1
2021-10-20 19:47:36,611 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : eval_metric
2021-10-20 19:47:36,611 - INFO - joeynmt.helpers - cfg.training.epochs                : 200
2021-10-20 19:47:36,611 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 1000
2021-10-20 19:47:36,611 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 100
2021-10-20 19:47:36,612 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu
2021-10-20 19:47:36,612 - INFO - joeynmt.helpers - cfg.training.overwrite             : True
2021-10-20 19:47:36,612 - INFO - joeynmt.helpers - cfg.training.shuffle               : True
2021-10-20 19:47:36,612 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True
2021-10-20 19:47:36,612 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 200
2021-10-20 19:47:36,612 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3, 6]
2021-10-20 19:47:36,612 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3
2021-10-20 19:47:36,612 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/baseline_transformer_full
2021-10-20 19:47:36,612 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier
2021-10-20 19:47:36,612 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros
2021-10-20 19:47:36,612 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0
2021-10-20 19:47:36,612 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier
2021-10-20 19:47:36,612 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0
2021-10-20 19:47:36,612 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer
2021-10-20 19:47:36,612 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6
2021-10-20 19:47:36,612 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 8
2021-10-20 19:47:36,612 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 512
2021-10-20 19:47:36,612 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True
2021-10-20 19:47:36,612 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.0
2021-10-20 19:47:36,612 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 512
2021-10-20 19:47:36,612 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 2048
2021-10-20 19:47:36,612 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.1
2021-10-20 19:47:36,612 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer
2021-10-20 19:47:36,612 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6
2021-10-20 19:47:36,612 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 8
2021-10-20 19:47:36,612 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 512
2021-10-20 19:47:36,612 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True
2021-10-20 19:47:36,612 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.0
2021-10-20 19:47:36,612 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 512
2021-10-20 19:47:36,612 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 2048
2021-10-20 19:47:36,612 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.1
2021-10-20 19:47:36,612 - INFO - joeynmt.helpers - Data set sizes: 
	train 36111,
	valid 1141,
	test 761
2021-10-20 19:47:36,612 - INFO - joeynmt.helpers - First training example:
	[SRC] s20357 s20351 s26505 s20500 s20500 s20500 s20500 s20500 s20351 s20357 s26505 s20500 s20500 s20500 s20500 s20500
	[TRG] ▁care er
2021-10-20 19:47:36,612 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) s20500 (5) s2ff00 (6) s38700 (7) s38800 (8) s22a04 (9) s30a00
2021-10-20 19:47:36,612 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) ▁the (6) . (7) ▁and (8) ▁of (9) ▁to
2021-10-20 19:47:36,612 - INFO - joeynmt.helpers - Number of Src words (types): 6452
2021-10-20 19:47:36,613 - INFO - joeynmt.helpers - Number of Trg words (types): 1987
2021-10-20 19:47:36,613 - INFO - joeynmt.training - Model(
	encoder=TransformerEncoder(num_layers=6, num_heads=8),
	decoder=TransformerDecoder(num_layers=6, num_heads=8),
	src_embed=Embeddings(embedding_dim=512, vocab_size=6452),
	trg_embed=Embeddings(embedding_dim=512, vocab_size=1987))
2021-10-20 19:47:36,619 - INFO - joeynmt.training - Train stats:
	device: cuda
	n_gpu: 1
	16-bits training: False
	gradient accumulation: 1
	batch size per device: 32
	total batch size (w. parallel & accumulation): 32
2021-10-20 19:47:36,620 - INFO - joeynmt.training - EPOCH 1
2021-10-20 19:47:49,650 - INFO - joeynmt.training - Epoch   1, Step:      100, Batch Loss:     5.210285, Tokens per Sec:     3633, Lr: 0.000100
2021-10-20 19:48:02,439 - INFO - joeynmt.training - Epoch   1, Step:      200, Batch Loss:     4.696100, Tokens per Sec:     3839, Lr: 0.000100
2021-10-20 19:48:14,966 - INFO - joeynmt.training - Epoch   1, Step:      300, Batch Loss:     4.582442, Tokens per Sec:     3846, Lr: 0.000100
2021-10-20 19:48:27,491 - INFO - joeynmt.training - Epoch   1, Step:      400, Batch Loss:     4.515465, Tokens per Sec:     3831, Lr: 0.000100
2021-10-20 19:48:39,807 - INFO - joeynmt.training - Epoch   1, Step:      500, Batch Loss:     4.243866, Tokens per Sec:     3912, Lr: 0.000100
2021-10-20 19:48:52,076 - INFO - joeynmt.training - Epoch   1, Step:      600, Batch Loss:     4.245036, Tokens per Sec:     3884, Lr: 0.000100
2021-10-20 19:49:04,343 - INFO - joeynmt.training - Epoch   1, Step:      700, Batch Loss:     4.086593, Tokens per Sec:     3912, Lr: 0.000100
2021-10-20 19:49:16,887 - INFO - joeynmt.training - Epoch   1, Step:      800, Batch Loss:     3.985582, Tokens per Sec:     3953, Lr: 0.000100
2021-10-20 19:49:29,006 - INFO - joeynmt.training - Epoch   1, Step:      900, Batch Loss:     4.056800, Tokens per Sec:     4035, Lr: 0.000100
2021-10-20 19:49:40,737 - INFO - joeynmt.training - Epoch   1, Step:     1000, Batch Loss:     4.060036, Tokens per Sec:     3927, Lr: 0.000100
2021-10-20 19:50:33,413 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-20 19:50:33,414 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-20 19:50:33,414 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-20 19:50:33,422 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-20 19:50:34,579 - INFO - joeynmt.training - Example #0
2021-10-20 19:50:34,579 - DEBUG - joeynmt.training - 	Raw source:     ['s15a30', 's15a30', 's28812', 's28802', 's36d01', 's2ff00', 's10011', 's2e707', 's19a10', 's26500', 's32107', 's32107', 's1004a', 's10041', 's22f04', 's38700', 's15a37', 's15a3f', 's22f04', 's36d01', 's2ff00', 's10e00', 's10e50', 's15a1a', 's26507', 's26512', 's20318', 's2ff00', 's16d18', 's16d10', 's26506', 's20310', 's32107', 's38700', 's10058', 's26526', 's10050', 's20500', 's20500', 's36d01', 's1f702', 's1f70a', 's22a00', 's15a37', 's15a37', 's15a51', 's11541', 's26507', 's2d612', 's2d60a', 's14248', 's14240', 's19a21', 's2e932', 's38800']
2021-10-20 19:50:34,579 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁verse', '▁12.', '▁the', '▁lord', '▁said', '▁to', '▁the', '▁lord', ',', '▁"', 'the', '▁people', '▁of', '▁the', '▁lord', ',', '▁"', 'the', '▁people', '▁are', '▁not', '▁be', '▁a', '▁man', '▁who', '▁are', '▁not', '▁be', '▁a', '▁man', '▁to', '▁the', '▁lord', '.']
2021-10-20 19:50:34,580 - INFO - joeynmt.training - 	Source:     s15a30 s15a30 s28812 s28802 s36d01 s2ff00 s10011 s2e707 s19a10 s26500 s32107 s32107 s1004a s10041 s22f04 s38700 s15a37 s15a3f s22f04 s36d01 s2ff00 s10e00 s10e50 s15a1a s26507 s26512 s20318 s2ff00 s16d18 s16d10 s26506 s20310 s32107 s38700 s10058 s26526 s10050 s20500 s20500 s36d01 s1f702 s1f70a s22a00 s15a37 s15a37 s15a51 s11541 s26507 s2d612 s2d60a s14248 s14240 s19a21 s2e932 s38800
2021-10-20 19:50:34,580 - INFO - joeynmt.training - 	Reference:  ▁to ▁bring ▁in ▁ever l ast ing ▁righteousness , ▁to ▁conf irm ▁the ▁prophet ic ▁vision , ▁and ▁to ▁an oint ▁the ▁most ▁holy ▁place .
2021-10-20 19:50:34,580 - INFO - joeynmt.training - 	Hypothesis: ▁verse ▁12. ▁the ▁lord ▁said ▁to ▁the ▁lord , ▁" the ▁people ▁of ▁the ▁lord , ▁" the ▁people ▁are ▁not ▁be ▁a ▁man ▁who ▁are ▁not ▁be ▁a ▁man ▁to ▁the ▁lord .
2021-10-20 19:50:34,580 - INFO - joeynmt.training - Example #1
2021-10-20 19:50:34,580 - DEBUG - joeynmt.training - 	Raw source:     ['s1bb20', 's38800', 's22a17', 's20b00', 's20357', 's15a57', 's15a57', 's22b07', 's20351', 's30a00', 's17620', 's1dc20', 's19220', 's14a20', 's20320', 's10e20', 's38700', 's1c519', 's1c511', 's15a17', 's15a1f', 's10028', 's10020', 's2b700', 's37c06', 's37c06', 's2b711', 's15a37', 's18220', 's28802', 's38700']
2021-10-20 19:50:34,580 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁verse', '▁12.', '▁the', '▁people', '▁of', '▁the', '▁people', '▁of', '▁the', '▁people', '▁of', '▁the', '▁people', '▁of', '▁the', '▁people', '▁of', '▁the', '▁people', '▁of', '▁the', '▁people', '.']
2021-10-20 19:50:34,580 - INFO - joeynmt.training - 	Source:     s1bb20 s38800 s22a17 s20b00 s20357 s15a57 s15a57 s22b07 s20351 s30a00 s17620 s1dc20 s19220 s14a20 s20320 s10e20 s38700 s1c519 s1c511 s15a17 s15a1f s10028 s10020 s2b700 s37c06 s37c06 s2b711 s15a37 s18220 s28802 s38700
2021-10-20 19:50:34,580 - INFO - joeynmt.training - 	Reference:  ▁jesus ▁returned ▁to ▁the ▁mount ▁of ▁o l ive s ,
2021-10-20 19:50:34,580 - INFO - joeynmt.training - 	Hypothesis: ▁verse ▁12. ▁the ▁people ▁of ▁the ▁people ▁of ▁the ▁people ▁of ▁the ▁people ▁of ▁the ▁people ▁of ▁the ▁people ▁of ▁the ▁people .
2021-10-20 19:50:34,580 - INFO - joeynmt.training - Example #2
2021-10-20 19:50:34,580 - DEBUG - joeynmt.training - 	Raw source:     ['s15a07', 's1f010', 's26507', 's1eb20', 's22114', 's38800', 's1dc0a', 's10003', 's20500', 's20500', 's30a00', 's10118', 's10120', 's29306', 's29316', 's15a3f', 's15a3f', 's22a14', 's22a04', 's38700', 's10047', 's15a57', 's15a5f', 's22f04', 's10041', 's2d60e', 's25500', 's2ff00', 's11010', 's14c39', 's26516', 's14c31', 's26502', 's30a00', 's38900', 's26500', 's1f540', 's30122', 's10000', 's2ff00', 's2b700', 's1f540', 's1f550', 's28806', 's14220', 's22a14', 's22a04', 's14228', 's10047', 's18d21', 's22a05', 's20500', 's20500', 's10047', 's38800']
2021-10-20 19:50:34,580 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁verse', '▁12.', '▁the', '▁people', '▁who', '▁are', '▁not', '▁be', '▁afraid', ',', '▁and', '▁they', '▁were', '▁not', '▁be', '▁afraid', ',', '▁and', '▁they', '▁were', '▁not', '▁be', '▁afraid', '.']
2021-10-20 19:50:34,580 - INFO - joeynmt.training - 	Source:     s15a07 s1f010 s26507 s1eb20 s22114 s38800 s1dc0a s10003 s20500 s20500 s30a00 s10118 s10120 s29306 s29316 s15a3f s15a3f s22a14 s22a04 s38700 s10047 s15a57 s15a5f s22f04 s10041 s2d60e s25500 s2ff00 s11010 s14c39 s26516 s14c31 s26502 s30a00 s38900 s26500 s1f540 s30122 s10000 s2ff00 s2b700 s1f540 s1f550 s28806 s14220 s22a14 s22a04 s14228 s10047 s18d21 s22a05 s20500 s20500 s10047 s38800
2021-10-20 19:50:34,580 - INFO - joeynmt.training - 	Reference:  ▁then ▁he ▁s tern ly ▁warn ed ▁the ▁disciples ▁not ▁to ▁tell ▁anyone ▁that ▁he ▁was ▁the ▁messiah .
2021-10-20 19:50:34,580 - INFO - joeynmt.training - 	Hypothesis: ▁verse ▁12. ▁the ▁people ▁who ▁are ▁not ▁be ▁afraid , ▁and ▁they ▁were ▁not ▁be ▁afraid , ▁and ▁they ▁were ▁not ▁be ▁afraid .
2021-10-20 19:50:34,580 - INFO - joeynmt.training - Example #3
2021-10-20 19:50:34,580 - DEBUG - joeynmt.training - 	Raw source:     ['s15a07', 's1f010', 's26507', 's1a520', 's1f540', 's38800', 's20500', 's10043', 's30a00', 's30124', 's10010', 's26500', 's22f10', 's20500', 's15a0a', 's22f00', 's15a02', 's20500', 's20320', 's1fb20', 's1f720', 's20320', 's1f720', 's11920', 's11502', 's14a20', 's14251', 's38700', 's1ce20', 's17620', 's11a20', 's11920', 's11520', 's1fb20', 's20320', 's1f720', 's1fb20', 's11520', 's38700', 's1f720', 's20320', 's16d20', 's11502', 's11520', 's19220', 's1f720', 's16d20', 's14c30', 's14c38', 's2a204', 's2a21c', 's10009', 's10002', 's2b714', 's2b705', 's15a30', 's15a30', 's2ea36', 's2ea4e', 's38800', 's2ff00', 's10001', 's10009', 's2b711', 's2b700', 's30a00', 's11e30', 's2eb06', 's1853f', 's18537', 's2b705', 's2b714', 's20500', 's10043', 's15a37', 's1f702', 's20600', 's38700', 's15a3f', 's15a37', 's2d608', 's2d610', 's26500', 's1f540', 's30122', 's15a30', 's15a30', 's2ea36', 's2ea4e', 's16d51', 's16d51', 's26c0a', 's26c1a', 's38800']
2021-10-20 19:50:34,580 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁verse', '▁12.', '▁i', '▁am', '▁not', '▁to', '▁the', '▁lord', ',', '▁i', '▁am', '▁not', '▁be', '▁a', '▁man', '▁who', '▁are', '▁not', '▁to', '▁the', '▁lord', '.', '▁i', '▁am', '▁not', '▁be', '▁a', '▁man', '▁who', '▁are', '▁not', '▁to', '▁the', '▁lord', ',', '▁and', '▁i', '▁am', '▁not', '▁be', '▁afraid', '▁of', '▁the', '▁lord', '.']
2021-10-20 19:50:34,580 - INFO - joeynmt.training - 	Source:     s15a07 s1f010 s26507 s1a520 s1f540 s38800 s20500 s10043 s30a00 s30124 s10010 s26500 s22f10 s20500 s15a0a s22f00 s15a02 s20500 s20320 s1fb20 s1f720 s20320 s1f720 s11920 s11502 s14a20 s14251 s38700 s1ce20 s17620 s11a20 s11920 s11520 s1fb20 s20320 s1f720 s1fb20 s11520 s38700 s1f720 s20320 s16d20 s11502 s11520 s19220 s1f720 s16d20 s14c30 s14c38 s2a204 s2a21c s10009 s10002 s2b714 s2b705 s15a30 s15a30 s2ea36 s2ea4e s38800 s2ff00 s10001 s10009 s2b711 s2b700 s30a00 s11e30 s2eb06 s1853f s18537 s2b705 s2b714 s20500 s10043 s15a37 s1f702 s20600 s38700 s15a3f s15a37 s2d608 s2d610 s26500 s1f540 s30122 s15a30 s15a30 s2ea36 s2ea4e s16d51 s16d51 s26c0a s26c1a s38800
2021-10-20 19:50:34,580 - INFO - joeynmt.training - 	Reference:  ▁verse ▁17. ▁i ▁am ▁very ▁gl ad ▁that ▁step h an as , ▁fort un at us , ▁and ▁a ch a ic us ▁have ▁come ▁here . ▁they ▁have ▁been ▁pro vid ing ▁the ▁help ▁you ▁w er en ' t ▁here ▁to ▁give ▁me .
2021-10-20 19:50:34,580 - INFO - joeynmt.training - 	Hypothesis: ▁verse ▁12. ▁i ▁am ▁not ▁to ▁the ▁lord , ▁i ▁am ▁not ▁be ▁a ▁man ▁who ▁are ▁not ▁to ▁the ▁lord . ▁i ▁am ▁not ▁be ▁a ▁man ▁who ▁are ▁not ▁to ▁the ▁lord , ▁and ▁i ▁am ▁not ▁be ▁afraid ▁of ▁the ▁lord .
2021-10-20 19:50:34,580 - INFO - joeynmt.training - Example #6
2021-10-20 19:50:34,580 - DEBUG - joeynmt.training - 	Raw source:     ['s15a07', 's1f010', 's26507', 's14450', 's23004', 's38800', 's1dc0a', 's10003', 's20500', 's20500', 's36d01', 's30a00', 's15a21', 's2ff00', 's22a03', 's15a00', 's1d451', 's1d437', 's22105', 's22a00', 's2ff00', 's20600', 's10001', 's38700', 's22a14', 's14220', 's22a04', 's14228', 's36d03', 's30d00', 's20500', 's10043', 's14c00', 's14c08', 's2a204', 's2a21c', 's14c59', 's14c57', 's1f750', 's1f759', 's26503', 's26513', 's38700', 's20500', 's10043', 's36d03', 's10057', 's10059', 's2b707', 's2b717', 's15a37', 's18250', 's28902', 's2ff00', 's15a10', 's2b700', 's38800', 's14c37', 's14c3f', 's22a04', 's22a14', 's30a00', 's36d01', 's15a37', 's18250', 's28902', 's10057', 's10059', 's2b707', 's2b717', 's38700', 's1d420', 's22e00', 's1d121', 's36d01', 's14c39', 's26516', 's14c31', 's26502', 's30d00', 's38900', 's2d612', 's2d60a', 's14248', 's14240', 's15a06', 's15a0e', 's26504', 's19a30', 's19a38', 's22a04', 's22a14', 's1c550', 's26507', 's15a57', 's38700', 's15a37', 's15a42', 's2880e', 's2880e', 's20e00', 's15a31', 's14c00', 's14c08', 's2a204', 's2a21c', 's38700', 's15a48', 's15a40', 's24916', 's24906', 's38800']
2021-10-20 19:50:34,580 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁verse', '▁12.', '▁i', '▁will', '▁be', '▁a', '▁man', '▁who', '▁are', '▁not', '▁to', '▁the', '▁lord', ',', '▁and', '▁i', '▁will', '▁be', '▁a', '▁man', '▁who', '▁are', '▁a', '▁man', '▁who', '▁are', '▁not', '▁be', '▁a', '▁man', '▁who', '▁are', '▁a', '▁man', '▁who', '▁are', '▁a', '▁man', '▁who', '▁are', '▁a', '▁man', '.']
2021-10-20 19:50:34,580 - INFO - joeynmt.training - 	Source:     s15a07 s1f010 s26507 s14450 s23004 s38800 s1dc0a s10003 s20500 s20500 s36d01 s30a00 s15a21 s2ff00 s22a03 s15a00 s1d451 s1d437 s22105 s22a00 s2ff00 s20600 s10001 s38700 s22a14 s14220 s22a04 s14228 s36d03 s30d00 s20500 s10043 s14c00 s14c08 s2a204 s2a21c s14c59 s14c57 s1f750 s1f759 s26503 s26513 s38700 s20500 s10043 s36d03 s10057 s10059 s2b707 s2b717 s15a37 s18250 s28902 s2ff00 s15a10 s2b700 s38800 s14c37 s14c3f s22a04 s22a14 s30a00 s36d01 s15a37 s18250 s28902 s10057 s10059 s2b707 s2b717 s38700 s1d420 s22e00 s1d121 s36d01 s14c39 s26516 s14c31 s26502 s30d00 s38900 s2d612 s2d60a s14248 s14240 s15a06 s15a0e s26504 s19a30 s19a38 s22a04 s22a14 s1c550 s26507 s15a57 s38700 s15a37 s15a42 s2880e s2880e s20e00 s15a31 s14c00 s14c08 s2a204 s2a21c s38700 s15a48 s15a40 s24916 s24906 s38800
2021-10-20 19:50:34,580 - INFO - joeynmt.training - 	Reference:  ▁then ▁it ▁says , ▁' i ▁will ▁return ▁to ▁the ▁person ▁i ▁came ▁from .' ▁so ▁it ▁return s ▁and ▁find s ▁its ▁form er ▁home ▁emp ty , ▁sw ept , ▁and ▁in ▁ord er .
2021-10-20 19:50:34,581 - INFO - joeynmt.training - 	Hypothesis: ▁verse ▁12. ▁i ▁will ▁be ▁a ▁man ▁who ▁are ▁not ▁to ▁the ▁lord , ▁and ▁i ▁will ▁be ▁a ▁man ▁who ▁are ▁a ▁man ▁who ▁are ▁not ▁be ▁a ▁man ▁who ▁are ▁a ▁man ▁who ▁are ▁a ▁man ▁who ▁are ▁a ▁man .
2021-10-20 19:50:34,581 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step     1000: bleu:   1.51, loss: 169659.3281, ppl:  50.2840, duration: 53.8432s
2021-10-20 19:50:47,329 - INFO - joeynmt.training - Epoch   1, Step:     1100, Batch Loss:     3.921200, Tokens per Sec:     3793, Lr: 0.000100
2021-10-20 19:50:51,295 - INFO - joeynmt.training - Epoch   1: total training loss 4943.34
2021-10-20 19:50:51,296 - INFO - joeynmt.training - EPOCH 2
2021-10-20 19:51:00,025 - INFO - joeynmt.training - Epoch   2, Step:     1200, Batch Loss:     3.661844, Tokens per Sec:     3913, Lr: 0.000100
2021-10-20 19:51:12,527 - INFO - joeynmt.training - Epoch   2, Step:     1300, Batch Loss:     3.768696, Tokens per Sec:     3833, Lr: 0.000100
2021-10-20 19:51:25,700 - INFO - joeynmt.training - Epoch   2, Step:     1400, Batch Loss:     3.454464, Tokens per Sec:     3878, Lr: 0.000100
2021-10-20 19:51:38,109 - INFO - joeynmt.training - Epoch   2, Step:     1500, Batch Loss:     3.524070, Tokens per Sec:     3844, Lr: 0.000100
2021-10-20 19:51:50,966 - INFO - joeynmt.training - Epoch   2, Step:     1600, Batch Loss:     3.759737, Tokens per Sec:     3779, Lr: 0.000100
2021-10-20 19:52:03,393 - INFO - joeynmt.training - Epoch   2, Step:     1700, Batch Loss:     3.513960, Tokens per Sec:     3913, Lr: 0.000100
2021-10-20 19:52:15,969 - INFO - joeynmt.training - Epoch   2, Step:     1800, Batch Loss:     3.509732, Tokens per Sec:     3760, Lr: 0.000100
2021-10-20 19:52:28,883 - INFO - joeynmt.training - Epoch   2, Step:     1900, Batch Loss:     3.206292, Tokens per Sec:     3693, Lr: 0.000100
2021-10-20 19:52:41,738 - INFO - joeynmt.training - Epoch   2, Step:     2000, Batch Loss:     3.174213, Tokens per Sec:     3749, Lr: 0.000100
2021-10-20 19:54:36,707 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-20 19:54:36,707 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-20 19:54:36,707 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-20 19:54:36,716 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-20 19:54:37,855 - INFO - joeynmt.training - Example #0
2021-10-20 19:54:37,855 - DEBUG - joeynmt.training - 	Raw source:     ['s15a30', 's15a30', 's28812', 's28802', 's36d01', 's2ff00', 's10011', 's2e707', 's19a10', 's26500', 's32107', 's32107', 's1004a', 's10041', 's22f04', 's38700', 's15a37', 's15a3f', 's22f04', 's36d01', 's2ff00', 's10e00', 's10e50', 's15a1a', 's26507', 's26512', 's20318', 's2ff00', 's16d18', 's16d10', 's26506', 's20310', 's32107', 's38700', 's10058', 's26526', 's10050', 's20500', 's20500', 's36d01', 's1f702', 's1f70a', 's22a00', 's15a37', 's15a37', 's15a51', 's11541', 's26507', 's2d612', 's2d60a', 's14248', 's14240', 's19a21', 's2e932', 's38800']
2021-10-20 19:54:37,855 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁pr', '▁15', ':2', '0', '▁\\', 'xb', '6', '▁the', '▁lord', '▁[', 'is', ']', '▁the', '▁lord', ':', '▁and', '▁the', '▁lord', ':', '▁and', '▁the', '▁lord', '▁shall', '▁be', '▁a', '▁man', '▁of', '▁the', '▁lord', '.\\', 'n', '\\', 'n', 'f', 'ort', ',', '▁he', '▁lord', ',', '▁he', '▁lord', ',', '▁he', '▁lord', ',', '▁he', '▁lord', '.']
2021-10-20 19:54:37,855 - INFO - joeynmt.training - 	Source:     s15a30 s15a30 s28812 s28802 s36d01 s2ff00 s10011 s2e707 s19a10 s26500 s32107 s32107 s1004a s10041 s22f04 s38700 s15a37 s15a3f s22f04 s36d01 s2ff00 s10e00 s10e50 s15a1a s26507 s26512 s20318 s2ff00 s16d18 s16d10 s26506 s20310 s32107 s38700 s10058 s26526 s10050 s20500 s20500 s36d01 s1f702 s1f70a s22a00 s15a37 s15a37 s15a51 s11541 s26507 s2d612 s2d60a s14248 s14240 s19a21 s2e932 s38800
2021-10-20 19:54:37,855 - INFO - joeynmt.training - 	Reference:  ▁to ▁bring ▁in ▁ever l ast ing ▁righteousness , ▁to ▁conf irm ▁the ▁prophet ic ▁vision , ▁and ▁to ▁an oint ▁the ▁most ▁holy ▁place .
2021-10-20 19:54:37,856 - INFO - joeynmt.training - 	Hypothesis: ▁pr ▁15 :2 0 ▁\ xb 6 ▁the ▁lord ▁[ is ] ▁the ▁lord : ▁and ▁the ▁lord : ▁and ▁the ▁lord ▁shall ▁be ▁a ▁man ▁of ▁the ▁lord .\ n \ n f ort , ▁he ▁lord , ▁he ▁lord , ▁he ▁lord , ▁he ▁lord .
2021-10-20 19:54:37,856 - INFO - joeynmt.training - Example #1
2021-10-20 19:54:37,856 - DEBUG - joeynmt.training - 	Raw source:     ['s1bb20', 's38800', 's22a17', 's20b00', 's20357', 's15a57', 's15a57', 's22b07', 's20351', 's30a00', 's17620', 's1dc20', 's19220', 's14a20', 's20320', 's10e20', 's38700', 's1c519', 's1c511', 's15a17', 's15a1f', 's10028', 's10020', 's2b700', 's37c06', 's37c06', 's2b711', 's15a37', 's18220', 's28802', 's38700']
2021-10-20 19:54:37,856 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁jesus', '▁replied', ',', '▁"', 'the', 'y', '▁of', '▁the', '▁temple', ',', '▁and', '▁the', '▁day', '▁of', '▁the', '▁temple', ',']
2021-10-20 19:54:37,856 - INFO - joeynmt.training - 	Source:     s1bb20 s38800 s22a17 s20b00 s20357 s15a57 s15a57 s22b07 s20351 s30a00 s17620 s1dc20 s19220 s14a20 s20320 s10e20 s38700 s1c519 s1c511 s15a17 s15a1f s10028 s10020 s2b700 s37c06 s37c06 s2b711 s15a37 s18220 s28802 s38700
2021-10-20 19:54:37,856 - INFO - joeynmt.training - 	Reference:  ▁jesus ▁returned ▁to ▁the ▁mount ▁of ▁o l ive s ,
2021-10-20 19:54:37,856 - INFO - joeynmt.training - 	Hypothesis: ▁jesus ▁replied , ▁" the y ▁of ▁the ▁temple , ▁and ▁the ▁day ▁of ▁the ▁temple ,
2021-10-20 19:54:37,856 - INFO - joeynmt.training - Example #2
2021-10-20 19:54:37,856 - DEBUG - joeynmt.training - 	Raw source:     ['s15a07', 's1f010', 's26507', 's1eb20', 's22114', 's38800', 's1dc0a', 's10003', 's20500', 's20500', 's30a00', 's10118', 's10120', 's29306', 's29316', 's15a3f', 's15a3f', 's22a14', 's22a04', 's38700', 's10047', 's15a57', 's15a5f', 's22f04', 's10041', 's2d60e', 's25500', 's2ff00', 's11010', 's14c39', 's26516', 's14c31', 's26502', 's30a00', 's38900', 's26500', 's1f540', 's30122', 's10000', 's2ff00', 's2b700', 's1f540', 's1f550', 's28806', 's14220', 's22a14', 's22a04', 's14228', 's10047', 's18d21', 's22a05', 's20500', 's20500', 's10047', 's38800']
2021-10-20 19:54:37,856 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁verse', '▁4.', '▁but', '▁they', '▁were', '▁not', '▁to', '▁the', '▁people', '▁of', '▁the', '▁people', '▁of', '▁the', '▁people', '▁of', '▁the', '▁people', '▁who', '▁had', '▁been', '▁given', '▁him', '.']
2021-10-20 19:54:37,856 - INFO - joeynmt.training - 	Source:     s15a07 s1f010 s26507 s1eb20 s22114 s38800 s1dc0a s10003 s20500 s20500 s30a00 s10118 s10120 s29306 s29316 s15a3f s15a3f s22a14 s22a04 s38700 s10047 s15a57 s15a5f s22f04 s10041 s2d60e s25500 s2ff00 s11010 s14c39 s26516 s14c31 s26502 s30a00 s38900 s26500 s1f540 s30122 s10000 s2ff00 s2b700 s1f540 s1f550 s28806 s14220 s22a14 s22a04 s14228 s10047 s18d21 s22a05 s20500 s20500 s10047 s38800
2021-10-20 19:54:37,856 - INFO - joeynmt.training - 	Reference:  ▁then ▁he ▁s tern ly ▁warn ed ▁the ▁disciples ▁not ▁to ▁tell ▁anyone ▁that ▁he ▁was ▁the ▁messiah .
2021-10-20 19:54:37,856 - INFO - joeynmt.training - 	Hypothesis: ▁verse ▁4. ▁but ▁they ▁were ▁not ▁to ▁the ▁people ▁of ▁the ▁people ▁of ▁the ▁people ▁of ▁the ▁people ▁who ▁had ▁been ▁given ▁him .
2021-10-20 19:54:37,856 - INFO - joeynmt.training - Example #3
2021-10-20 19:54:37,856 - DEBUG - joeynmt.training - 	Raw source:     ['s15a07', 's1f010', 's26507', 's1a520', 's1f540', 's38800', 's20500', 's10043', 's30a00', 's30124', 's10010', 's26500', 's22f10', 's20500', 's15a0a', 's22f00', 's15a02', 's20500', 's20320', 's1fb20', 's1f720', 's20320', 's1f720', 's11920', 's11502', 's14a20', 's14251', 's38700', 's1ce20', 's17620', 's11a20', 's11920', 's11520', 's1fb20', 's20320', 's1f720', 's1fb20', 's11520', 's38700', 's1f720', 's20320', 's16d20', 's11502', 's11520', 's19220', 's1f720', 's16d20', 's14c30', 's14c38', 's2a204', 's2a21c', 's10009', 's10002', 's2b714', 's2b705', 's15a30', 's15a30', 's2ea36', 's2ea4e', 's38800', 's2ff00', 's10001', 's10009', 's2b711', 's2b700', 's30a00', 's11e30', 's2eb06', 's1853f', 's18537', 's2b705', 's2b714', 's20500', 's10043', 's15a37', 's1f702', 's20600', 's38700', 's15a3f', 's15a37', 's2d608', 's2d610', 's26500', 's1f540', 's30122', 's15a30', 's15a30', 's2ea36', 's2ea4e', 's16d51', 's16d51', 's26c0a', 's26c1a', 's38800']
2021-10-20 19:54:37,856 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁verse', '▁4.', '▁then', '▁he', '▁said', ',', '▁"', 'i', '▁am', '▁the', '▁king', '▁of', '▁the', '▁king', '▁of', '▁the', '▁king', '▁of', '▁the', '▁king', '▁of', '▁the', '▁king', '▁of', '▁the', '▁king', '▁of', '▁the', '▁king', '▁of', '▁the', '▁king', '▁of', '▁the', '▁king', '.', '▁he', '▁was', '▁the', '▁king', '▁of', '▁the', '▁world', '.']
2021-10-20 19:54:37,856 - INFO - joeynmt.training - 	Source:     s15a07 s1f010 s26507 s1a520 s1f540 s38800 s20500 s10043 s30a00 s30124 s10010 s26500 s22f10 s20500 s15a0a s22f00 s15a02 s20500 s20320 s1fb20 s1f720 s20320 s1f720 s11920 s11502 s14a20 s14251 s38700 s1ce20 s17620 s11a20 s11920 s11520 s1fb20 s20320 s1f720 s1fb20 s11520 s38700 s1f720 s20320 s16d20 s11502 s11520 s19220 s1f720 s16d20 s14c30 s14c38 s2a204 s2a21c s10009 s10002 s2b714 s2b705 s15a30 s15a30 s2ea36 s2ea4e s38800 s2ff00 s10001 s10009 s2b711 s2b700 s30a00 s11e30 s2eb06 s1853f s18537 s2b705 s2b714 s20500 s10043 s15a37 s1f702 s20600 s38700 s15a3f s15a37 s2d608 s2d610 s26500 s1f540 s30122 s15a30 s15a30 s2ea36 s2ea4e s16d51 s16d51 s26c0a s26c1a s38800
2021-10-20 19:54:37,856 - INFO - joeynmt.training - 	Reference:  ▁verse ▁17. ▁i ▁am ▁very ▁gl ad ▁that ▁step h an as , ▁fort un at us , ▁and ▁a ch a ic us ▁have ▁come ▁here . ▁they ▁have ▁been ▁pro vid ing ▁the ▁help ▁you ▁w er en ' t ▁here ▁to ▁give ▁me .
2021-10-20 19:54:37,856 - INFO - joeynmt.training - 	Hypothesis: ▁verse ▁4. ▁then ▁he ▁said , ▁" i ▁am ▁the ▁king ▁of ▁the ▁king ▁of ▁the ▁king ▁of ▁the ▁king ▁of ▁the ▁king ▁of ▁the ▁king ▁of ▁the ▁king ▁of ▁the ▁king ▁of ▁the ▁king . ▁he ▁was ▁the ▁king ▁of ▁the ▁world .
2021-10-20 19:54:37,856 - INFO - joeynmt.training - Example #6
2021-10-20 19:54:37,856 - DEBUG - joeynmt.training - 	Raw source:     ['s15a07', 's1f010', 's26507', 's14450', 's23004', 's38800', 's1dc0a', 's10003', 's20500', 's20500', 's36d01', 's30a00', 's15a21', 's2ff00', 's22a03', 's15a00', 's1d451', 's1d437', 's22105', 's22a00', 's2ff00', 's20600', 's10001', 's38700', 's22a14', 's14220', 's22a04', 's14228', 's36d03', 's30d00', 's20500', 's10043', 's14c00', 's14c08', 's2a204', 's2a21c', 's14c59', 's14c57', 's1f750', 's1f759', 's26503', 's26513', 's38700', 's20500', 's10043', 's36d03', 's10057', 's10059', 's2b707', 's2b717', 's15a37', 's18250', 's28902', 's2ff00', 's15a10', 's2b700', 's38800', 's14c37', 's14c3f', 's22a04', 's22a14', 's30a00', 's36d01', 's15a37', 's18250', 's28902', 's10057', 's10059', 's2b707', 's2b717', 's38700', 's1d420', 's22e00', 's1d121', 's36d01', 's14c39', 's26516', 's14c31', 's26502', 's30d00', 's38900', 's2d612', 's2d60a', 's14248', 's14240', 's15a06', 's15a0e', 's26504', 's19a30', 's19a38', 's22a04', 's22a14', 's1c550', 's26507', 's15a57', 's38700', 's15a37', 's15a42', 's2880e', 's2880e', 's20e00', 's15a31', 's14c00', 's14c08', 's2a204', 's2a21c', 's38700', 's15a48', 's15a40', 's24916', 's24906', 's38800']
2021-10-20 19:54:37,856 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁verse', '▁4.', '▁"', 'i', '▁am', '▁the', '▁king', '▁of', '▁the', '▁king', '▁of', '▁the', '▁king', '▁of', '▁the', '▁king', '▁of', '▁the', '▁king', '▁of', '▁the', '▁king', '▁of', '▁the', '▁king', '▁of', '▁the', '▁king', '▁of', '▁the', '▁king', '▁of', '▁the', '▁king', '▁of', '▁the', '▁king', '▁of', '▁the', '▁king', '▁of', '▁the', '▁king', '.']
2021-10-20 19:54:37,856 - INFO - joeynmt.training - 	Source:     s15a07 s1f010 s26507 s14450 s23004 s38800 s1dc0a s10003 s20500 s20500 s36d01 s30a00 s15a21 s2ff00 s22a03 s15a00 s1d451 s1d437 s22105 s22a00 s2ff00 s20600 s10001 s38700 s22a14 s14220 s22a04 s14228 s36d03 s30d00 s20500 s10043 s14c00 s14c08 s2a204 s2a21c s14c59 s14c57 s1f750 s1f759 s26503 s26513 s38700 s20500 s10043 s36d03 s10057 s10059 s2b707 s2b717 s15a37 s18250 s28902 s2ff00 s15a10 s2b700 s38800 s14c37 s14c3f s22a04 s22a14 s30a00 s36d01 s15a37 s18250 s28902 s10057 s10059 s2b707 s2b717 s38700 s1d420 s22e00 s1d121 s36d01 s14c39 s26516 s14c31 s26502 s30d00 s38900 s2d612 s2d60a s14248 s14240 s15a06 s15a0e s26504 s19a30 s19a38 s22a04 s22a14 s1c550 s26507 s15a57 s38700 s15a37 s15a42 s2880e s2880e s20e00 s15a31 s14c00 s14c08 s2a204 s2a21c s38700 s15a48 s15a40 s24916 s24906 s38800
2021-10-20 19:54:37,856 - INFO - joeynmt.training - 	Reference:  ▁then ▁it ▁says , ▁' i ▁will ▁return ▁to ▁the ▁person ▁i ▁came ▁from .' ▁so ▁it ▁return s ▁and ▁find s ▁its ▁form er ▁home ▁emp ty , ▁sw ept , ▁and ▁in ▁ord er .
2021-10-20 19:54:37,856 - INFO - joeynmt.training - 	Hypothesis: ▁verse ▁4. ▁" i ▁am ▁the ▁king ▁of ▁the ▁king ▁of ▁the ▁king ▁of ▁the ▁king ▁of ▁the ▁king ▁of ▁the ▁king ▁of ▁the ▁king ▁of ▁the ▁king ▁of ▁the ▁king ▁of ▁the ▁king ▁of ▁the ▁king ▁of ▁the ▁king ▁of ▁the ▁king .
2021-10-20 19:54:37,856 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step     2000: bleu:   3.34, loss: 150159.3438, ppl:  32.0534, duration: 116.1177s
2021-10-20 19:54:50,426 - INFO - joeynmt.training - Epoch   2, Step:     2100, Batch Loss:     3.444837, Tokens per Sec:     3786, Lr: 0.000100
2021-10-20 19:55:03,078 - INFO - joeynmt.training - Epoch   2, Step:     2200, Batch Loss:     3.402426, Tokens per Sec:     3624, Lr: 0.000100
2021-10-20 19:55:10,467 - INFO - joeynmt.training - Epoch   2: total training loss 3985.21
2021-10-20 19:55:10,467 - INFO - joeynmt.training - EPOCH 3
2021-10-20 19:55:15,679 - INFO - joeynmt.training - Epoch   3, Step:     2300, Batch Loss:     3.180109, Tokens per Sec:     3872, Lr: 0.000100
2021-10-20 19:55:28,608 - INFO - joeynmt.training - Epoch   3, Step:     2400, Batch Loss:     3.343131, Tokens per Sec:     3735, Lr: 0.000100
2021-10-20 19:55:41,162 - INFO - joeynmt.training - Epoch   3, Step:     2500, Batch Loss:     3.251403, Tokens per Sec:     3767, Lr: 0.000100
2021-10-20 19:55:53,993 - INFO - joeynmt.training - Epoch   3, Step:     2600, Batch Loss:     3.098825, Tokens per Sec:     3743, Lr: 0.000100
2021-10-20 19:56:06,689 - INFO - joeynmt.training - Epoch   3, Step:     2700, Batch Loss:     3.095275, Tokens per Sec:     3916, Lr: 0.000100
2021-10-20 19:56:19,348 - INFO - joeynmt.training - Epoch   3, Step:     2800, Batch Loss:     3.274069, Tokens per Sec:     3860, Lr: 0.000100
2021-10-20 19:56:31,849 - INFO - joeynmt.training - Epoch   3, Step:     2900, Batch Loss:     3.095548, Tokens per Sec:     3918, Lr: 0.000100
2021-10-20 19:56:44,284 - INFO - joeynmt.training - Epoch   3, Step:     3000, Batch Loss:     2.835858, Tokens per Sec:     3861, Lr: 0.000100
2021-10-20 19:57:32,318 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-20 19:57:32,320 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-20 19:57:32,321 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-20 19:57:32,330 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-20 19:57:33,471 - INFO - joeynmt.training - Example #0
2021-10-20 19:57:33,471 - DEBUG - joeynmt.training - 	Raw source:     ['s15a30', 's15a30', 's28812', 's28802', 's36d01', 's2ff00', 's10011', 's2e707', 's19a10', 's26500', 's32107', 's32107', 's1004a', 's10041', 's22f04', 's38700', 's15a37', 's15a3f', 's22f04', 's36d01', 's2ff00', 's10e00', 's10e50', 's15a1a', 's26507', 's26512', 's20318', 's2ff00', 's16d18', 's16d10', 's26506', 's20310', 's32107', 's38700', 's10058', 's26526', 's10050', 's20500', 's20500', 's36d01', 's1f702', 's1f70a', 's22a00', 's15a37', 's15a37', 's15a51', 's11541', 's26507', 's2d612', 's2d60a', 's14248', 's14240', 's19a21', 's2e932', 's38800']
2021-10-20 19:57:33,471 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁"', 'i', '▁have', '▁been', '▁un', 'ited', '▁in', '▁the', '▁world', ',', '▁and', '▁the', '▁world', '▁is', '▁the', '▁world', '.']
2021-10-20 19:57:33,471 - INFO - joeynmt.training - 	Source:     s15a30 s15a30 s28812 s28802 s36d01 s2ff00 s10011 s2e707 s19a10 s26500 s32107 s32107 s1004a s10041 s22f04 s38700 s15a37 s15a3f s22f04 s36d01 s2ff00 s10e00 s10e50 s15a1a s26507 s26512 s20318 s2ff00 s16d18 s16d10 s26506 s20310 s32107 s38700 s10058 s26526 s10050 s20500 s20500 s36d01 s1f702 s1f70a s22a00 s15a37 s15a37 s15a51 s11541 s26507 s2d612 s2d60a s14248 s14240 s19a21 s2e932 s38800
2021-10-20 19:57:33,471 - INFO - joeynmt.training - 	Reference:  ▁to ▁bring ▁in ▁ever l ast ing ▁righteousness , ▁to ▁conf irm ▁the ▁prophet ic ▁vision , ▁and ▁to ▁an oint ▁the ▁most ▁holy ▁place .
2021-10-20 19:57:33,471 - INFO - joeynmt.training - 	Hypothesis: ▁" i ▁have ▁been ▁un ited ▁in ▁the ▁world , ▁and ▁the ▁world ▁is ▁the ▁world .
2021-10-20 19:57:33,471 - INFO - joeynmt.training - Example #1
2021-10-20 19:57:33,471 - DEBUG - joeynmt.training - 	Raw source:     ['s1bb20', 's38800', 's22a17', 's20b00', 's20357', 's15a57', 's15a57', 's22b07', 's20351', 's30a00', 's17620', 's1dc20', 's19220', 's14a20', 's20320', 's10e20', 's38700', 's1c519', 's1c511', 's15a17', 's15a1f', 's10028', 's10020', 's2b700', 's37c06', 's37c06', 's2b711', 's15a37', 's18220', 's28802', 's38700']
2021-10-20 19:57:33,471 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁8', '▁and', '▁the', '▁day', '▁of', '▁the', '▁disciples', '▁were', '▁on', '▁jesus', ',']
2021-10-20 19:57:33,471 - INFO - joeynmt.training - 	Source:     s1bb20 s38800 s22a17 s20b00 s20357 s15a57 s15a57 s22b07 s20351 s30a00 s17620 s1dc20 s19220 s14a20 s20320 s10e20 s38700 s1c519 s1c511 s15a17 s15a1f s10028 s10020 s2b700 s37c06 s37c06 s2b711 s15a37 s18220 s28802 s38700
2021-10-20 19:57:33,471 - INFO - joeynmt.training - 	Reference:  ▁jesus ▁returned ▁to ▁the ▁mount ▁of ▁o l ive s ,
2021-10-20 19:57:33,471 - INFO - joeynmt.training - 	Hypothesis: ▁8 ▁and ▁the ▁day ▁of ▁the ▁disciples ▁were ▁on ▁jesus ,
2021-10-20 19:57:33,471 - INFO - joeynmt.training - Example #2
2021-10-20 19:57:33,471 - DEBUG - joeynmt.training - 	Raw source:     ['s15a07', 's1f010', 's26507', 's1eb20', 's22114', 's38800', 's1dc0a', 's10003', 's20500', 's20500', 's30a00', 's10118', 's10120', 's29306', 's29316', 's15a3f', 's15a3f', 's22a14', 's22a04', 's38700', 's10047', 's15a57', 's15a5f', 's22f04', 's10041', 's2d60e', 's25500', 's2ff00', 's11010', 's14c39', 's26516', 's14c31', 's26502', 's30a00', 's38900', 's26500', 's1f540', 's30122', 's10000', 's2ff00', 's2b700', 's1f540', 's1f550', 's28806', 's14220', 's22a14', 's22a04', 's14228', 's10047', 's18d21', 's22a05', 's20500', 's20500', 's10047', 's38800']
2021-10-20 19:57:33,471 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁verse', '▁10.', '▁then', '▁he', '▁said', '▁to', '▁them', ',', '▁"', 'i', '▁am', '▁not', '▁to', '▁do', '▁you', ',', '▁but', '▁you', '▁are', '▁not', '▁to', '▁do', '▁you', '."']
2021-10-20 19:57:33,472 - INFO - joeynmt.training - 	Source:     s15a07 s1f010 s26507 s1eb20 s22114 s38800 s1dc0a s10003 s20500 s20500 s30a00 s10118 s10120 s29306 s29316 s15a3f s15a3f s22a14 s22a04 s38700 s10047 s15a57 s15a5f s22f04 s10041 s2d60e s25500 s2ff00 s11010 s14c39 s26516 s14c31 s26502 s30a00 s38900 s26500 s1f540 s30122 s10000 s2ff00 s2b700 s1f540 s1f550 s28806 s14220 s22a14 s22a04 s14228 s10047 s18d21 s22a05 s20500 s20500 s10047 s38800
2021-10-20 19:57:33,472 - INFO - joeynmt.training - 	Reference:  ▁then ▁he ▁s tern ly ▁warn ed ▁the ▁disciples ▁not ▁to ▁tell ▁anyone ▁that ▁he ▁was ▁the ▁messiah .
2021-10-20 19:57:33,472 - INFO - joeynmt.training - 	Hypothesis: ▁verse ▁10. ▁then ▁he ▁said ▁to ▁them , ▁" i ▁am ▁not ▁to ▁do ▁you , ▁but ▁you ▁are ▁not ▁to ▁do ▁you ."
2021-10-20 19:57:33,472 - INFO - joeynmt.training - Example #3
2021-10-20 19:57:33,472 - DEBUG - joeynmt.training - 	Raw source:     ['s15a07', 's1f010', 's26507', 's1a520', 's1f540', 's38800', 's20500', 's10043', 's30a00', 's30124', 's10010', 's26500', 's22f10', 's20500', 's15a0a', 's22f00', 's15a02', 's20500', 's20320', 's1fb20', 's1f720', 's20320', 's1f720', 's11920', 's11502', 's14a20', 's14251', 's38700', 's1ce20', 's17620', 's11a20', 's11920', 's11520', 's1fb20', 's20320', 's1f720', 's1fb20', 's11520', 's38700', 's1f720', 's20320', 's16d20', 's11502', 's11520', 's19220', 's1f720', 's16d20', 's14c30', 's14c38', 's2a204', 's2a21c', 's10009', 's10002', 's2b714', 's2b705', 's15a30', 's15a30', 's2ea36', 's2ea4e', 's38800', 's2ff00', 's10001', 's10009', 's2b711', 's2b700', 's30a00', 's11e30', 's2eb06', 's1853f', 's18537', 's2b705', 's2b714', 's20500', 's10043', 's15a37', 's1f702', 's20600', 's38700', 's15a3f', 's15a37', 's2d608', 's2d610', 's26500', 's1f540', 's30122', 's15a30', 's15a30', 's2ea36', 's2ea4e', 's16d51', 's16d51', 's26c0a', 's26c1a', 's38800']
2021-10-20 19:57:33,472 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁verse', '▁7.', '▁i', '▁am', '▁writing', '▁to', '▁the', '▁king', '▁of', '▁the', '▁city', ',', '▁and', '▁i', '▁was', '▁the', '▁temple', '▁of', '▁the', '▁city', '.', '▁i', '▁was', '▁the', '▁son', '▁of', '▁god', "'", 's', '▁armies', '.', '▁i', '▁was', '▁not', '▁to', '▁be', '▁afraid', '▁of', '▁you', '.']
2021-10-20 19:57:33,472 - INFO - joeynmt.training - 	Source:     s15a07 s1f010 s26507 s1a520 s1f540 s38800 s20500 s10043 s30a00 s30124 s10010 s26500 s22f10 s20500 s15a0a s22f00 s15a02 s20500 s20320 s1fb20 s1f720 s20320 s1f720 s11920 s11502 s14a20 s14251 s38700 s1ce20 s17620 s11a20 s11920 s11520 s1fb20 s20320 s1f720 s1fb20 s11520 s38700 s1f720 s20320 s16d20 s11502 s11520 s19220 s1f720 s16d20 s14c30 s14c38 s2a204 s2a21c s10009 s10002 s2b714 s2b705 s15a30 s15a30 s2ea36 s2ea4e s38800 s2ff00 s10001 s10009 s2b711 s2b700 s30a00 s11e30 s2eb06 s1853f s18537 s2b705 s2b714 s20500 s10043 s15a37 s1f702 s20600 s38700 s15a3f s15a37 s2d608 s2d610 s26500 s1f540 s30122 s15a30 s15a30 s2ea36 s2ea4e s16d51 s16d51 s26c0a s26c1a s38800
2021-10-20 19:57:33,472 - INFO - joeynmt.training - 	Reference:  ▁verse ▁17. ▁i ▁am ▁very ▁gl ad ▁that ▁step h an as , ▁fort un at us , ▁and ▁a ch a ic us ▁have ▁come ▁here . ▁they ▁have ▁been ▁pro vid ing ▁the ▁help ▁you ▁w er en ' t ▁here ▁to ▁give ▁me .
2021-10-20 19:57:33,472 - INFO - joeynmt.training - 	Hypothesis: ▁verse ▁7. ▁i ▁am ▁writing ▁to ▁the ▁king ▁of ▁the ▁city , ▁and ▁i ▁was ▁the ▁temple ▁of ▁the ▁city . ▁i ▁was ▁the ▁son ▁of ▁god ' s ▁armies . ▁i ▁was ▁not ▁to ▁be ▁afraid ▁of ▁you .
2021-10-20 19:57:33,472 - INFO - joeynmt.training - Example #6
2021-10-20 19:57:33,472 - DEBUG - joeynmt.training - 	Raw source:     ['s15a07', 's1f010', 's26507', 's14450', 's23004', 's38800', 's1dc0a', 's10003', 's20500', 's20500', 's36d01', 's30a00', 's15a21', 's2ff00', 's22a03', 's15a00', 's1d451', 's1d437', 's22105', 's22a00', 's2ff00', 's20600', 's10001', 's38700', 's22a14', 's14220', 's22a04', 's14228', 's36d03', 's30d00', 's20500', 's10043', 's14c00', 's14c08', 's2a204', 's2a21c', 's14c59', 's14c57', 's1f750', 's1f759', 's26503', 's26513', 's38700', 's20500', 's10043', 's36d03', 's10057', 's10059', 's2b707', 's2b717', 's15a37', 's18250', 's28902', 's2ff00', 's15a10', 's2b700', 's38800', 's14c37', 's14c3f', 's22a04', 's22a14', 's30a00', 's36d01', 's15a37', 's18250', 's28902', 's10057', 's10059', 's2b707', 's2b717', 's38700', 's1d420', 's22e00', 's1d121', 's36d01', 's14c39', 's26516', 's14c31', 's26502', 's30d00', 's38900', 's2d612', 's2d60a', 's14248', 's14240', 's15a06', 's15a0e', 's26504', 's19a30', 's19a38', 's22a04', 's22a14', 's1c550', 's26507', 's15a57', 's38700', 's15a37', 's15a42', 's2880e', 's2880e', 's20e00', 's15a31', 's14c00', 's14c08', 's2a204', 's2a21c', 's38700', 's15a48', 's15a40', 's24916', 's24906', 's38800']
2021-10-20 19:57:33,472 - DEBUG - joeynmt.training - 	Raw hypothesis: ['▁verse', '▁4', '4.', '▁then', '▁i', '▁said', '▁to', '▁the', '▁king', ',', '▁"', 'i', '▁will', '▁be', '▁a', '▁great', '▁man', '▁who', '▁will', '▁be', '▁a', '▁great', '▁man', '▁in', '▁the', '▁temple', '▁of', '▁the', '▁king', '▁of', '▁the', '▁king', '▁of', '▁the', '▁king', "'", 's', '▁armies', '.']
2021-10-20 19:57:33,472 - INFO - joeynmt.training - 	Source:     s15a07 s1f010 s26507 s14450 s23004 s38800 s1dc0a s10003 s20500 s20500 s36d01 s30a00 s15a21 s2ff00 s22a03 s15a00 s1d451 s1d437 s22105 s22a00 s2ff00 s20600 s10001 s38700 s22a14 s14220 s22a04 s14228 s36d03 s30d00 s20500 s10043 s14c00 s14c08 s2a204 s2a21c s14c59 s14c57 s1f750 s1f759 s26503 s26513 s38700 s20500 s10043 s36d03 s10057 s10059 s2b707 s2b717 s15a37 s18250 s28902 s2ff00 s15a10 s2b700 s38800 s14c37 s14c3f s22a04 s22a14 s30a00 s36d01 s15a37 s18250 s28902 s10057 s10059 s2b707 s2b717 s38700 s1d420 s22e00 s1d121 s36d01 s14c39 s26516 s14c31 s26502 s30d00 s38900 s2d612 s2d60a s14248 s14240 s15a06 s15a0e s26504 s19a30 s19a38 s22a04 s22a14 s1c550 s26507 s15a57 s38700 s15a37 s15a42 s2880e s2880e s20e00 s15a31 s14c00 s14c08 s2a204 s2a21c s38700 s15a48 s15a40 s24916 s24906 s38800
2021-10-20 19:57:33,472 - INFO - joeynmt.training - 	Reference:  ▁then ▁it ▁says , ▁' i ▁will ▁return ▁to ▁the ▁person ▁i ▁came ▁from .' ▁so ▁it ▁return s ▁and ▁find s ▁its ▁form er ▁home ▁emp ty , ▁sw ept , ▁and ▁in ▁ord er .
2021-10-20 19:57:33,472 - INFO - joeynmt.training - 	Hypothesis: ▁verse ▁4 4. ▁then ▁i ▁said ▁to ▁the ▁king , ▁" i ▁will ▁be ▁a ▁great ▁man ▁who ▁will ▁be ▁a ▁great ▁man ▁in ▁the ▁temple ▁of ▁the ▁king ▁of ▁the ▁king ▁of ▁the ▁king ' s ▁armies .
2021-10-20 19:57:33,472 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step     3000: bleu:   4.75, loss: 138215.2812, ppl:  24.3272, duration: 49.1882s
2021-10-20 19:57:46,143 - INFO - joeynmt.training - Epoch   3, Step:     3100, Batch Loss:     3.014495, Tokens per Sec:     3726, Lr: 0.000100
2021-10-20 19:57:59,101 - INFO - joeynmt.training - Epoch   3, Step:     3200, Batch Loss:     2.990002, Tokens per Sec:     3672, Lr: 0.000100
2021-10-20 19:58:11,980 - INFO - joeynmt.training - Epoch   3, Step:     3300, Batch Loss:     2.794182, Tokens per Sec:     3774, Lr: 0.000100
2021-10-20 19:58:22,890 - INFO - joeynmt.training - Epoch   3: total training loss 3507.02
2021-10-20 19:58:22,890 - INFO - joeynmt.training - EPOCH 4
2021-10-20 19:58:24,618 - INFO - joeynmt.training - Epoch   4, Step:     3400, Batch Loss:     2.801909, Tokens per Sec:     3921, Lr: 0.000100
2021-10-20 19:58:37,455 - INFO - joeynmt.training - Epoch   4, Step:     3500, Batch Loss:     2.629371, Tokens per Sec:     3712, Lr: 0.000100
2021-10-20 19:58:50,047 - INFO - joeynmt.training - Epoch   4, Step:     3600, Batch Loss:     2.423125, Tokens per Sec:     3741, Lr: 0.000100
2021-10-20 19:59:02,151 - INFO - joeynmt.training - Epoch   4, Step:     3700, Batch Loss:     2.816037, Tokens per Sec:     3920, Lr: 0.000100
2021-10-20 19:59:15,117 - INFO - joeynmt.training - Epoch   4, Step:     3800, Batch Loss:     2.752935, Tokens per Sec:     3785, Lr: 0.000100
2021-10-20 19:59:28,266 - INFO - joeynmt.training - Epoch   4, Step:     3900, Batch Loss:     2.559106, Tokens per Sec:     3781, Lr: 0.000100
2021-10-20 19:59:40,965 - INFO - joeynmt.training - Epoch   4, Step:     4000, Batch Loss:     3.097181, Tokens per Sec:     3684, Lr: 0.000100
